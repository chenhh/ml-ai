---
description: Broyden–Fletcher–Goldfarb–Shanno (BFGS) 法
---

# BFGS法

## 簡介

**BFGS法** 是一種用於求解無約束優化問題的數值方法，屬於擬牛頓法（Quasi-Newton Methods）的一類。它廣泛應用於非線性優化問題中，特別是在目標函式的梯度可以計算但Hessian矩陣難以直接計算或儲存的情況下。

BFGS 方法能有效逼近 Hessian 矩陣的逆矩陣，而無需直接計算二階導數，因此比標準牛頓法更適合高維度問題。

在無約束優化問題中，我們希望找到一個函數$$f(x)$$ 的最小值點 $$x^{*}$$，即：$$\min_{x\in \mathbb{R}^n} ​f(x)$$，

其中$$f(x)$$是一個光滑的目標函式。

### 牛頓法

傳統的牛頓法利用了目標函式的梯度$$\nabla f(x)$$ 和Hessian矩陣 $$\nabla^2 f(x)$$ 來進行迭代更新。

$$x_{t+1}=x_t-H_f(x_t)^{-1}\nabla f(x_t)$$

* $$\nabla f(x_t)$$為梯度向量。
* $$H_f(x_t)^{-1}$$是函式$$f$$ 在$$x_t$$ 處的Hessian逆矩陣。

牛頓法具有二次收斂速度，在接近解時收斂非常快。然而，牛頓法的主要缺點是：

1. **需要計算海森矩陣**：對於複雜函式，計算海森矩陣可能非常耗時甚至不可行。
2. **需要求海森矩陣的逆矩陣**：求逆矩陣也是計算量大的操作，尤其當問題維度很高時。
3. **海森矩陣可能非正定**：牛頓法需要海森矩陣是正定的，才能保證搜尋方向是下降方向。

然而，計算和儲存Hessian矩陣可能非常昂貴，尤其是在高維問題中。為瞭解決這個問題，擬牛頓法（如BFGS）通過構造一個近似的Hessian矩陣來避免直接計算Hessian，從而降低了計算成本。

BFGS 法就是最成功的擬牛頓法之一。它透過一種巧妙的公式，**直接迭代更新海森逆矩陣的近似矩陣**，從而避免了每次迭代都要求解線性系統。

## 演算法

### **初始化**

* 給定初始點$$x_0$$​。
* 設定初始海森逆矩陣近似$$B_0​=I$$ (單位矩陣)。
* 設定迭代次數 $$k=0$$。

### **迭代步驟 (直到收斂)**

* **計算搜尋方向 ​：** $$p_k​=−B_k​\nabla f(x_k​)$$
  * BFGS 法的搜尋方向$$p_k$$​ 是由當前的海森逆矩陣近似$$B_k$$​ 和負梯度方向 $$−\nabla f(x_k​)$$ 相乘得到的。
* **線搜尋 (Line Search) 確定步長 ​：**
  * 沿著搜尋方向$$p_k$$​ 進行線搜尋，尋找合適的步長 $$\alpha_k​>0$$，使得目標函式$$f(x)$$ 能夠充分下降。常用的線搜尋方法包括 Wolfe 條件、Armijo 規則等。 線搜尋的目的是找到一個合適的步長，以保證函式值在每次迭代中都得到有效下降，並為後續的 BFGS 更新公式提供必要的資訊。
  * 例如，使用 Wolfe 條件，需要找到滿足以下兩個條件的$$\alpha_k$$​：
    * **充分下降條件 (Sufficient Decrease Condition)：** $$f(x_k​+\alpha_k​p_k​) \leq f(x_k​)+c_1\alpha_k​ \nabla f(x_k​)^\top p_k$$​ (保證函式值下降)
    * **曲率條件 (Curvature Condition)：** $$\nabla f(x_k​+\alpha_k​ p_k​)^{\top} p_k​  \geq c_2​ \nabla f(x_k​)^\top p_k$$​ (防止步長過小)
    * 其中 $$0<c1​<c2​<1$$ 是常數 (通常 $$c1​=10^{−4},~c2​=0.9$$)。
* **更新迭代點：** $$x_{k+1}​=x_k​+\alpha_k​ p_k​$$。
* **計算新的梯度：**$$\nabla f(x_{k+1}​)$$。
* **計算值：**$$s_k​=x_{k+1}​−x_{k}​$$和$$y_k​=\nabla f(x_{k+1​})−\nabla f(x_k​)。$$
* **更新Hessian逆矩陣近似​：** 使用 BFGS 更新公式： $$B_{k+1}=B_k-\frac{B_k y_k y_k^\top B_k}{y_k^\top B_k y_k}+ \frac{s_k s_k^\top}{y_k^\top s_k}$$。
  * 這確保$$B_{k+1}$$滿足擬牛頓條件（secant condition）$$B_{k+1}y_k=s_k$$。
  * 絕大多數的 BFGS 演算法實現都是使用這個公式來更新 Hessian 逆矩陣的近似。
  * 在 BFGS 公式之前，另一種著名的擬牛頓更新公式是 **DFP (Davidon–Fletcher–Powell) 公式：**$$B_{k+1}^{(DFP)}=B_k-\frac{(B_k y_k)(B_k y_k)^\top }{y_k^\top B_k y_k}+ \frac{s_k s_k^\top}{y_k^\top s_k}$$。DFP 公式與 BFGS 公式在形式上非常相似，它們都使用秩一修正項來更新 Hessian 逆矩陣近似。 **BFGS 公式可以被視為是對 DFP 公式的一種「對偶」形式**。
  * 在歷史發展上，DFP 公式先被提出，而後 BFGS 公式被獨立發現 (Broyden, Fletcher, Goldfarb, Shanno 各自獨立匯出，因此稱為 BFGS)。 **在實際應用中，BFGS 公式通常比 DFP 公式表現更好，更穩定，收斂速度也更快，因此 BFGS 公式逐漸取代了 DFP 公式，成為擬牛頓法中最受歡迎和廣泛使用的方法。**
  * 也可得到對應的 **Hessian 矩陣近似更新公式** $$H_{k+1}=H_k-\frac{H_k s_k s_k^\top H_k}{s_k^\top H_k s_k}+ \frac{y_k y_k^\top}{y_k^\top s_k}$$，這個公式直接更新 Hessian 矩陣的近似​。**在實際應用中，它不如直接更新 Hessian 逆矩陣的公式常用。**
* **檢查收斂性：** 判斷是否滿足收斂條件，例如：
  * 梯度范數 $$\|\nabla f(x_{k+1})\|$$ 是否足夠小 (小於預設容忍度$$\epsilon$$)。
  * 迭代步長$$\|x_{k+1}​−x_k​\|$$是否足夠小。
  * 函式值的變化$$\| f(x_{k+1}​)−f(x_k​)\|$$ 是否足夠小。 如果滿足收斂條件，則$$x_{k+1}$$​ 即為近似解，演算法終止。
  * 迭代次數$$k=k+1$$，返回步驟 2。
* **輸出結果：** 當達到收斂條件時，輸出近似解 $$x_{k+1}$$​。

### **線搜尋的重要性**

線搜尋在 BFGS 法中至關重要。 **適當的線搜尋不僅保證了演算法的收斂性，還有助於保持海森逆矩陣近似的正定性。** 常用的線搜尋方法，如 Wolfe 條件，能夠確保步長$$\alpha_k$$​ 既能使函式值充分下降，又能避免步長過小，從而提高演算法的效率和穩健性。

## **優點**

* **超線性收斂速度 (Superlinear Convergence)：** 在接近解時，BFGS 法的收斂速度非常快，通常優於梯度下降法等一階方法，接近牛頓法的二次收斂速度。
* **不需要計算海森矩陣：** BFGS 法避免了顯式計算和反轉海森矩陣，降低了計算複雜度，特別是對於高維問題或海森矩陣難以計算的函式。
* **海森近似矩陣保持正定性：** 在適當的條件下 (例如精確線搜尋或 Wolfe 線搜尋)，BFGS 迭代產生的海森逆矩陣近似$$B_k$$​ 始終保持正定，保證搜尋方向是下降方向，從而提高了演算法的穩健性。
* **適用範圍廣泛：** BFGS 法對於許多型別的無約束最佳化問題都表現良好，是一種通用的最佳化演算法。
* **相較於最速下降法，對問題的縮放 (Scaling) 不敏感：** BFGS 法的收斂性不像最速下降法那樣容易受到問題變數尺度變化的影響。
* **記憶體效率相對較高：** 雖然 BFGS 法需要儲存海森逆矩陣的近似矩陣 (一個$$n \times n$$ 的矩陣，其中$$n$$ 是問題的維度)，但在某些情況下，相較於需要計算和儲存完整海森矩陣的牛頓法，其記憶體需求更為合理。

## **缺點**

* **仍然是區域性最佳化方法：** BFGS 法只能保證收斂到區域性最小值，無法保證找到全域性最小值。對於非凸函式，演算法的結果可能取決於初始點的選擇。
* **需要梯度資訊：** BFGS 法需要計算目標函式的梯度，對於梯度難以計算或不存在的函式，BFGS 法不適用。
* **對於極高維問題，海森矩陣近似的儲存和更新仍然有計算成本：** 對於非常高維的問題 (例如數十萬維以上)，儲存和更新$$n \times n$$ 的海森逆矩陣近似矩陣可能變得計算量和記憶體消耗都很大。
* **矩陣近似可能變得病態 (Ill-conditioned)：** 在迭代過程中，海森近似矩陣可能會變得病態，導致數值不穩定或收斂速度減慢。

## **有限記憶體 BFGS 法 (L-BFGS)**

L-BFGS 法是一種非常流行的 BFGS 變體，特別適用於大規模最佳化問題。 L-BFGS 法 **不顯式儲存和更新完整的海森逆矩陣近似 Bk​，而是只儲存少量的歷史迭代資訊 (**&#x4F8B;如最近 $$m$$ 次迭代的 $$s_k$$​ 和 $$y_k$$​ 向量&#x5C0D;**)**。在每次迭代時，L-BFGS 法利用這些儲存的向量對，透過遞迴的方式 (兩步遞迴法) 來計算矩陣向量乘積 $$B_k​ \nabla f(x_k​)$$，而不需要顯式組裝出矩陣 $$B_k$$​。

L-BFGS 法顯著降低了記憶體需求，使其能夠應用於更大規模的最佳化問題。在機器學習、大規模資料分析等領域，L-BFGS 法得到了非常廣泛的應用。
