# 梯度下降法(gradient descent)

## 簡介

梯度下降法（Gradient Descent）是一種廣泛應用於最佳化問題的迭代演算法，主要用來尋找目標函式的最優解（通常是最小值）。它的核心思想是利用函式的梯度（即導數或偏導數）來指引搜尋方向，逐步調整引數，使目標函式值逐步減小，直到收斂到極值點。

## 基本概念

假設我們要最小化一個目標函式 $$f(x)$$：

* $$x$$ 可以是標量或向量（例如 $$x = [x_1, x_2, \dots, x_n]$$）。
* 梯度 $$\nabla f(x)$$表示$$f(x)$$在$$x$$處的最陡上升方向，其負梯度$$-\nabla f(x)$$則是最陡下降方向。
* 梯度下降法通過沿著負梯度方向逐步移動來降低$$f(x)$$值。

## 演算法

梯度下降法的更新規則為：$$x_{t+1}=x_t−η\nabla f(x_t)$$

* $$\nabla f(x_t)$$是目標函數在當前點的梯度。
* $$\eta$$是學習率（步長），控制每次更新的幅度。也可改成時變參數$$\eta_t$$。
  * 若 $$\eta$$太大，更新步伐過大，可能跳過最小值甚至發散。&#x20;
  * 若 $$\eta$$太小，更新速度慢，收斂過程變長。
  * 解決方案：動態學習率調整（如 Adam, RMSprop, Momentum）。 學習率衰減（Learning Rate Decay）。

1. **初始化**：隨機選擇初始參數$$x_0$$​。
2. **計算梯度**：計算當前點的梯度$$\nabla f(x)$$。
3. **更新參數**：沿負梯度方向更新參數。
4. **重複迭代**：重複步驟 2 和 3，直到滿足停止條件（如梯度接近零或達到最大迭代次數）。

## 變形演算法

### 批次梯度下降（Batch Gradient Descent, BGD）

在每次更新時，使用整個資料集計算梯度，即原本的方法。

* **優點**：每次更新方向穩定，適合凸函式優化。
* **缺點**：當資料集很大時，計算整個資料集的梯度開銷較大，速度較慢。

### 隨機梯度下降（Stochastic Gradient Descent, SGD）

在每次更新時，得到一**個新隨機樣本就**計算梯度：$$x_{t+1} = x_t - \eta \nabla f(x_i)$$

* **優點**：
  * 每次更新計算量小，適合大規模資料集。
  * 可能跳出區域性最小值，提高收斂機會。
* **缺點**：更新方向具有較大隨機性，可能導致收斂不穩定或振盪。

### 小批次梯度下降（Mini-Batch Gradient Descent, MBGD）

在每次更新時，使用**小批次（mini-batch）樣本**計算梯度：$$x_{t+1} = x_t - \eta \nabla f(x_{bat})$$

* **優點**：
  * 兼具批次和隨機梯度下降的優勢，計算效率較高。
  * 梯度較穩定，適合平行計算。
* **缺點**：仍可能陷入區域性最小值。

## 優點

* 簡單易實現。
* 適用於大規模資料和複雜模型。
* 在凸函數中能收斂到全域性最優，在非凸函數中能收斂到區域性最優。

## 缺點

* 學習率選擇困難：過小導致收斂慢，過大可能導致震盪或不收斂。
* 容易陷入區域性最優（對於非凸函數）。
* 對初始值敏感。
