# Nelder–Mead 方法

## 簡介

Nelder–Mead 方法 （又稱為 下山單純形法 或 單純形優化法 ）是一種用於求解非線性優化問題的數值方法。該方法由 John Nelder 和 Roger Mead 於 1965 年提出，是一種基於幾何變換的直接搜尋演算法，適用於無約束優化問題。

與梯度下降法不同，<mark style="color:red;">**Nelder–Mead 方法不需要計算目標函式的梯度**</mark>，因此特別適合於那些梯度難以計算或不可微的情況。

## 核心思想：單純形幾何變換

elder-Mead 方法的核心思想是使用**單純形 (Simplex)** 作為搜尋的基本幾何形狀，並透過一系列的**幾何變換 (Geometric Operations)** 來逐步逼近區域性最小值。

**單純形 (Simplex)**：在 n 維空間中，單純形是由 n+1 個頂點構成的凸多面體。例如：

* 在 1 維空間 (線) 中，單純形是一個線段 (2 個頂點)。
* 在 2 維空間 (平面) 中，單純形是一個三角形 (3 個頂點)
* 在 3 維空間 中，單純形是一個四面體 (4 個頂點)。

<figure><img src="../../.gitbook/assets/image (66).png" alt="" width="188"><figcaption><p>三維空間的單純形是一個四面體</p></figcaption></figure>

**幾何變換 (Geometric Operations)**： Nelder-Mead 方法主要使用以下四種幾何變換來調整單純形的形狀和位置，以朝向目標函式值更小的區域移動：

1. **反射 (Reflection)**
2. **擴張 (Expansion)**
3. **收縮 (Contraction)**
4. **縮減 (Shrinkage)**

## 演算法步驟 (基本 Nelder-Mead 方法)

以下是基本的 Nelder-Mead 方法演算法步驟，用於尋找目標函式$$f(x)$$的區域性最小值，其中$$x$$ 是$$n$$維向量：

#### **初始化 (Initialization)**：

* 選擇$$n+1$$個初始點 $$x1​,x2​,\dots,x_{n+1}$$​，構成初始單純形。 通常可以選擇一個起始點$$x_1$$​，然後基於此點生成其他點，例如$$x_{i+1}​=x_1​+he_i$$​，其中$$h$$是一個步長， $$e_i$$​ 是第$$i$$ 個單位向量。
* 計算每個頂點的目標函式值$$f(x_1​),f(x_2​),\dots,f(x_{n+1}​)$$。

#### **排序 (Ordering)**：

根據函式值對頂點進行排序，找到：

* **最佳點 (Best Point)**： $$x_b$$​，對應最小的函式值 $$f(x_b​)=\min\{f(x_1​),\dots,f(x_{n+1}​)\}$$。
* **最壞點 (Worst Point)**： $$x_w$$​，對應最大的函式值 $$f(x_w​)=\max\{f(x_1​),\dots,f(x_{n+1}​)\}$$。
* **次壞點 (Second Worst Point)**： $$x_s$$​，對應次大的函式值。

#### **計算質心 (Centroid)**：

* 計算除了最壞點$$x_w$$​ 之外所有頂點的質心 $$x_c​​=\frac{1}{n} \sum_{i \neq w}​x_i$$。

**反射 (Reflection)**：

* 計算反射點$$x_r​​=x_c​+\alpha(x_c​−x_w​)=x_c​+\alpha( \text{ reflection vector })$$，其中$$\alpha>0$$ 是反射係數 (通常 $$\alpha=1$$)。
* 計算 $$f(x_r​)$$。

**判斷反射效果**：

* 如果 $$f(x_r​)<f(x_b​)$$ (反射點比最佳點更好)，則進行 **擴張** 。
* 如果$$f(x_r​)<f(x_s​)$$ (反射點比次壞點好，但不如最佳點)，則用反射點$$x_r$$​ 替換最壞點 $$x_w​$$，並回到步驟 2 (重新排序)。
* 如果 $$f(x_r​) \geq f(x_s​)$$ (反射點不如次壞點好)，則進行 **收縮** 。

#### **擴張 (Expansion)**&#x20;

(如果反射成功，即 $$f(x_r​)<f(x_b​)$$)：

* 計算擴張點 $$x_e​​=x_c​+\gamma(x_r​−x_c​)=x_c​+\gamma(\text{ expansion vector })$$，其中$$\gamma>1$$ 是擴張係數 (通常$$\gamma=2$$)。
* 計算 $$f(x_e​)$$。

#### **判斷擴張效果**：

* 如果 $$f(x_e​)<f(x_r​)$$ (擴張點比反射點更好)，則用擴張點$$x_e$$​ 替換最壞點$$x_w$$​。
* 否則 (擴張點不如反射點好)，則仍然用反射點$$x_r$$​ 替換最壞點$$x_w$$​。
* 回到步驟  (重新排序)。

#### **收縮 (Contraction)**&#x20;

(如果反射不成功，即$$f(x_r​) \geq f(x_s​)$$)：

**判斷收縮型別**：

* 如果 f(xr​)\<f(xw​) (反射點比最壞點好，但不如次壞點)，則進行 **外收縮 (Outside Contraction)**：
  * 計算外收縮點 xoc​=xc​+ρ(xr​−xc​)=xc​+ρ(outside contraction vector)，其中 0<ρ≤0.5 是收縮係數 (通常 ρ=0.5)。
  * 計算 f(xoc​)。
  * 如果 f(xoc​)≤f(xr​) (外收縮點比反射點好)，則用外收縮點 xoc​ 替換最壞點 xw​。
  * 否則 (外收縮點不如反射點好)，則進行 **縮減** (步驟 7)。
* 如果 f(xr​)≥f(xw​) (反射點比最壞點還差)，則進行 **內收縮 (Inside Contraction)**：
  * 計算內收縮點 xic​=xc​+σ(xw​−xc​)=xc​+σ(inside contraction vector)，其中 0<σ≤0.5 是收縮係數 (通常 σ=0.5)。
  * 計算 f(xic​)。
  * 如果 f(xic​)\<f(xw​) (內收縮點比最壞點好)，則用內收縮點 xic​ 替換最壞點 xw​。
  * 否則 (內收縮點不如最壞點好)，則進行 **縮減** (步驟 7)。



**縮減 (Shrinkage)** (如果收縮不成功，即外收縮或內收縮都未能改善)：

* 用縮減操作來大幅度縮小單純形，將所有點都朝向最佳點 xb​ 收縮： xi​=xb​+δ(xi​−xb​) for all i=b，其中 0<δ≤1 是縮減係數 (通常 δ=0.5)。
* 重新計算所有頂點的函式值 f(xi​)。
* 回到步驟 2 (重新排序)。

- **終止條件 (Termination)**：
  * 檢查是否滿足終止條件，例如：
    * 函式值的單純形標準差小於某個容忍度。
    * 單純形的大小 (例如邊長) 小於某個容忍度。
    * 達到最大迭代次數。
  * 如果滿足終止條件，則最佳點 xb​ 即為近似區域性最小值，演算法終止。
  * 否則，回到步驟 2 繼續迭代。
