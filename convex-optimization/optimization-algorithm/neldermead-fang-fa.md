# Nelder–Mead 方法

## 簡介

Nelder–Mead 方法 （又稱為 下山單純形法 或 單純形優化法 ）是一種用於求解非線性優化問題的數值方法。該方法由 John Nelder 和 Roger Mead 於 1965 年提出，是一種基於幾何變換的直接搜尋演算法，適用於無約束優化問題。

與梯度下降法不同，<mark style="color:red;">**Nelder–Mead 方法不需要計算目標函式的梯度**</mark>，因此特別適合於那些梯度難以計算或不可微的情況。

## 核心思想：單純形幾何變換

elder-Mead 方法的核心思想是使用**單純形 (Simplex)** 作為搜尋的基本幾何形狀，並透過一系列的**幾何變換 (Geometric Operations)** 來逐步逼近區域性最小值。

**單純形 (Simplex)**：在 n 維空間中，單純形是由 n+1 個頂點構成的凸多面體。例如：

* 在 1 維空間 (線) 中，單純形是一個線段 (2 個頂點)。
* 在 2 維空間 (平面) 中，單純形是一個三角形 (3 個頂點)
* 在 3 維空間 中，單純形是一個四面體 (4 個頂點)。

<figure><img src="../../.gitbook/assets/image (109).png" alt="" width="188"><figcaption><p>三維空間的單純形是一個四面體</p></figcaption></figure>

**幾何變換 (Geometric Operations)**： Nelder-Mead 方法主要使用以下四種幾何變換來調整單純形的形狀和位置，以朝向目標函式值更小的區域移動：

1. **反射 (Reflection)**
2. **擴張 (Expansion)**
3. **收縮 (Contraction)**
4. **縮減 (Shrinkage)**

## 演算法步驟 (基本 Nelder-Mead 方法)

1. **初始化**：在$$n$$維空間中選擇 $$n+1$$個點，構成初始單純形。
2. **排序**：計算每個頂點的函數值並排序。
3. **反射**：將最差點通過重心反射，生成反射點。
4. **擴展**：若反射點優於當前最優點，嘗試擴展。
5. **收縮**：若反射點不優於次差點，進行收縮。
6. **縮小**：若以上步驟無效，縮小單純形向最優點靠攏。

以下是基本的 Nelder-Mead 方法演算法步驟，用於尋找目標函式$$f(x)$$的區域性最小值，其中$$x$$ 是$$n$$維向量：

#### **1. 初始化 (Initialization)**：

* 選擇$$n+1$$個初始點 $$x1​,x2​,\dots,x_{n+1}$$​，構成初始單純形。 通常可以選擇一個起始點$$x_1$$​，然後基於此點生成其他點，例如$$x_{i+1}​=x_1​+he_i$$​，其中$$h$$是一個步長， $$e_i$$​ 是第$$i$$ 個單位向量。
* 計算每個頂點的目標函式值$$f(x_1​),f(x_2​),\dots,f(x_{n+1}​)$$。

#### **2. 排序 (Ordering)**：

根據函式值對頂點進行排序，找到：

* **最佳點 (Best Point)**： $$x_b$$​，對應最小的函式值 $$f(x_b​)=\min\{f(x_1​),\dots,f(x_{n+1}​)\}$$。
* **最壞點 (Worst Point)**： $$x_w$$​，對應最大的函式值 $$f(x_w​)=\max\{f(x_1​),\dots,f(x_{n+1}​)\}$$。
* **次壞點 (Second Worst Point)**： $$x_s$$​，對應次大的函式值。

#### **3. 計算質心 (Centroid)**：

* 計算除了最壞點$$x_w$$​ 之外所有頂點的質心 $$x_c​​=\frac{1}{n} \sum_{i \neq w}​x_i$$。

4. **反射 (Reflection)**：

* 計算反射點$$x_r​​=x_c​+\alpha(x_c​−x_w​)=x_c​+\alpha( \text{ reflection vector })$$，其中$$\alpha>0$$ 是反射係數 (通常 $$\alpha=1$$)。
* 計算 $$f(x_r​)$$。

**判斷反射效果**：

* 如果 $$f(x_r​)<f(x_b​)$$ (反射點比最佳點更好)，則進行 **擴張** 。
* 如果$$f(x_r​)<f(x_s​)$$ (反射點比次壞點好，但不如最佳點)，則用反射點$$x_r$$​ 替換最壞點 $$x_w​$$，並回到步驟 2 (重新排序)。
* 如果 $$f(x_r​) \geq f(x_s​)$$ (反射點不如次壞點好)，則進行 **收縮** 。

#### **5. 擴張 (Expansion)**&#x20;

(如果反射成功，即 $$f(x_r​)<f(x_b​)$$)：

* 計算擴張點 $$x_e​​=x_c​+\gamma(x_r​−x_c​)=x_c​+\gamma(\text{ expansion vector })$$，其中$$\gamma>1$$ 是擴張係數 (通常$$\gamma=2$$)。
* 計算 $$f(x_e​)$$。

#### **判斷擴張效果**：

* 如果 $$f(x_e​)<f(x_r​)$$ (擴張點比反射點更好)，則用擴張點$$x_e$$​ 替換最壞點$$x_w$$​。
* 否則 (擴張點不如反射點好)，則仍然用反射點$$x_r$$​ 替換最壞點$$x_w$$​。
* 回到步驟  (重新排序)。

6. **收縮 (Contraction)**&#x20;

(如果反射不成功，即$$f(x_r​) \geq f(x_s​)$$)：

**判斷收縮型別**：

* 如果 $$f(x_r​)<f(x_w​)$$ (反射點比最壞點好，但不如次壞點)，則進行 **外收縮 (Outside Contraction)**：
  * 計算外收縮點 $$x_{oc}​=x)c​+\rho(x_r​−x_c​)=x_c​+\rho( \text{ outside contraction vector })$$，其中 $$0 < \rho \leq 0.5$$ 是收縮係數 (通常 $$\rho=0.5$$)。
  * 計算 $$f(x_{oc}​)$$。
  * 如果 $$f(x_{oc}​)\leq f(x_r​)$$ (外收縮點比反射點好)，則用外收縮點 $$x_{oc}​ 替換最壞點 x_w$$​。
  * 否則 (外收縮點不如反射點好)，則進行 **縮減** 。
* 如果 $$f(x_r​)\geq f(x_w​)$$ (反射點比最壞點還差)，則進行 **內收縮 (Inside Contraction)**：
  * 計算內收縮點 $$x_{ic}​=x_c​+\sigma (x_w​−x_c​)=x_c​+ \sigma( \text{ inside contraction vector })$$，其中 $$0< \sigma \leq 0.5$$ 是收縮係數 (通常 $$\sigma=0.5$$)。
  * 計算 $$f(x_{ic}​)$$。
  * 如果 $$f(x_{ic}​)<f(x_w​)$$ (內收縮點比最壞點好)，則用內收縮點 $$x_{ic}$$​ 替換最壞點 $$x_w$$​。
  * 否則 (內收縮點不如最壞點好)，則進行 **縮減**。

#### **7. 縮減 (Shrinkage)**&#x20;

(如果收縮不成功，即外收縮或內收縮都未能改善)：

* 用縮減操作來大幅度縮小單純形，將所有點都朝向最佳點$$x_b$$​ 收縮： $$x_i​=x_b​+\delta(x_i​−x_b​),  \forall i\neq b$$，其中$$0<\delta\leq 1$$ 是縮減係數 (通常 $$\delta=0.5$$)。
* 重新計算所有頂點的函式值 $$f(x_i​)$$。
* 回到步驟 (重新排序)。

#### **8. 終止條件 (Termination)**：

* 檢查是否滿足終止條件，例如：
  * 函式值的單純形標準差小於某個容忍度。
  * 單純形的大小 (例如邊長) 小於某個容忍度。
  * 達到最大迭代次數。
* 如果滿足終止條件，則最佳點 $$x_b​$$ 即為近似區域性最小值，演算法終止。
* 否則，回到步驟 2 繼續迭代。

### **係數的選擇**

典型的 Nelder-Mead 方法使用以下係數值：

* 反射係數$$\alpha=1$$
* 擴張係數$$\gamma=2$$
* 收縮係數$$\rho=0.5$$
* 縮減係數$$\delta=0.5$$

這些係數值在許多情況下表現良好，但也可以根據具體問題進行調整。

## **優點和缺點**

### **優點**

* **無需導數資訊**： Nelder-Mead 方法不需要計算目標函式的導數 (梯度)，因此適用於不可微分或導數難以計算的函式。
* **概念簡單，易於實現**： 演算法邏輯相對簡單，容易編碼實現。
* **適用性廣泛**： 適用於各種維度的無約束最佳化問題，尤其是低維問題。
* **對函式性質要求不高**： 對目標函式的光滑性、凸性等性質要求不高，即使函式具有噪聲或不連續點，也可能有效。
* **實用性強**： 在許多實際問題中，即使理論收斂性較慢，仍然能快速找到近似解。

### **缺點**

* **收斂速度較慢**： 相較於使用導數資訊的最佳化方法 (如梯度下降法、牛頓法)，Nelder-Mead 方法的收斂速度通常較慢，尤其在高維問題中。
* **容易陷入區域性最小值**： 作為一種區域性搜尋方法，容易陷入區域性最小值，而無法保證找到全域性最小值。
* **收斂性理論不完善**： 對於一般非凸函式，理論上不能保證收斂到區域性最小值 (儘管在實踐中通常能有效收斂)。
* **對初始單純形敏感**： 演算法的效能和收斂性可能受到初始單純形的影響。
* **在高維空間效率降低**： 隨著維度的增加，單純形的頂點數量也線性增加，計算量也會增加，效率可能降低。

## **應用領域**

Nelder-Mead 方法廣泛應用於各種領域，特別是在以下情況：

* **目標函式複雜或未知**： 當目標函式的解析表示式未知，或者計算成本很高時(註：現在已有自動微分計算方法)。
* **函式不可微分或有噪聲**： 當目標函式不可微分，或者包含噪聲，使得梯度資訊不可靠時。
* **低維最佳化問題**： 在維度不高 (例如小於 10 維) 的問題中，Nelder-Mead 方法通常表現良好。
* **實驗資料擬合**： 用於模型引數最佳化，使得模型預測值與實驗資料最佳吻合。
* **工程設計最佳化**： 例如機械結構設計、控制系統引數調整等。
* **機器學習引數調整**： 例如在某些機器學習模型的超引數調整中，當梯度資訊難以獲得時。
