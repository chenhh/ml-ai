# 資訊熵(information entropy)

## 簡介

由於無序、混亂、不確定性或驚奇可以被視為資訊的不同色調，熵作為其衡量標準就顯得非常方便。

考慮一個隨機實驗，其結果為$$x_1, x_2, \dots , x_N$$的機率分別為$$p_1, p_2, \dots, p_N$$。 我們可以說，這些結果是離散隨機變數$$X$$的值。$$X$$的每個值$$x_i$$代表一個事件，其發生的機率為$$p_i$$。事件$$x_i$$的機率$$p_i$$可以解釋為對事件$$x_i$$發生的不確定性的衡量。我們也可以說，事件$$x_i$$的發生提供了一個關於該機率$$p_i$$正確的可能性的測量資訊（Batty，2010）。如果$$p_i$$很低，比如說0.01，那麼我們有理由確信事件$$x_i$$不會發生，如果$$x_i$$真的發生了，那麼對於$$p_i=0.01$$的$$x_i$$的發生會有很大的驚訝，因為我們對它的預期是高度不確定的。另一方面，如果$$p_i$$非常高，比如說$$0.99$$，那麼就有理由肯定事件$$x_i$$會發生，如果$$x_i$$真的發生了，那麼對於$$p_i=0.99$$的$$x_i$$的發生幾乎不會有任何驚訝，因為我們對它的預期是相當確定的。

事件發生的不確定性表明，隨機變數可能具有不同的值。只有在事件存在不確定性的情況下，通過觀察才能獲得資訊。<mark style="color:red;">如果一個事件發生的機率很高，它所傳遞的資訊就比較少，反之亦然</mark>。另一方面，需要更多的資訊來描述機率較低或更不確定的事件，或減少對這種事件發生的不確定性。<mark style="color:red;">同樣地，如果一個事件更確定會發生，那麼它的發生或觀察所傳遞的資訊就會更少，需要更少的資訊來描述它。這表明，一個事件的不確定性越大，其發生所傳遞的資訊就越多，或需要更多的資訊來描述它</mark>。這意味著熵、資訊、不確定性和驚奇之間存在著聯絡。

們可以根據發生的機率來衡量不確定性或其補充的確定性或資訊。如果$$p(x_i)=0.5$$，發生的不確定性將是最大的。應該注意的是，不確定性度量的分佈不應基於實驗的單一事件的發生，而應基於來自互斥事件集合的任何事件，其聯合體等於實驗或所有結果的集合。對事件集合的不確定性的衡量被稱為熵。因此，<mark style="color:red;">熵可以被解釋為在實驗之前對事件的不確定性的衡量</mark>。一旦進行了實驗，關於事件的結果是已知的，不確定性就被消除了。這意味著，實驗產生的關於事件的資訊等於事件集合的熵，意味著不確定性等於資訊。

現在問題來了。當兩個獨立的事件$$x$$和$$y$$以機率$$p_x$$和$$p_y$$發生時，如何計算資訊？$$x$$和$$y$$共同發生的機率是$$p_xp_y$$。符合前述邏輯的算法是從它們的共同發生中獲得的資訊，將是它們發生機率的倒數，即$$1/(p_xp_y)$$。這表明，此資訊不等於從事件$$x$$的發生中獲得的資訊$$1/p_x$$和從事件$$y$$的發生中獲得的資訊$$1/p_y$$之和。即$$\displaystyle \frac{1}{p_x p_y} \neq \frac{1}{p_x} + \frac{1}{p_y}$$。

上述可使用函數$$g(\cdot)$$使等號成立，即$$\displaystyle g\left(\frac{1}{p_x p_y} \right) =  g \left( \frac{1}{p_x}  \right) +  g\left( \frac{1}{p_y} \right)$$，而使等號成立的$$g(x)=-\log(x)$$，可得$$\displaystyle -\log\left(\frac{1}{p_x p_y} \right) =  -\log \left( \frac{1}{p_x}  \right) -\log \left( \frac{1}{p_y} \right)$$。

## 參考資料

* Vijay P. Singh,  "_Entropy theory and its application in environmental and water engineering," ch1,_ John Wiley & Sons, 2013.
