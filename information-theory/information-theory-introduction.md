# 資訊理論簡介

## 通訊系統

在通訊系統中，我們試圖把資訊在嘈雜的環境中從一個點傳遞到另一個點。 考慮下面的情境，一個秘書需要定期發送傳真，而她想要在每個頁面傳達盡可能多的資訊。她可選擇使用較小的字體，使得每個頁面有更多的字元。然而，系統中有兩個因素可能導致錯誤。

* 首先，傳真機的解析度是有限的。
* 第二，發送的字元可能由於接收到的雜訊而被錯誤地接收。

因此，如果字體大小太小，字元可能在傳真機上不易識別。另一方面，雖然有些字元傳真機可能無法識別，收件人仍能由上下文中的單詞辨認字元。換句話說，沒有必要選擇這樣的字體大小使得傳真機上所有的字元都可以識別。由此問題可得出：什麼是最有意義的信息量可以在一頁傳真上傳送?

在點對點通信模型中，訊息是由資訊源生成的。資訊由發射機轉換成適合傳輸的信號。在傳輸過程中，訊號可能受到噪音源的污染，因此接收的訊號可能與發送訊號不同。根據接收到的訊號，接收機對消息作出估計並將其發送到目的地。

在這種點對點通信系統的抽象模型中，人們只關心訊息源生成的消息是否能正確地發送到接收方，而不必擔心接收方如何實際使用消息。在某種程度上，Shannon的模型並不能涵蓋通信系統的所有可能方面。然而，為了建立一個精確而有用的資訊理論，理論的範圍必須加以限制。

![&#x9EDE;&#x5C0D;&#x9EDE;&#x901A;&#x8A0A;&#x6A21;&#x578B;](../.gitbook/assets/point-to-point-model-min.png)

Shannon從通訊的角度介紹了“資訊”的兩個基本概念。

* 首先，**資訊是不確定的**。更具體地說，如果我們感興趣的資訊是確定性的，那麼它根本就沒有價值，因為它已經是已知的，沒有不確定性。
  * 從這個角度來看，例如，電視廣播頻道上靜止圖像的連續傳輸是多餘的。
  * 因此，資訊源自然地被建模為隨機變數或隨機過程，和機率論用來發展資訊理論。
* 第二，**被傳輸的資訊是數位編碼的**。這意味著資訊源應該首先被轉換成一個0/1的位元\(bit\)，和剩下的任務是將接收器正確的接收這些資訊，且這些位元沒有實際意義。

**位元是測量資訊量的單位**。在Shannon的理論中，資訊是和長度，重量這些物理屬性一樣，是種可以測量和規範的東西。由於對於通信系統而言，其傳遞的資訊具有隨機性，所以定量描述資訊應基於隨機事件。任何資訊都存在冗余，冗餘大小與資訊中每個符號（數字、字母或單詞）的出現機率或者說不確定性有關。

通常一個訊號源發送出什麼符號是不確定的，衡量它可以根據其出現的機率來度量。機率大，出現機會多，不確定性小；反之就大。例如，當極限條件下，一個訊號源只發送一種符號，即發送內容是確定的，即機率為100%，此時接收方無法從接收訊號中獲得任何資訊，即資訊的量為零。而反之發送方和接收方約定，符號1代表二進位數字0，符號2代表二進位數字1，則接收端可以通過接收到的訊號，獲取一定資訊。

同時Shannon提出了用資訊熵\(information entropy\)來定量衡量資訊的大小。我們先設隨機事件發生的不確定性為發生機率π的函數f\(π\)，該函數具有如下三條性質：

* 單調性：機率 $$\pi$$ 越大的事件，資訊熵 $$f(\pi)$$ 越小，反之亦然。
* 非負性： $$f(\pi ) \geq 0$$ 。
* 可加性：多隨機事件同時發生存在的總不確定性的度量，可以表示為各時間不確定性度量的和。
  * 隨機事件 $$X = x_1$$ 與 $$Y = y_1$$同時發生的機率為 $$P(X=x_1, Y=y_1)=P(X=x_1)P(Y=y_1)$$ 
  * 而 $$f(\pi)$$ 應滿足 $$f(P(X=x_1, Y=y_1))=f(P(X=x_1))+f(P(Y=y_1))$$ 

從數學上證明滿足上述性質的資訊熵函數 $$H(X)$$ ，具有唯一的如下的形式。

$$
H(X)=-k\sum_{i=1}^N(P(x_i) \log(P(x_i))
$$





## 參考資料

* Shannon, Claude E. "A mathematical theory of communication." The Bell system technical journal 27.3 \(1948\): 379-423.

