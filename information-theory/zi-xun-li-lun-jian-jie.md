# 資訊理論簡介

## 簡介

訊息是很抽象的概念，難以定義。訊息有很強的主觀性和實用現，同樣的訊息對不同的人常常有不同的主觀價值和意義。例如天氣預報，對於在室用和戶外工作的人的意義不同。

訊息有以下的基本性質：

1. 訊息有可度量性。
2. 訊息有普遍性。
3. 訊息有可創造性。
4. 訊息有無限性。
5. 訊息有可傳遞性。
6. 訊息有相對性，對於同一事物，不同觀察者所得到的訊息量不同。
7. 訊息有可加工性，可被壓縮、增加抗干擾能力、加密等。
8. 訊息有可轉能性，在一定條件下可轉成物質、能量與時間。
9. 訊息有有序性，可用於消除系統的不確定性。
10. 訊息有共享性，同一訊息可被無限的人所獲得，不會使交流者失去原有的訊息。

## 資訊理論與編碼的研究

Shannon在1948年發表的A Mathematical Theory of Communication是資訊理論的公認起源。

資訊理論或通訊系統的抽象模型如下圖：

![通訊系統抽象模型](../.gitbook/assets/686px-Shannon\_communication\_system.svg.png)

在所有的資訊交流系統中，通有一個發出訊息的發送端(信源)，至少有一個接收端(信宿)，以及訊息流通的通道(信道)。在訊息傳送過程中，一定會有噪音源發出噪音干擾訊息。為了把訊息放入信道中傳輸，所以需要先將訊息編碼，在傳送到信宿後，進行解碼還原訊息。



## 資訊理論

資訊理論回答了通信理論中的兩個基本問題：

* 什麼是最終的數據壓縮(答案：熵 H),以及
* 什麼是通信的最終傳輸速率(答案：信道容量 C)。

因此一些人認為資訊理論是通訊理論的一個子集。事實上,它在統計物理學(熱力學)、計算機科學(Kolmogorov 複雜性或算法複雜度)、統計推斷(奧卡姆剃刀：最簡單的解釋是最好的) 以及機率和統計(誤差指數最優假設檢驗和估計)都有貢獻。

在 1940 年代初期，人們認為不可能以可忽略的錯誤概率以正值速率發送信息。 Shannon 證明了對於低於信道容量的所有通信速率之錯誤機率幾乎為零，這讓通信理論界感到驚訝。

信道容量可以簡單地從信道的噪聲特性中計算出來。Shannon進一步認為，諸如音樂和語音之類的隨機過程具有不可簡化的複雜性，低於該複雜性信號就無法被壓縮。他將此命名為熵(entropy)，以尊重該詞在熱力學中的平行使用，並認為如果熵源小於信道容量時，可以實現漸近無差錯(asymptotically error-free)通信。

![資訊理論做為通訊理論的極限點](../.gitbook/assets/mutual\_information\_rate-min.png)

今天的資訊理論代表了所有可能的通信方案集合的極端點。資料壓縮最小值$$I(X: \hat{X})$$ 位於通信方案集合的一個極端。所有資料壓縮方案，都需要說明壓縮率至少等于这個最小值(註：互資訊的最小值為兩變數共有的資訊，無法再被壓縮)。另一个極端是資料傳輸最大值$$I(X;Y)$$，被稱為信道容量。因此,所有的調變方案和資料壓縮方案都位於這些極限之間。

資訊理論論還提出了實現這些通信極限的方法。然而,這些理論上最優的通信方案，儘管它們很漂亮，但在計算上可能會變得不切實際。Turbo碼的出現最終實現了計算的實用性。應用資訊理論根怷的一個很好的例子是在CD和DVD上使用糾錯碼。

### Kolmogorov 複雜性

Kolmogorov、Chaitin 和 Solomonoff 提出了 這樣的想法，<mark style="color:red;">即資料字串的複雜性可以通過計算字串的最短二進制軟體程式的長度來定義</mark>。因此，複雜度是最小描述長度(minimum description length)。這種複雜性的定義是獨立於計算機，並且具有根本的重要性。因此Kolmogorov 複雜性為描述性複雜性理論奠定了基礎。&#x20;

如果序列是從具有熵$$H$$ 的分佈中隨機抽取時，則Kolmogorov複雜度$$K$$大約等於Shannen熵$$H$$。因此，資訊理論和 Kolmogorov 複雜度之間的聯繫是完美的。Kolmogorov複雜性比Shannon熵更基本。

可以將計算複雜度(時間複雜度) 和 Kolmogorov 複雜度(程式長度或描述複雜度) 視為座標上的兩個軸，對應於程式執行時間和程式長度。 Kolmogorov 複雜度側重於沿第二軸最小化，而計算複雜度側重於第一軸最小化。











## 參考資料

* Cover, Thomas M. Elements of information theory, 2nd. John Wiley & Sons, 2012.
