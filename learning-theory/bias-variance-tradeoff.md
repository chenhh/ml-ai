# Bias-Variance Tradeoff

## 問題簡介

* 過度擬合\(Overfitting\)：找出來的模型受到訓練資料的影響太大，使得對預測的效果不佳。可由訓練集的低誤差與測試集的高誤差中觀察得到。
* 低度\(欠\)擬合\(Underfitting\)：模型對於資料的描述能力太差，無法正確解釋資料。可由訓練集與測試集均為高誤差觀察得到。

偏差\(Bias\)就是我們模型的準度，我們希望模型越準越好，也就是希望Bias降很低，但此時我們又會遇到另一個問題，要是模型針對我們訓練集資料學得很好，就會降低對其他資料的準度，也就是變異\(Variance\)會提高，所以我們在做的是一個取捨，用過度擬合與低度擬合的角度來看的話：

* 過度擬合：試圖去降低偏差，到最後偏差變很小，然而變異卻提高許多。
* 低度擬合：偏差很高，準度不足。

低偏差的模型在訓練集合上更加準確，低變異的模型在不同的訓練集合上性能更加穩定。舉兩個極端的例子：

* 記住訓練集合上所有資料的答案\(類別\)，這樣的系統是低偏差、高變異。
* 無論輸入什麼資料，總是預測一個相同的答案\(類別\)，這樣的系統是高偏差、低變異。

因此在模型的選擇上需要進行偏差和變異的權衡。



![&#x504F;&#x5DEE;&#x8207;&#x8B8A;&#x7570;&#x6578;&#x7684;&#x5F71;&#x97FF;](../.gitbook/assets/bias-and-variance_orig-min.png)

顯然複雜的模型能更好的擬合訓練集合能更好的擬合訓練集合上的點，但是同時高複雜度的模型泛化能力差，造成了高變異。

下圖中橫坐標的右側是過度擬合的情況，而左側是低度擬合的情況。過度擬合可以這麼解釋：

* 訓練樣本得到的輸出和其期望的輸出總誤差很小，但是測試樣本得到的輸出和其期望的輸出誤差卻很大。
* 因此為了得到一致的輸出誤差，使得模型變得相當複雜。

想像某個學習算法產生了一個過度擬合的分類器，這個分類器能夠百分之百正確地分類樣本資料，但也就為了能夠對樣本完全正確的分類，使得它的構造如此複雜，規則如此嚴格，以至於任何與樣本數據稍有不同的資料它全都認為不屬於這個類別\(低泛化能力\)。

![&#x6A21;&#x578B;&#x8907;&#x96DC;&#x5EA6;&#x5C0D;&#x9810;&#x6E2C;&#x80FD;&#x529B;&#x7684;&#x5F71;&#x97FF;](../.gitbook/assets/bias-variance-tradeoff-min.png)

![&#x6A21;&#x578B;&#x8907;&#x96DC;&#x5EA6;&#x5C0D;&#x9810;&#x6E2C;&#x504F;&#x5DEE;-&#x8B8A;&#x7570;&#x6578;&#x7684;&#x5F71;&#x97FF;](../.gitbook/assets/bias-variance-tradeoff.jpg)

## 訓練誤差與測試誤差

![&#x904E;&#x5EA6;&#x8207;&#x4F4E;&#x5EA6;&#x64EC;&#x5408;&#x65BC;&#x6A21;&#x578B;&#x9810;&#x6E2C;&#x7684;&#x7D50;&#x679C;](../.gitbook/assets/over-under-fitting-min.png)

### 低度擬合

當訓練一個模型時，**若發現不論是在訓練集或是測試集資料都無法達到一定的準度時，就可能是遇到低度擬合的狀況**。通常造成低度擬合的主要原因包含『訓練時間不足』、『模型複雜度不足』，而這兩種狀況都不難解決。可以透過增加訓練迭代的次數來解決「訓練時間不足」的問題，透過調整參數數量來解決「模型複雜度不足」的問題。如果上述調整仍然無法改善，可能就要考慮是訓練資料本身的問題。

### 過度擬合

當隨個訓練的時間增長、迭代的次數增加，我們訓練集與測試集的誤差都會逐步的下降，但當我們觀察到訓練集與測試集的誤差開始分道揚鑣時，就可能是過度擬合狀況發生。

**用一句話描述過度擬合：模型過度去學習、硬背訓練資料**。

而造成過度擬合的主要原因包含：

* 訓練資料不足：當我們的訓練集太小時，模型找不到泛化的特徵，因此就頃向去硬記所擁有的資訊。
  * 解法：最直接的辦法就是區收集或爬取更多樣性的訓練資料。
* 迭代次數過多：當我們不斷讓模型去學習訓練資料集時，到了最後模型就會試圖去硬背特徵，而大大降低了泛化能力。

  * 可以透過設置Early Stopping來解決，Early Stopping其實就是透過觀察測試資料集Loss的變化來停止訓練，並透過調整 patience決定容忍度，假如我們設置patience=10，也就是說當測試資料集的Loss在10個epoch後都沒有下降，此時就要停止訓練。
  * Dropout為神經網路中常用的技巧，隨機關閉NN層中的一定比例神經元\(讓其值為0\)，藉此降低模型對各個神經元的依賴性。
  * Regularizer不管是在ML/DL中都時常會被使用，其原理是在損失函數後加上懲罰項。當模型在收斂時，若發現某個特徵比較有用時，最直接的作法就是提高這個特徵的權重，而為了避免模型過度依賴這個特徵，我們就在損失函數上加上這個特徵的權重，當模型試圖提高這個權重時，也會因此提高損失，藉此抑制這個權重的大幅度起伏。Regularizer又可以分為L1\(Lasso\)、L2\(Ridge\)，通常來說我們會使用L2 Regularization，表現較穩定也比較不會造成權重歸0的狀況。



* 模型複雜度太高：當我們使用一個很強的模型去學習時，模型較容易發生過擬合的狀況。



