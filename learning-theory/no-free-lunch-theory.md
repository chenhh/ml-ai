# No Free Lunch Theory

## 簡介

在瞭解機器學習的概念後，通常會想到，如果能有一個可以處理所有問題的演算法，那麼就可以解決所有問題，那就能省去大量人力，大家都不用工作了。這是一個尋找通用演算法的問題，只要能有一個超強的演算法，那就能很快地製造出符合不同需求的機器人。

但是已經有人用了數學證明了並不存在一個能一統天下的演算法模型，這就是 NFL\(No Free Lunch Theorems\) 沒有免費的午餐定理。

**NFL簡單的說：如果一演演算法對於某個類型的問題比另外的演算法效率高，那麼它一定不具有普適性（即一定存在另外一類問題使得這個演算法的效率低於隨機搜尋）。即每個演算法都有自己適用的問題的范圍。**

因此NFL定理讓我們清楚地認識到，脫離具體問題，空泛地談論「什麼學習演算法更好」毫無意義，因為若考慮所有潛在的問題，則所有的演算法一樣好。要談論演算法的相對優劣，必須要針對具體問題；在某些問題上表現好的學習演算法，在另一問題上卻可能不盡如人意，學習演算法自身的歸納偏好與問題是否相配，往往會起到決定性作用。

NFL 定理有兩個：

* No Free Lunch for Supervised Machine Learning \(WOLPERT, David H., 1996. The lack of a priori distinctions between learning algorithms. Neural Computation, 8\(7\), 1341–1390.\)
* No Free Lunch for Search/Optimization \(WOLPERT, David H., and William G. MACREADY, 1997. No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation, 1\(1\), 67–82.\)。

實際上在了解到這個定理的概念後，我們要知道，**在不考慮具體問題的情況下，沒有任何一個算法比另一個算法更優**，甚至直接胡亂猜測還會更好。**但如果針對某個具體的特定的問題，確實可找到表現比較好的機器學習演算法，但這個演算法，卻無法解決其他的問題。**

也可以說如果我們對要解決的問題一無所知，且並假設其分佈完全隨機且平等，那麼任何演算法的預期性能都是相似的。在某個領域、特定假設下表現卓越的演算法，不一定在另一個領域也能是最厲害的。正因如此，我們才需要研究和發明更多的機器學習算法來處理不同的假設和數據，也就是處理不同的問題。

NFL for Machine Learning 有兩條規則：

* 沒有一個機器學習演算法，在所有可能的函數中，能夠比隨機猜測的結果更好。
* 每個機器學習演算法都必須包含一些資料之外的知識或者假設，才能夠將資料一般化。

NFL告訴我們，沒有哪個演算法比其他演算法高效。但這並不是說這些研究沒有意義，因為在實際應用中，有些演算法就是明顯比另一些高效，這並不是錯覺。因為NFL的前提是考慮所有可能的目標函數，而實際生活中只會遇到其中極少一部分，這裡面是有free lunch的。所以要說它的意義的話，不是說研究最佳化演算法沒有意義，而是提醒我們要轉換思路，從針對的問題集合（或者分佈）出發去設計和改進演算法。



## 參考資料

* [https://www.no-free-lunch.org/](https://www.no-free-lunch.org/)
* 


