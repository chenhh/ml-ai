# 統計量\(statistics\)

## 統計量\(statistics\)

> 令$$X_1,X_2\ldots,X_N$$ 為一由感興趣的母體所得大小為$$N$$的隨機樣本，即$$X_1,X_2, \ldots,X_N$$為獨立同分佈的樣本
>
>  一統計量為一個由樣本\(向量\)$$X=\{X_1,X_2, \ldots,X_N\}$$ 與某個已知常數所形成的函數，亦為一隨機變數，可作為母體參數\(population parameters\)的推論之用。
>
> 向量$$X$$的一實值函數$$\mathrm{T}(X)=\mathrm{T}(X_1,X_2,\ldots ,X_N ) \in \mathbb{R}$$稱為$$X$$的統計量。

*  統計量是對樣本的加工，好的統計量應該能夠將樣本中關於母體分佈的未知資訊盡可能集中起來。
* 若要研究某參數分佈族中的未知參數，為此抽取了一組樣本，樣本中所包含的資訊可分為兩類:
  * 關於未知參數的資訊。
  * 以及關於樣本結構的資訊等其它資訊。
* **統計量是樣本的不帶任何未知量\(如參數\)的函數**。而函數就是一種「濃縮」訊息的動作，因此統計量中所包含的資訊，通常比整個樣本資料所包含的來得少。
* 統計量仍為一隨機變數，因為其值會隨$$X$$的實現而變動。
* 統計量為一泛函\(functional\)，因為$$X$$在連續的狀況下為一函數，而上述定義只是取連續函數的觀測值而為離散數列。
* **一統計量的機率分佈稱作此統計量的抽樣分佈（sample distribution）**。  從實務觀點，此分佈可作為經由重複取樣所得的統計量的可能觀察值之相對頻率圖得出。

## 充分統計量\(sufficient statistics\)

> 假設隨機變數$$X$$之機率分佈與參數（向量）$$\theta$$有關。若給定一統計量 $$T(X)$$之後，$$X$$之條件分佈與$$\theta$$無關（does not depend on $$\theta$$）， 則$$T(X)$$稱為$$\theta$$的一充分統計量。
>
> * $$\mathrm{P}(X=x|T(X)=t, \theta)=\mathrm{P}(X=x|T(X)=t)$$或
> * $$\mathrm{P} (\theta|T(X)=t, X=x) = \mathrm{P}(\theta|X=x)$$或
> * $$\mathrm{P}(\theta, X=x|T(X)=t)=\mathrm{P}(\theta|T(X)=t) \mathrm{P}(X=x|T(X)=t)$$

一個隨機變數的機率分佈，可能取決於一些分佈參數的值（如常態分佈取決於$$(\mu, \sigma^2)$$ ，Poisson分佈取決於$$\lambda$$，均為充分統計量）。**而充分統計量，則能夠完全捕捉這些參數所包含的關於分佈的信息**。也就是說，如果知道充分統計量的值，那麼這個隨機變數關於它的條件分佈，不再取決於原來參數的。

 Fisher在1922年提出充分統計量的概念，其以估計的目的而言，充分統計量等價於原始的資料，因此好的估計量只須基於充分統計量。之後此概念擴充為不論是估計、檢定以及信賴區間等參數的推論，均只須基於充分統計量。

* **充分統計量不唯一且不一定存在**。
* 知道充分統計量後，因為其完備保留資料的特性，因此不再需要其它資料（隨機變數的實現值）。  或者說某些統計量，可保留資料中有用的資訊，在去除掉不相干的資料後，不會影響對參數的推估；這類統計量可達到資料縮減（data reduction）的目的，並使資料不損失者，稱為充分統計量。
* 若$$T(\cdot)$$為$$\theta$$的充分統計量，則知道$$T(X)$$之後，$$X$$的變化與$$\theta$$無關，也就是樣本中不再含有更多關於$$\theta$$的資料；剩餘的資料中，對於了解$$\theta$$不具價值。
* 以資訊理論的說法就是給定$$T(X)$$之後，$$X$$與$$\theta$$的互資訊為0，即$$\mathrm{I}(X;\theta|T(X))=0$$。
* 由於充分統計量中仍可能保有多餘的資料，因此把多餘的資料刪去後，可得**最小充分統計量**。  將樣本加工成統計量要求越簡化越好，**最簡化的充分統計量叫最小充分統計量**。統計量的另一個重要的基本概念是完全統計量，**完全充分統計量是最小充分統計量**。











