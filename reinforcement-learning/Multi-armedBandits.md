---
description: Multi-armedBandits
---

# 多臂吃角子老虎機

## 簡介

<mark style="color:red;">強化學習區別於其他型別學習的最重要特徵是，它使用的訓練資訊是對改採取的行動進行評估，而不是通過給出正確的行動進行指導</mark>。這就是創造主動探索的需要，明確地尋找好的行為。純粹的評價性反饋表明改採取的行動有多好，但不表明它是最好還是最差的行動。

另一方面，純粹的指導性反饋指出了應該採取的正確行動，與實際採取的行動無關。這種反饋是監督學習的基礎，包括模式分類、人工神經網路和系統識別的大部分內容。在它們的純粹形式中，這兩種反饋是截然不同的：<mark style="color:blue;">評價性反饋完全取決於改採取的行動，而指導性反饋則與改採取的行動無關</mark>。

## k臂吃角子老虎機問題(k-armed bandit problem)

考慮以下學習問題：決策者反復面臨著在$$k$$個不同的選項，或行動中的選擇。每次選擇之後，決策者都會收到一個數值獎勵（註：只知道所選的行動的獎勵，沒有選擇的行動不知道其獎勵），這個獎勵是從一個固定的機率分佈中選擇的，取決於你所選擇的行動。你的目標是在某個時間段內使預期的總報酬最大化。

這是$$k$$臂吃角子老虎機問題的原始形式，因類似於老虎機而得名，只不過它有$$k$$個槓桿而不是一個。<mark style="color:blue;">每一個行動選擇就像老虎機的一個槓桿，而獎勵就是擊中大獎的報酬</mark>。通過反復的行動選擇，你要把你的行動集中在最好的槓桿上，使你的贏利最大化。另一個比喻是，醫生在一系列重病患者的實驗性治療中進行選擇。每個行動都是對治療方法的選擇，而每個獎勵都是病人的生存或福祉。今天，「吃角子老虎機問題」一詞有時被用於上述問題的概括。

在吃角子老虎機問題中，$$k$$個行動都有一個預期的或平均的獎勵，可稱為該行動的價值(value)。我們用$$A_t$$(變數)表示在時間步驟t上選擇的行動，用$$R_t$$(變數)表示相應行動的報酬。 那麼，任意行動$$a$$(實現值)的價值可表示為$$q_{*}(a)$$，是指在$$a$$被選中的情況下的預期報酬：

$$q_{*}(a) \equiv \mathrm{E}(R_t ~|~ A_t = a)$$

如果你知道每個行動的價值，那麼解決吃角子老虎機問題將很簡單：你將總是選擇價值最高的行動。但一般無法確切估計地知道行動的價值。我們把行動$$a$$在時間$$t$$的估計值表示為$$Q_t(a)$$，且希望$$Q_t(a)$$能夠逼近$$q_{*}(a)$$。

### 利用與探索

如果你保持對行動價值的估計，那麼在任何時間點，至少有一個行動的估計值是最大的。我們稱這些為<mark style="color:blue;">貪婪的行動 (greedy action)</mark>。

* 當你選擇這些行動之一時，我們說你是在利用(exploiting)你對行動價值的現有知識。
* 如果你選擇了一個非貪婪的行動，那麼我們說你是在探索(exploring)，因為這使你能夠提高你對非貪婪行動價值的估計。
* 為了使一步的預期報酬最大化，利用是正確的，但從長遠來看，探索可能產生更大的總報酬。

例如，假設一個貪婪的行動的價值是確定的，而其他幾個行動被估計為幾乎一樣好，但有很大的不確定性。這種不確定性使得這些其他行動中至少有一個可能實際上比貪婪行動更好，但你不知道是哪一個。如果你有很多時間可進行行動選擇，那麼探索非貪婪行動並發現其中哪些行動比貪婪行動更好，可能會更好。

在探索過程中，短期回報較低，但長期回報較高，因為在你發現更好的行動後，你可以多次利用它們。<mark style="color:blue;">由於不可能既探索又利用任何單一的行動選擇，人們經常提到探索和利用之間的「沖突」</mark>。​

在任何具體情況下，是探索好還是利用好，以復雜的方式取決於估計的精確值、不確定性和剩餘時間步驟的數量。

對於吃角子老虎機和相關問題的特定數學公式，<mark style="color:blue;">有許多復雜的方法來平衡探索和開發。 然而，這些方法大多對靜止性和先驗知識做了強有力的假設，這些假設在應用中和我們在後續章節中考慮的全部強化學習問題中要麼被違反，要麼無法驗證</mark>。當這些方法的理論假設不適用時，對這些方法的最優性或有界損失的保證就沒有什麼用處。

在本書中，我們並不擔心以一種復雜的方式來平衡探索和利用；我們只擔心平衡它們的問題。平衡探索和利用的需要是強化學習中出現的一個獨特的挑戰。

## 行動-價值方法(action-value method)

我們首先要更仔細地研究估計行動價值和使用估計值來做出行動選擇決策的方法，這些方法我們統稱為<mark style="color:red;">行動價值法</mark>。

回顧一下，一個行動的真實價值是該行動被選中時的平均獎勵。估算的一個自然方法是對實際收到的獎勵進行平均化：

* $$\begin{aligned} Q_t(a)&=\frac{\text{時間t之前採取行動a的報酬總和} }{\text{時間t之前採取行動a的次數} } \\ &= \frac{\sum_{i=1}^{t-1} R_i \cdot \mathrm{I}(A_i = a) }{\sum_{i=1}^{t-1} \mathrm{I}(A_i = a) } \end{aligned}$$
* 若分母為0時，令$$Q_t(a)=0$$。​
* <mark style="color:blue;">註：此方法在行動集合很大時，必須要有很多參考的資料才能得到較好的估計值</mark>。

最簡單的行動選擇規則是選擇具有最高估計值的行動之一，也就是上一節中定義的貪婪行動之一。如果有一個以上的貪婪行動，那麼就以某種任意的方式，也許是隨機的方式在這些行動中進行選擇。我們把這種<mark style="color:red;">貪婪行動選擇方法</mark>寫成：

$$\displaystyle A_t \equiv \argmax_a Q_t(a)$$

貪婪的行動選擇總是利用當前的知識來最大化眼前的回報；它根本不花時間去抽查明顯較差的行動，看看它們是否真的會更好。

一個簡單的替代方案是在大多數時候表現得很貪婪，但每隔一段時間，比如說以小機率$$\epsilon$$，而從所有的行動中隨機選擇概率相同的行動，與行動價值估計無關。我們把使用這種近乎貪婪的行動選擇規則的方法稱為「$$\epsilon$$-<mark style="color:red;">貪婪方法</mark>」。

這些方法的一個優點是，在極限情況下，隨著步驟數的增加，每一個行動都會被實行無限次，從而確保所有的$$Q_t(a)$$都收斂到$$q_{*}(a)$$。這當然意味著選擇最優行動的機率會收斂到大於$$1-\epsilon$$，然而，這些只是漸進的保證，對於這些方法的實際有效性沒有說明什麼。

## 10臂吃角子老虎機的測試結果

以為下$$k=10$$的2000次隨機測試。

## 價值函數估計式的漸近實作

到目前為止，我們所討論的行動值方法都是將行動值作為觀察到的獎勵的樣本平均數來估計。我們現在要討論的問題是，這些平均值如何能夠以一種高效的計算方式來計算，特別是在恆定記憶體和恆定每步時間計算的情況下。

考慮單一行動，為了簡化符號，令$$R_i$$為第$$i$$​次選擇該行動的報酬，$$Q_n$$​為該行動已被選擇$$n-1$$​次後的估計價值，即：

$$Q_n\equiv \frac{R_1 + R_2 + \dots + R_{n-1}}{n}$$

上式為有批次資料時的計算方式，如果資料是逐漸在線進入時，計算方法可改為：

$$\begin{aligned} Q_{n+1} & = \frac{1}{n} \sum_{i=1}^n R_i \\ 	& = \frac{1}{n } (R_n + (n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} R_i) \\ 	& = \frac{1}{n } (R_n + (n-1) Q_n) \\ 	& = Q_n + \frac{1}{n} (R_n - Q_n) \end{aligned}$$

因此可得$$\epsilon$$-貪婪的吃角子老虎機演算法的虛擬碼如下：

```python
for a = 1 to k:
    Q(a) = 0  # 行動a的價值函數估計值
    N(a) = 0  # 行動a被執行的次數
	

while(true){
    A = argmax_{a} Q(a) with probability 1 - e 
        or a random action with probability e
    R = bandit(A)  # 以行動A玩老虎機，得到報酬R
    N(A) = N(A) + 1
    Q(A) = Q(A) + 1/N(A)[R-Q(A)]
}
```

## 追踨非定態問題(tracking a non-stationary problem)

上述討論的平均方法適合獎勵機率不隨時間變化的老虎機問題。<mark style="color:red;">而我們經常遇到的強化學習問題實際上是非穩態的。在這種情況下，給最近的獎勵比給過去的獎勵更多的權重是有意義的</mark>。

### 固定權重加權平均

最流行的方法之一是使用一個恆定的步長引數。例如，用於更新$$n-1$$個過去獎勵的平均值$$Q_n$$的增量更新規則被修改為：

* $$Q_{n+1} = Q_n  + \alpha [R_n - Q_n], ~ \alpha \in (0, 1]$$
* $$\alpha$$為固定的常數。

因此$$Q_{n+1}$$為過去報酬的加權平均值如下。

$$\begin{aligned} Q_{n+1} & = Q_n  + \alpha [R_n - Q_n]  \\ 	& = \alpha R_n + (1-\alpha) Q_n \\ 	& = \alpha R_n + (1-\alpha )[\alpha R_{n-1} + (1-\alpha )Q_{n-1}]  \\ 	& = \alpha R_n + (1-\alpha) \alpha R_{n-1} + (1-\alpha)^2 Q_{n-1} \\ 	& \dots \\ 	& = (1-\alpha)^n Q_1 + \sum_{i=1}^n \alpha (1-\alpha)^{n-i}R_i  \end{aligned}$$

因為$$(1-\alpha)^n  + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}=1$$，所以稱為加權平均。事實上，權重是按照指數$$1-\alpha$$衰減的。因此，這有時被稱為<mark style="color:red;">指數化的時間加權平均值</mark>。

### 變動權重加權平均

令$$\alpha_n(a)$$表示用於處理第$$n$$次選擇行動$$a$$後收到的獎勵的步長參數。當$$\alpha_n(a)=\frac{1}{n}$$時退化為簡單平均法。但當然不能保證對序列$$\{\alpha_n(a)\}$$的所有選擇都能收斂。

隨機近似理論中一個著名的結果給我們<mark style="color:red;">保證機率收斂所需的條件為</mark>：

$$\begin{aligned}  & \displaystyle \sum_{n=1}^\infty \alpha_n(a) = \infty \\  & \sum_{n=1}^\infty \alpha_n^2 (a) < \infty \end{aligned}$$

第一個條件是為了保證步長足夠大，以最終克服任何初始條件或隨機波動。第二個條件保證最終步長變得足夠小，以保證收斂。

請注意，這兩個收斂條件在樣本平均的情況下成立$$\alpha_n(a)=\frac{1}{n}$$，但在恆定步長引數的情況下不成立$$\alpha_n(a)=\alpha$$。

<mark style="color:red;">在後一種情況下，第二個條件沒有得到滿足，這表明估計值從未完全收斂，而是繼續隨著最近收到的獎勵而變化</mark>。正如我們上面提到的，這在非平穩環境中實際上是可取的，而實際上非平穩的問題是強化學習中最常見的。

<mark style="color:blue;">此外，滿足上述條件的步長引數序列往往收斂得很慢，或者需要相當的調整才能獲得滿意的收斂率</mark>。盡管滿足這些收斂條件的步長引數序列經常被用於理論工作中，但它們很少被用於應用和實證研究中。

## 樂觀的行動價值函數初始值

到目前為止，我們所討論的所有方法都在一定程度上依賴於初始行動值的估計，$$Q_1(a)$$。

用統計學的語言來說，這些方法都因其初始估計值而有偏差。對於樣本平均法來說，一旦所有的行動都被選擇了至少一次，這種偏差就會消失，但是對於常數$$\alpha$$的方法來說，這種偏差是永久性的，盡管隨著時間的推移而減少影響。<mark style="color:blue;">在實踐中，這種偏差通常不是一個問題，有時可能非常有幫助。缺點是初始估計值在中成為一組必須由用戶挑選的參數，如果只是為了將它們全部設置為零的話</mark>。好處是，它們提供了一種簡單的方法來提供一些關於可以預期的獎勵水平的預先知識。

初始動作值也可以作為鼓勵探索的一種簡單方式來使用。假設我們沒有像在10臂老虎機中那樣將初始行動值設置為零，而是將它們全部設置為+5。回顧一下，這個問題中的$$q_{*}(a)$$是從均值為0、變異數為1的常態分佈中選出的。因此，+5的初始估計是非常樂觀的。但這種樂觀主義鼓勵了行動價值方法的探索。無論最初選擇哪種行動，獎勵都小於初始估計值；學習者切換到其他行動，對它所得到的獎勵感到 "失望"。其結果是，在價值估計收斂之前，所有的行動都要嘗試幾次。即使一直選擇貪婪的行動，該系統也會進行相當數量的探索。

## 信心上限的行動選擇(Upper-Confidence-Bound Action Selection)

之所以需要探索，是因為行動-價值估計的准確性總是存在不確定性。$$\epsilon$$-貪婪的行動選擇迫使非貪婪的行動被嘗試，但不加區分，對那些接近貪婪的或特別不確定的行動沒有偏好。

<mark style="color:red;">更好的做法是根據非貪婪行動的實際最優潛力來選擇它們，同時考慮到它們的估計值離最大值有多近以及這些估計值的不確定性</mark>。這樣做的一個有效方法是根據以下因素選擇行動：

* $$A_t = \argmax_a\left\{   Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}  \right\}$$
* $$N_t(a)$$在時間$$t$$(不包含$$t$$​)之前，行動$$a$$被選中的次數。
* 當$$N_t(a)=0$$ , $$a$$被定義為有最大值的行動。
* $$c >0$$為控制探索強度的參數。

這種置信度上限（UCB）行動選擇的想法是，平方根項是對行動$$a$$的估計值的不確定性或變異數的衡量。因此，被最大化的數量是一種對行動$$a$$的可能真實值的上限，$$c$$決定了信心水平。

* 每次選擇行動$$a$$時，不確定性大概都會減少。因為$$N_t(a)$$遞增，並且由於它出現在分母中，不確定性項減少了。
* 另一方面，每次選擇$$a$$以外的行動時，$$t$$增加，但$$N_t(a)$$不增加；
* 因為$$t$$出現在分子中，不確定性估計增加。使用自然對數意味著隨著時間的推移，增幅會越來越小，但卻是無限制的；
* 所有的行動最終都會被選擇，<mark style="color:red;">但價值估計較低的行動，或者已經被頻繁選擇的行動，會隨著時間的推移，被選擇的頻率越來越低</mark>。

UCB通常表現良好，但比「$$\epsilon$$-貪婪方法」更難超越老虎機問題擴展到普遍的強化學習環境。另一個困難是處理非定態問題；需要更復雜的方法。另一個困難是處理大的狀態空間，特別是函數近似法時。使得UCB在實務上並不好用。

## Gradient Bandit Algorithms(梯度老虎機算法)

。在本節中，我們考慮為每個行動$$a$$學習一個數字的偏好$$H_t(a)$$。偏好越大，該行動就越經常被採取，但該偏好在獎勵方面沒有影響。只有一個行動對另一個行動的相對偏好是重要的；如果我們把所有的行動偏好都加到1000，就不會對行動機率產生影響，這些機率是根據軟最大分佈（即Gibbs或Boltzmann分佈）確定的，如下所示：

* $$\displaystyle \mathrm{P}(A_t=a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)$$
* $$\pi_t(a)$$為時間$$t$$​時選擇行動$$a$$​的機率。
* 初始時，所有行動的偏好均相同，即$$\forall a, ~H_1(a)=0$$​，此時所有行動被選擇的機率均相同。

## 參考資料

* Richard Suttion and Andrew G. Barto, "Reinforcement Learning: An Introduction," 2nd, 2018, chapter 2.
