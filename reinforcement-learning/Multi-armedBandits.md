---
description: Multi-armedBandits
---

# 多臂吃角子老虎機

## 簡介

<mark style="color:red;">強化學習區別於其他型別學習的最重要特徵是，它使用的訓練資訊是對改採取的行動進行評估，而不是通過給出正確的行動進行指導</mark>。這就是創造主動探索的需要，明確地尋找好的行為。純粹的評價性反饋表明改採取的行動有多好，但不表明它是最好還是最差的行動。

另一方面，純粹的指導性反饋指出了應該採取的正確行動，與實際採取的行動無關。這種反饋是監督學習的基礎，包括模式分類、人工神經網路和系統識別的大部分內容。在它們的純粹形式中，這兩種反饋是截然不同的：<mark style="color:blue;">評價性反饋完全取決於改採取的行動，而指導性反饋則與改採取的行動無關</mark>。

## k臂吃角子老虎機問題(k-armed bandit problem)

考慮以下學習問題：決策者反復面臨著在$$k$$個不同的選項，或行動中的選擇。每次選擇之後，決策者都會收到一個數值獎勵（註：只知道所選的行動的獎勵，沒有選擇的行動不知道其獎勵），這個獎勵是從一個固定的機率分佈中選擇的，取決於你所選擇的行動。你的目標是在某個時間段內使預期的總報酬最大化。

這是$$k$$臂吃角子老虎機問題的原始形式，因類似於老虎機而得名，只不過它有$$k$$個槓桿而不是一個。<mark style="color:blue;">每一個行動選擇就像老虎機的一個槓桿，而獎勵就是擊中大獎的報酬</mark>。通過反復的行動選擇，你要把你的行動集中在最好的槓桿上，使你的贏利最大化。另一個比喻是，醫生在一系列重病患者的實驗性治療中進行選擇。每個行動都是對治療方法的選擇，而每個獎勵都是病人的生存或福祉。今天，「吃角子老虎機問題」一詞有時被用於上述問題的概括。

在吃角子老虎機問題中，$$k$$個行動都有一個預期的或平均的獎勵，可稱為該行動的價值(value)。我們用$$A_t$$表示在時間步驟t上選擇的行動，用Rt表示相應的報酬。 那麼，任意行動a的價值，表示為q⇤(a)，是指在a被選中的情況下的預期報酬

## 參考資料

* Richard Suttion and Andrew G. Barto, "Reinforcement Learning: An Introduction," 2nd, 2018, chapter 2.
