---
description: Multi-armedBandits
---

# 多臂吃角子老虎機

## 簡介

<mark style="color:red;">強化學習區別於其他型別學習的最重要特徵是，它使用的訓練資訊是對改採取的行動進行評估，而不是通過給出正確的行動進行指導</mark>。這就是創造主動探索的需要，明確地尋找好的行為。純粹的評價性反饋表明改採取的行動有多好，但不表明它是最好還是最差的行動。

另一方面，純粹的指導性反饋指出了應該採取的正確行動，與實際採取的行動無關。這種反饋是監督學習的基礎，包括模式分類、人工神經網路和系統識別的大部分內容。在它們的純粹形式中，這兩種反饋是截然不同的：<mark style="color:blue;">評價性反饋完全取決於改採取的行動，而指導性反饋則與改採取的行動無關</mark>。

## k臂吃角子老虎機問題(k-armed bandit problem)

考慮以下學習問題：決策者反復面臨著在$$k$$個不同的選項，或行動中的選擇。每次選擇之後，決策者都會收到一個數值獎勵（註：只知道所選的行動的獎勵，沒有選擇的行動不知道其獎勵），這個獎勵是從一個固定的機率分佈中選擇的，取決於你所選擇的行動。你的目標是在某個時間段內使預期的總報酬最大化。

這是$$k$$臂吃角子老虎機問題的原始形式，因類似於老虎機而得名，只不過它有$$k$$個槓桿而不是一個。<mark style="color:blue;">每一個行動選擇就像老虎機的一個槓桿，而獎勵就是擊中大獎的報酬</mark>。通過反復的行動選擇，你要把你的行動集中在最好的槓桿上，使你的贏利最大化。另一個比喻是，醫生在一系列重病患者的實驗性治療中進行選擇。每個行動都是對治療方法的選擇，而每個獎勵都是病人的生存或福祉。今天，「吃角子老虎機問題」一詞有時被用於上述問題的概括。

在吃角子老虎機問題中，$$k$$個行動都有一個預期的或平均的獎勵，可稱為該行動的價值(value)。我們用$$A_t$$(變數)表示在時間步驟t上選擇的行動，用$$R_t$$(變數)表示相應行動的報酬。 那麼，任意行動$$a$$(實現值)的價值可表示為$$q_{*}(a)$$，是指在$$a$$被選中的情況下的預期報酬：

$$q_{*}(a) \equiv \mathrm{E}(R_t ~|~ A_t = a)$$

如果你知道每個行動的價值，那麼解決吃角子老虎機問題將很簡單：你將總是選擇價值最高的行動。但一般無法確切估計地知道行動的價值。我們把行動$$a$$在時間$$t$$的估計值表示為$$Q_t(a)$$，且希望$$Q_t(a)$$能夠逼近$$q_{*}(a)$$。

### 利用與探索

如果你保持對行動價值的估計，那麼在任何時間點，至少有一個行動的估計值是最大的。我們稱這些為<mark style="color:blue;">貪婪的行動 (greedy action)</mark>。

* 當你選擇這些行動之一時，我們說你是在利用(exploiting)你對行動價值的現有知識。
* 如果你選擇了一個非貪婪的行動，那麼我們說你是在探索(exploring)，因為這使你能夠提高你對非貪婪行動價值的估計。
* 為了使一步的預期報酬最大化，利用是正確的，但從長遠來看，探索可能產生更大的總報酬。

例如，假設一個貪婪的行動的價值是確定的，而其他幾個行動被估計為幾乎一樣好，但有很大的不確定性。這種不確定性使得這些其他行動中至少有一個可能實際上比貪婪行動更好，但你不知道是哪一個。如果你有很多時間可進行行動選擇，那麼探索非貪婪行動並發現其中哪些行動比貪婪行動更好，可能會更好。

在探索過程中，短期回報較低，但長期回報較高，因為在你發現更好的行動後，你可以多次利用它們。<mark style="color:blue;">由於不可能既探索又利用任何單一的行動選擇，人們經常提到探索和利用之間的「沖突」</mark>。​

在任何具體情況下，是探索好還是利用好，以復雜的方式取決於估計的精確值、不確定性和剩餘時間步驟的數量。

對於吃角子老虎機和相關問題的特定數學公式，<mark style="color:blue;">有許多復雜的方法來平衡探索和開發。 然而，這些方法大多對靜止性和先驗知識做了強有力的假設，這些假設在應用中和我們在後續章節中考慮的全部強化學習問題中要麼被違反，要麼無法驗證</mark>。當這些方法的理論假設不適用時，對這些方法的最優性或有界損失的保證就沒有什麼用處。

在本書中，我們並不擔心以一種復雜的方式來平衡探索和利用；我們只擔心平衡它們的問題。平衡探索和利用的需要是強化學習中出現的一個獨特的挑戰。

## 參考資料

* Richard Suttion and Andrew G. Barto, "Reinforcement Learning: An Introduction," 2nd, 2018, chapter 2.
