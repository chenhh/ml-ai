# 強化學習介紹

## 簡介

<mark style="color:blue;">當我們思考學習的本質時，我們可能首先想到的是通過與環境的互動來學習</mark>。鍛煉這種互動會產生大量關於原因和後果的資訊，關於行動的後果，以及關於為了實現目標應該怎麼做。在我們的一生中，這種互動無疑是我們對環境和自身知識的主要來源。無論我們是在學習駕駛汽車還是進行對話，我們都敏銳地意識到我們的環境是如何對我們的行為做出反應的，並且我們試圖通過我們的行為來影響所發生的事情。<mark style="color:blue;">從互動中學習是幾乎所有學習和智慧理論的基礎思想</mark>。

我們用動力系統理論的思想來正式確定強化學習的問題，<mark style="color:red;">具體來說，就是對不完全已知的馬可夫決策過程(MDP)的最佳控制</mark>。基本的想法是簡單地捕捉一個學習代理所面臨的實際問題的最重要方面，隨著時間的推移與環境互動以實現目標。<mark style="color:blue;">一個學習型代理必須能夠在某種程度上感知其環境的狀態，並且必須能夠採取影響狀態的行動。代理人還必須有一個或多個與環境狀態有關的目標</mark>。MDP旨在包括這三個方面--感覺、行動和目標--以其最簡單的形式，而不使其中任何一個瑣碎化。任何適合解決此類問題的方法，我們都認為是一種強化學習方法。

### 與監督式學習(supervised learning)的區別

<mark style="color:blue;">監督學習是指從一個有知識的外部監督者提供的有標簽的例子的訓練集中學</mark>習。每個例子都是對一種情況的描述，以及系統對這種情況應該採取的正確行動的說明--標簽，這通常是為了確定該情況所屬的類別。

這種學習的目的是讓系統推斷或概括其反應，以便在訓練集中不存在的情況下正確行事。這是一種重要的學習方式，但僅憑這一點還不足以從互動中學習。在互動問題中，要獲得既正確又能代表代理人必須採取行動的所有情況的期望行為的例子往往是不實際的。在未知的領域--人們期望學習是最有益的--<mark style="color:red;">代理人必須能夠從它自己的經驗中學習</mark>。

### 與非監督式學習(unsupervised learning)的區別

強化學習也不同於無監督學習，後者通常是尋找隱藏在未標記資料集合中的結構。雖然人們可能會認為強化學習是一種無監督學習，因為它不依賴於正確行為的例子，<mark style="color:red;">但強化學習是試圖使獎勵訊號最大化，而不是試圖找到隱藏的結構</mark>。揭示代理人經驗中的結構在強化學習中當然是有用的，但其本身並不能解決最大化獎勵訊號的強化學習問題。因此，我們認為強化學習是第三種機器學習正規化，與監督學習和無監督學習以及其他正規化並列。

### 探索和利用(exploration and exploitation)的平衡

強化學習中出現的一個挑戰，是探索和利用之間的權衡。<mark style="color:blue;">為了獲得大量的獎勵，強化學習代理必須傾向於它在過去嘗試過的行動，並且發現這些行動在產生獎勵方面是有效的。但為了發現這些行動，它必須嘗試以前沒有選擇過的行動</mark>。代理人必須利用它已經經歷過的東西來獲得獎勵，但它也必須探索，以便在未來做出更好的行動選擇。

無論是探索還是利用，都不能完全追求，否則就會失敗的任務。代理人必須嘗試各種行動，並逐步傾向於那些似乎是最好的行動。在一個隨機的任務中，每個行動都必須被多次嘗試，以獲得對其預期回報的可靠估計。數學家們對探索-開發困境進行了幾十年的深入研究，但仍未得到解決。平衡探索和利用的整個問題沒有出現在監督和無監督學習的正規型式中。

### 代理人(agent)

強化學習的另一個關鍵特徵是，<mark style="color:blue;">它明確考慮了目標導向的代理人與不確定環境互動的整個問題</mark>。這與許多考慮子問題而不解決它們如何融入大局的方法形成對比。例如，我們已經提到，許多機器學習研究關注的是監督學習，而沒有明確說明這種能力最終將如何發揮作用。其他研究人員開發了具有一般目標的規劃理論，但沒有考慮規劃在實時決策中的作用，也沒有考慮規劃所需的預測模型從何而來的問題。盡管這些方法產生了許多有用的結果，但它們對孤立的子問題的關注是一個重要的限制。

強化學習採取了相反的做法，從一個完整的、互動的、尋求目標的代理人開始。<mark style="color:blue;">所有的強化學習代理人都有明確的目標，可以感知其環境的各個方面，並可以選擇行動來影響其環境。此外，通常從一開始就假設代理人必須在它所面臨的環境有很大不確定性的情況下進行操作</mark>。

### 強化學習的應用

* 一個國際象棋高手如何走下一步。這種選擇既是通過規劃預測可能的回應和反饋，也是通過對特定位置和步驟的可取性的直接、直觀的判斷。
* 一個自適應控制器實時調整一個工廠的執行引數。該控制器在指定的邊際成本基礎上，最佳化產量/成本/質量的權衡，而不嚴格遵守工程師最初建議的設定點。
* 一隻小羚羊在出生幾分鐘後就掙扎著站起來。半小時後，它就 以每小時20英裡的速度奔跑。
* 掃地機器人決定它是應該進入一個新的房間尋找更多的垃圾來收集，還是開始嘗試找到回到電池充電站的路。它做出決定的依據是其電池當前的充電水平，以及它在過去能多快多容易地找到充電站。

這些例子的共同特點是非常基本的，以至於很容易被忽視。<mark style="color:red;">所有的例子都涉及到一個主動決策的代理人和它的環境之間的互動，在這種互動中，盡管環境不確定，但代理人仍試圖實現一個目標</mark>。

<mark style="color:red;">代理人的行動被允許影響環境的未來狀態（例如，下一個棋子的位置、煉油廠的儲油罐水平、機器人的下一個位置和其電池的未來充電水平），從而影響代理人在以後可利用的行動和機會</mark>。正確的選擇需要考慮到<mark style="color:blue;">行動的間接、延遲的後果</mark>，因此可能需要預知或計畫。同時，在所有這些例子中，行動的影響不能被完全預測；因此，代理人必須經常監測其環境並作出適當的反應。

<mark style="color:red;">所有這些例子涉及的目標都是明確的，即代理人可以根據它能直接感知的東西來判斷其目標的進展</mark>。棋手知道他是否贏了，煉油廠控制器知道正在生產多少石油，小羚羊知道它什麼時候跌倒，移動機器人知道它的電池什麼時候用完。

<mark style="color:red;">在所有這些例子中，代理人可以利用其經驗隨著時間的推移提高其效能</mark>。棋手完善了他用來評估位置的直覺，從而改善了他的棋藝；羚羊小腿提高了它的奔跑效率。

## 強化學習的重要元素

除了<mark style="color:red;">代理人(agent)</mark>和環境之外，我們可以確定強化學習系統的四個主要子元素：

* <mark style="color:red;">策略(policy)</mark>
* <mark style="color:red;">獎勵訊號(reward signal)</mark>
* <mark style="color:red;">價值函式(value function)</mark>
* <mark style="color:red;">環境模型(model of environment)(非必要)</mark>

<mark style="color:red;">策略定義了代理人在特定時間的行為方式</mark>。粗略地說，策略是一種從環境的感知狀態到處於這些狀態時要採取的行動的對映。它對應於心理學中所謂的一組刺激-反應規則或關聯。在某些情況下，策略可能是一個簡單的函式或查詢表，而在其他情況下，它可能涉及廣泛的計算，如搜尋過程。策略是強化學習代理的核心，因為它本身就足以決定行為。一般來說，策略可能是隨機的，每個行動有不同的機率。

<mark style="color:red;">獎勵訊號定義了強化學習問題的目標</mark>。在每個時間步驟中，環境向強化學習代理傳送一個稱為獎勵的單一數字。代理人的唯一目標是使其在長期內收到的<mark style="color:blue;">總獎勵最大化</mark>。因此，獎勵訊號定義了什麼是代理的好事件和壞事件。在一個生物系統中，我們可以認為獎勵類似於快樂或痛苦的體驗。它們是代理人所面臨的問題的直接和決定性特徵。獎勵訊號是改變策略的主要依據；如果策略所選擇的行動之後的獎勵很低，那麼就可以改變策略，在未來那種情況下選擇其他行動。一般來說，獎勵訊號可能是環境狀態和改採取的行動的隨機函式。

<mark style="color:red;">獎賞訊號表明什麼是眼前的好事(短期)，而價值函式則規定什麼是長期的好事</mark>。粗略地說，一個狀態的價值是一個代理人從該狀態開始，在未來可以期望積累的獎勵總量。獎勵決定了環境狀態的直接的、內在的可取性，而價值則表明了在考慮到可能會出現的狀態和這些狀態中可獲得的獎勵後，狀態的長期可取性。例如，一個狀態可能總是產生低的即時獎勵，但仍然有高的價值，因為它經常被其他產生高獎勵的狀態所跟隨。或者相反的情況也可能是真的。

獎勵在某種意義上是主要的，而作為對獎勵的預測，價值是次要的。沒有獎賞就沒有價值，而估計價值的唯一目的是為了獲得更多的獎賞。然而，<mark style="color:red;">在做決定和評估決定時，我們最關心的還是價值</mark>。行動選擇是基於價值判斷而做出的。<mark style="color:red;">我們尋求能夠帶來最高價值狀態的行動，而不是最高的回報，因為這些行動從長遠來看能夠為我們獲得最大的回報</mark>。

不幸的是，<mark style="color:blue;">確定價值比確定獎勵要難得多。獎勵基本上是由環境直接給予的，但價值必須從代理人在其整個生命週期中的觀察序列中進行估計和重新估計</mark>。事實上，我們所考慮的幾乎所有強化學習演算法的最重要組成部分是一種有效估計數值的方法。價值估計的核心作用可以說是過去六十年來人們在強化學習方面學到的最重要的東西。

一些強化學習系統的最後一個要素是一個環境模型。這是一種模仿環境行為的東西，或者更廣泛地說，它允許對環境的行為方式進行推斷。例如，給定一個狀態和行動，模型可以預測結果的下一個狀態和下一個獎勵。模型被用於規劃，我們指的是在實際經歷之前通過考慮未來可能的情況來決定行動方案的任何方式。<mark style="color:blue;">解決使用模型和規劃的強化學習問題的方法被稱為基於模型的方法，與之相對的是更簡單的無模型方法，後者是明確的試錯學習者，被視為幾乎與規劃相反</mark>。現代強化學習跨越了從低層次的試錯學習到高層次的慎重規劃的范圍。

### 限制與範圍

強化學習在很大程度上依賴於<mark style="color:red;">狀態(state)</mark>的概念，它是策略和價值函式的輸入，同時也是模型的輸入和輸出。<mark style="color:red;">非正式地講，我們可以把狀態看作是一個訊號，它向代理人傳達了某一特定時間的 "環境如何 "的感覺</mark>。然而，更廣泛地說，把狀態看作是代理人可以得到的關於其環境的任何資訊。實際上，我們假設狀態訊號是由一些預處理系統產生的，這些預處理系統在名義上是代理人環境的一部分。

我們不涉及構建、改變或學習狀態訊號的問題。採取這種方法並不是因為我們認為狀態表示是不重要的，而是為了充分關注決策問題。換句話說，我們<mark style="color:blue;">關注的不是設計狀態訊號，而是決定根據現有的任何狀態訊號採取何種行動</mark>。

我們在本書中考慮的大多數強化學習方法都是圍繞著估計價值函式(value function)而展開的，但嚴格來說，解決強化學習問題並不需要這樣做。例如，遺傳演算法、遺傳程式設計、模擬退火和其他最佳化方法等解決方法從不估計價值函式。<mark style="color:blue;">這些方法應用多個靜態策略，每個策略都在很長一段時間內與環境的單獨例項相互作用。獲得最多獎勵的策略，以及它們的隨機變化，被帶到下一代政策中，這個過程不斷重復</mark>。我們稱這些為進化方法，因為它們的操作類似於生物進化產生具有熟練行為的生物體的方式，即使它們在各自的生命中沒有學習。如果策略的空間足夠小，或者可以結構化，以便好的策略是常見的或容易找到的，或者如果有大量的時間可用於搜尋，那麼進化方法可以是有效的。

此外，進化方法在學習代理不能感知其環境的完整狀態的問題上也有優勢。我們的重點是強化學習方法，這些方法在與環境互動的同時進行學習，而進化方法不這樣做。在許多情況下，能夠利用個體行為互動的細節的方法比進化方法要有效得多。進化方法忽略了強化學習問題的許多有用的結構：他們沒有利用他們正在尋找的策略是一個從狀態到行動的函式這一事實；他們沒有注意到個體在其生命週期中經過哪些狀態，或者它選擇哪些行動。在某些情況下，這種資訊可能會產生誤導（例如，當狀態被誤解時），但更多的時候，它應該能使搜尋更有效率。雖然進化和學習有許多共同的特點，並且自然地一起工作，但我們認為進化方法本身並不特別適合強化學習問題。
