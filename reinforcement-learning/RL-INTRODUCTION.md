# 強化學習介紹

## 簡介

<mark style="color:blue;">當我們思考學習的本質時，我們可能首先想到的是通過與環境的互動來學習</mark>。鍛煉這種互動會產生大量關於原因和後果的資訊，關於行動的後果，以及關於為了實現目標應該怎麼做。在我們的一生中，這種互動無疑是我們對環境和自身知識的主要來源。無論我們是在學習駕駛汽車還是進行對話，我們都敏銳地意識到我們的環境是如何對我們的行為做出反應的，並且我們試圖通過我們的行為來影響所發生的事情。<mark style="color:blue;">從互動中學習是幾乎所有學習和智慧理論的基礎思想</mark>。

我們用動力系統理論的思想來正式確定強化學習的問題，<mark style="color:red;">具體來說，就是對不完全已知的馬可夫決策過程(MDP)的最佳控制</mark>。基本的想法是簡單地捕捉一個學習代理所面臨的實際問題的最重要方面，隨著時間的推移與環境互動以實現目標。<mark style="color:blue;">一個學習型代理必須能夠在某種程度上感知其環境的狀態，並且必須能夠採取影響狀態的行動。代理人還必須有一個或多個與環境狀態有關的目標</mark>。MDP旨在包括這三個方面--感覺、行動和目標--以其最簡單的形式，而不使其中任何一個瑣碎化。任何適合解決此類問題的方法，我們都認為是一種強化學習方法。

### 與監督式學習(supervised learning)的區別

<mark style="color:blue;">監督學習是指從一個有知識的外部監督者提供的有標簽的例子的訓練集中學</mark>習。每個例子都是對一種情況的描述，以及系統對這種情況應該採取的正確行動的說明--標簽，這通常是為了確定該情況所屬的類別。

這種學習的目的是讓系統推斷或概括其反應，以便在訓練集中不存在的情況下正確行事。這是一種重要的學習方式，但僅憑這一點還不足以從互動中學習。在互動問題中，要獲得既正確又能代表代理人必須採取行動的所有情況的期望行為的例子往往是不實際的。在未知的領域--人們期望學習是最有益的--<mark style="color:red;">代理人必須能夠從它自己的經驗中學習</mark>。

### 與非監督式學習(unsupervised learning)的區別

強化學習也不同於無監督學習，後者通常是尋找隱藏在未標記資料集合中的結構。雖然人們可能會認為強化學習是一種無監督學習，因為它不依賴於正確行為的例子，<mark style="color:red;">但強化學習是試圖使獎勵訊號最大化，而不是試圖找到隱藏的結構</mark>。揭示代理人經驗中的結構在強化學習中當然是有用的，但其本身並不能解決最大化獎勵訊號的強化學習問題。因此，我們認為強化學習是第三種機器學習正規化，與監督學習和無監督學習以及其他正規化並列。

### 探索和利用(exploration and exploitation)的平衡

強化學習中出現的一個挑戰，是探索和利用之間的權衡。<mark style="color:blue;">為了獲得大量的獎勵，強化學習代理必須傾向於它在過去嘗試過的行動，並且發現這些行動在產生獎勵方面是有效的。但為了發現這些行動，它必須嘗試以前沒有選擇過的行動</mark>。代理人必須利用它已經經歷過的東西來獲得獎勵，但它也必須探索，以便在未來做出更好的行動選擇。

無論是探索還是利用，都不能完全追求，否則就會失敗的任務。代理人必須嘗試各種行動，並逐步傾向於那些似乎是最好的行動。在一個隨機的任務中，每個行動都必須被多次嘗試，以獲得對其預期回報的可靠估計。數學家們對探索-開發困境進行了幾十年的深入研究，但仍未得到解決。平衡探索和利用的整個問題沒有出現在監督和無監督學習的正規型式中。

### 代理人(agent)

強化學習的另一個關鍵特徵是，<mark style="color:blue;">它明確考慮了目標導向的代理人與不確定環境互動的整個問題</mark>。這與許多考慮子問題而不解決它們如何融入大局的方法形成對比。例如，我們已經提到，許多機器學習研究關注的是監督學習，而沒有明確說明這種能力最終將如何發揮作用。其他研究人員開發了具有一般目標的規劃理論，但沒有考慮規劃在實時決策中的作用，也沒有考慮規劃所需的預測模型從何而來的問題。盡管這些方法產生了許多有用的結果，但它們對孤立的子問題的關注是一個重要的限制。

強化學習採取了相反的做法，從一個完整的、互動的、尋求目標的代理人開始。<mark style="color:blue;">所有的強化學習代理人都有明確的目標，可以感知其環境的各個方面，並可以選擇行動來影響其環境。此外，通常從一開始就假設代理人必須在它所面臨的環境有很大不確定性的情況下進行操作</mark>。

### 強化學習的應用

* 一個國際象棋高手如何走下一步。這種選擇既是通過規劃預測可能的回應和反饋，也是通過對特定位置和步驟的可取性的直接、直觀的判斷。
* 一個自適應控制器實時調整一個工廠的執行引數。該控制器在指定的邊際成本基礎上，最佳化產量/成本/質量的權衡，而不嚴格遵守工程師最初建議的設定點。
* 一隻小羚羊在出生幾分鐘後就掙扎著站起來。半小時後，它就 以每小時20英裡的速度奔跑。
* 掃地機器人決定它是應該進入一個新的房間尋找更多的垃圾來收集，還是開始嘗試找到回到電池充電站的路。它做出決定的依據是其電池當前的充電水平，以及它在過去能多快多容易地找到充電站。

這些例子的共同特點是非常基本的，以至於很容易被忽視。<mark style="color:red;">所有的例子都涉及到一個主動決策的代理人和它的環境之間的互動，在這種互動中，盡管環境不確定，但代理人仍試圖實現一個目標</mark>。

<mark style="color:red;">代理人的行動被允許影響環境的未來狀態（例如，下一個棋子的位置、煉油廠的儲油罐水平、機器人的下一個位置和其電池的未來充電水平），從而影響代理人在以後可利用的行動和機會</mark>。正確的選擇需要考慮到<mark style="color:blue;">行動的間接、延遲的後果</mark>，因此可能需要預知或計畫。同時，在所有這些例子中，行動的影響不能被完全預測；因此，代理人必須經常監測其環境並作出適當的反應。

<mark style="color:red;">所有這些例子涉及的目標都是明確的，即代理人可以根據它能直接感知的東西來判斷其目標的進展</mark>。棋手知道他是否贏了，煉油廠控制器知道正在生產多少石油，小羚羊知道它什麼時候跌倒，移動機器人知道它的電池什麼時候用完。

<mark style="color:red;">在所有這些例子中，代理人可以利用其經驗隨著時間的推移提高其效能</mark>。棋手完善了他用來評估位置的直覺，從而改善了他的棋藝；羚羊小腿提高了它的奔跑效率。

## 強化學習的重要元素

除了<mark style="color:red;">代理人(agent)</mark>和環境之外，我們可以確定強化學習系統的四個主要子元素：

* <mark style="color:red;">策略(policy)</mark>
* <mark style="color:red;">獎勵訊號(reward signal)</mark>
* <mark style="color:red;">價值函式(value function)</mark>
* <mark style="color:red;">環境模型(model of environment)(非必要)</mark>

<mark style="color:red;">策略定義了代理人在特定時間的行為方式</mark>。粗略地說，策略是一種從環境的感知狀態到處於這些狀態時要採取的行動的對映。它對應於心理學中所謂的一組刺激-反應規則或關聯。在某些情況下，策略可能是一個簡單的函式或查詢表，而在其他情況下，它可能涉及廣泛的計算，如搜尋過程。策略是強化學習代理的核心，因為它本身就足以決定行為。一般來說，策略可能是隨機的，每個行動有不同的機率。

<mark style="color:red;">獎勵訊號定義了強化學習問題的目標</mark>。在每個時間步驟中，環境向強化學習代理傳送一個稱為獎勵的單一數字。代理人的唯一目標是使其在長期內收到的<mark style="color:blue;">總獎勵最大化</mark>。因此，獎勵訊號定義了什麼是代理的好事件和壞事件。在一個生物系統中，我們可以認為獎勵類似於快樂或痛苦的體驗。它們是代理人所面臨的問題的直接和決定性特徵。獎勵訊號是改變策略的主要依據；如果策略所選擇的行動之後的獎勵很低，那麼就可以改變策略，在未來那種情況下選擇其他行動。一般來說，獎勵訊號可能是環境狀態和改採取的行動的隨機函式。

<mark style="color:red;">獎賞訊號表明什麼是眼前的好事(短期)，而價值函式則規定什麼是長期的好事</mark>。粗略地說，一個狀態的價值是一個代理人從該狀態開始，在未來可以期望積累的獎勵總量。獎勵決定了環境狀態的直接的、內在的可取性，而價值則表明了在考慮到可能會出現的狀態和這些狀態中可獲得的獎勵後，狀態的長期可取性。例如，一個狀態可能總是產生低的即時獎勵，但仍然有高的價值，因為它經常被其他產生高獎勵的狀態所跟隨。或者相反的情況也可能是真的。

獎勵在某種意義上是主要的，而作為對獎勵的預測，價值是次要的。沒有獎賞就沒有價值，而估計價值的唯一目的是為了獲得更多的獎賞。然而，<mark style="color:red;">在做決定和評估決定時，我們最關心的還是價值</mark>。行動選擇是基於價值判斷而做出的。<mark style="color:red;">我們尋求能夠帶來最高價值狀態的行動，而不是最高的回報，因為這些行動從長遠來看能夠為我們獲得最大的回報</mark>。

不幸的是，<mark style="color:blue;">確定價值比確定獎勵要難得多。獎勵基本上是由環境直接給予的，但價值必須從代理人在其整個生命週期中的觀察序列中進行估計和重新估計</mark>。事實上，我們所考慮的幾乎所有強化學習演算法的最重要組成部分是一種有效估計數值的方法。價值估計的核心作用可以說是過去六十年來人們在強化學習方面學到的最重要的東西。

一些強化學習系統的最後一個要素是一個環境模型。這是一種模仿環境行為的東西，或者更廣泛地說，它允許對環境的行為方式進行推斷。例如，給定一個狀態和行動，模型可以預測結果的下一個狀態和下一個獎勵。模型被用於規劃，我們指的是在實際經歷之前通過考慮未來可能的情況來決定行動方案的任何方式。<mark style="color:blue;">解決使用模型和規劃的強化學習問題的方法被稱為基於模型的方法，與之相對的是更簡單的無模型方法，後者是明確的試錯學習者，被視為幾乎與規劃相反</mark>。現代強化學習跨越了從低層次的試錯學習到高層次的慎重規劃的范圍。

### 限制與範圍

強化學習在很大程度上依賴於<mark style="color:red;">狀態(state)</mark>的概念，它是策略和價值函式的輸入，同時也是模型的輸入和輸出。<mark style="color:red;">非正式地講，我們可以把狀態看作是一個訊號，它向代理人傳達了某一特定時間的 "環境如何 "的感覺</mark>。然而，更廣泛地說，把狀態看作是代理人可以得到的關於其環境的任何資訊。實際上，我們假設狀態訊號是由一些預處理系統產生的，這些預處理系統在名義上是代理人環境的一部分。

我們不涉及構建、改變或學習狀態訊號的問題。採取這種方法並不是因為我們認為狀態表示是不重要的，而是為了充分關注決策問題。換句話說，我們<mark style="color:blue;">關注的不是設計狀態訊號，而是決定根據現有的任何狀態訊號採取何種行動</mark>。

我們在本書中考慮的大多數強化學習方法都是圍繞著估計價值函式(value function)而展開的，但嚴格來說，解決強化學習問題並不需要這樣做。例如，遺傳演算法、遺傳程式設計、模擬退火和其他最佳化方法等解決方法從不估計價值函式。<mark style="color:blue;">這些方法應用多個靜態策略，每個策略都在很長一段時間內與環境的單獨例項相互作用。獲得最多獎勵的策略，以及它們的隨機變化，被帶到下一代政策中，這個過程不斷重復</mark>。我們稱這些為進化方法，因為它們的操作類似於生物進化產生具有熟練行為的生物體的方式，即使它們在各自的生命中沒有學習。如果策略的空間足夠小，或者可以結構化，以便好的策略是常見的或容易找到的，或者如果有大量的時間可用於搜尋，那麼進化方法可以是有效的。

此外，進化方法在學習代理不能感知其環境的完整狀態的問題上也有優勢。我們的重點是強化學習方法，這些方法在與環境互動的同時進行學習，而進化方法不這樣做。在許多情況下，能夠利用個體行為互動的細節的方法比進化方法要有效得多。進化方法忽略了強化學習問題的許多有用的結構：他們沒有利用他們正在尋找的策略是一個從狀態到行動的函式這一事實；他們沒有注意到個體在其生命週期中經過哪些狀態，或者它選擇哪些行動。在某些情況下，這種資訊可能會產生誤導（例如，當狀態被誤解時），但更多的時候，它應該能使搜尋更有效率。雖然進化和學習有許多共同的特點，並且自然地一起工作，但我們認為進化方法本身並不特別適合強化學習問題。

## 範例：井字游戲

考慮一下我們熟悉的兒童游戲井字游戲。兩個玩家輪流在一個三乘三的棋盤上玩。一個人玩X，另一個人玩O，直到有一個人在水平、垂直或對角線上連放三個標記即贏得比賽。如果棋盤填滿了，雙方都沒有連續得到三個標記，那麼游戲就是平局。因為一個熟練的棋手可以做到永遠不輸，讓我們假設我們是在和一個不完美的棋手對弈，他的下法有時是不正確的，而讓我們贏。事實上，就目前而言，讓我們認為平局和輸棋對我們同樣不利。我們如何構建一個能夠發現對手棋局中的不完美之處，並學會將其獲勝的機會最大化的棋手？

雖然這是一個簡單的問題，但它不能輕易地通過經典技術得到滿意的解決。

* 例如，賽局理論中的經典 "最小化 "解決方案在這裡是不正確的，因為它假定了對手的一種特定的游戲方式。例如，一個最小化的玩家永遠不會達到一個可能會輸的游戲狀態，即使事實上它總是因為對手的不正確游戲而從該狀態中獲勝。
* 順序決策問題的經典最佳化方法，如動態規劃，可以計算出任何對手的最優解，但需要輸入該對手的完整規格，包括對手在每個棋盤狀態下的每一步棋的機率。

讓我們假設這個問題沒有先驗的資訊，因為絕大多數的實際問題都沒有這種資訊。另一方面，這種資訊可以從經驗中估計出來，在這種情況下，可以通過與對手下許多棋來估計。<mark style="color:blue;">在這個問題上，我們能做的最好的事情就是首先學習一個對手的行為模型，達到一定的置信度，然後應用動態規劃來計算一個給定的近似對手模型的最優解</mark>。這與我們研究的一些強化學習方法沒有什麼區別。

應用於這個問題的進化方法將直接在可能的策略空間中搜尋一個具有高機率贏得對手的政策。在這裡，策略是一個規則，它告訴棋手在游戲的每一種狀態下應該走什麼棋。對於所考慮的每一個策略，通過與對手進行一些游戲，可以獲得對其獲勝機率的估計。然後，這一評估將指導下一步考慮哪種或哪幾種策略。一個典型的進化方法將在策略空間中爬坡，連續生成和評估策略，試圖獲得增量改進。

下面是如何用價值函式的方法來處理井字游戲問題的。首先，我們將建立一個數字表，每個游戲的可能狀態都有一個表。每個數字都是對我們在該狀態下獲勝機率的最新估計。我們將這一估計值視為該狀態的價值，而整個表格就是所學的價值函式。

假設我們總是玩X，那麼對於所有連續有三個X的狀態，獲勝的機率是1，因為我們已經贏了。同樣地，對於所有連續有三個Os的狀態，或者被填滿的狀態，正確的機率是0，因為我們不能贏得遊戲。我們將所有其他狀態的初始值設為0.5，代表我們有50%的獲勝機會的猜測。然後我們與對手進行多次遊戲。為了選擇我們的棋步，我們檢查我們每一步可能的棋步所產生的狀態（棋盤上的每個空白處都有一個），並在表中查詢它們的當前值。大多數時候，我們都是貪婪地下棋，選擇有最大價值的狀態的棋步，也就是說，具有最高的估計獲勝機率。然而，偶爾我們也會從其他棋子中隨機選擇。這些被稱為探索性的行動，因為它們使我們體驗到我們可能永遠不會看到的狀態。



## 參考資料

* Richard Suttion and Andrew G. Barto, "Reinforcement Learning: An Introduction," 2nd, 2018, chapter 1.
