# 強化學習介紹

## 簡介

<mark style="color:blue;">當我們思考學習的本質時，我們可能首先想到的是通過與環境的互動來學習</mark>。鍛煉這種互動會產生大量關於原因和後果的資訊，關於行動的後果，以及關於為了實現目標應該怎麼做。在我們的一生中，這種互動無疑是我們對環境和自身知識的主要來源。無論我們是在學習駕駛汽車還是進行對話，我們都敏銳地意識到我們的環境是如何對我們的行為做出反應的，並且我們試圖通過我們的行為來影響所發生的事情。<mark style="color:blue;">從互動中學習是幾乎所有學習和智慧理論的基礎思想</mark>。

我們用動力系統理論的思想來正式確定強化學習的問題，<mark style="color:red;">具體來說，就是對不完全已知的馬可夫決策過程(MDP)的最佳控制</mark>。基本的想法是簡單地捕捉一個學習代理所面臨的實際問題的最重要方面，隨著時間的推移與環境互動以實現目標。<mark style="color:blue;">一個學習型代理必須能夠在某種程度上感知其環境的狀態，並且必須能夠採取影響狀態的行動。代理人還必須有一個或多個與環境狀態有關的目標</mark>。MDP旨在包括這三個方面--感覺、行動和目標--以其最簡單的形式，而不使其中任何一個瑣碎化。任何適合解決此類問題的方法，我們都認為是一種強化學習方法。

### 與監督式學習(supervised learning)的區別

<mark style="color:blue;">監督學習是指從一個有知識的外部監督者提供的有標簽的例子的訓練集中學</mark>習。每個例子都是對一種情況的描述，以及系統對這種情況應該採取的正確行動的說明--標簽，這通常是為了確定該情況所屬的類別。

這種學習的目的是讓系統推斷或概括其反應，以便在訓練集中不存在的情況下正確行事。這是一種重要的學習方式，但僅憑這一點還不足以從互動中學習。在互動問題中，要獲得既正確又能代表代理人必須採取行動的所有情況的期望行為的例子往往是不實際的。在未知的領域--人們期望學習是最有益的--<mark style="color:red;">代理人必須能夠從它自己的經驗中學習</mark>。

### 與非監督式學習(unsupervised learning)的區別

強化學習也不同於無監督學習，後者通常是尋找隱藏在未標記資料集合中的結構。雖然人們可能會認為強化學習是一種無監督學習，因為它不依賴於正確行為的例子，<mark style="color:red;">但強化學習是試圖使獎勵訊號最大化，而不是試圖找到隱藏的結構</mark>。揭示代理人經驗中的結構在強化學習中當然是有用的，但其本身並不能解決最大化獎勵訊號的強化學習問題。因此，我們認為強化學習是第三種機器學習正規化，與監督學習和無監督學習以及其他正規化並列。

### 探索和利用(exploration and exploitation)的平衡

強化學習中出現的一個挑戰，是探索和利用之間的權衡。<mark style="color:blue;">為了獲得大量的獎勵，強化學習代理必須傾向於它在過去嘗試過的行動，並且發現這些行動在產生獎勵方面是有效的。但為了發現這些行動，它必須嘗試以前沒有選擇過的行動</mark>。代理人必須利用它已經經歷過的東西來獲得獎勵，但它也必須探索，以便在未來做出更好的行動選擇。

無論是探索還是利用，都不能完全追求，否則就會失敗的任務。代理人必須嘗試各種行動，並逐步傾向於那些似乎是最好的行動。在一個隨機的任務中，每個行動都必須被多次嘗試，以獲得對其預期回報的可靠估計。數學家們對探索-開發困境進行了幾十年的深入研究，但仍未得到解決。平衡探索和利用的整個問題沒有出現在監督和無監督學習的正規型式中。

### 代理人(agent)

強化學習的另一個關鍵特徵是，<mark style="color:blue;">它明確考慮了目標導向的代理人與不確定環境互動的整個問題</mark>。這與許多考慮子問題而不解決它們如何融入大局的方法形成對比。例如，我們已經提到，許多機器學習研究關注的是監督學習，而沒有明確說明這種能力最終將如何發揮作用。其他研究人員開發了具有一般目標的規劃理論，但沒有考慮規劃在實時決策中的作用，也沒有考慮規劃所需的預測模型從何而來的問題。盡管這些方法產生了許多有用的結果，但它們對孤立的子問題的關注是一個重要的限制。

強化學習採取了相反的做法，從一個完整的、互動的、尋求目標的代理人開始。<mark style="color:blue;">所有的強化學習代理人都有明確的目標，可以感知其環境的各個方面，並可以選擇行動來影響其環境。此外，通常從一開始就假設代理人必須在它所面臨的環境有很大不確定性的情況下進行操作</mark>。

### 強化學習的應用

* 一個國際象棋高手如何走下一步。這種選擇既是通過規劃預測可能的回應和反饋，也是通過對特定位置和步驟的可取性的直接、直觀的判斷。
* 一個自適應控制器實時調整一個工廠的執行引數。該控制器在指定的邊際成本基礎上，最佳化產量/成本/質量的權衡，而不嚴格遵守工程師最初建議的設定點。
* 一隻小羚羊在出生幾分鐘後就掙扎著站起來。半小時後，它就 以每小時20英裡的速度奔跑。
* 掃地機器人決定它是應該進入一個新的房間尋找更多的垃圾來收集，還是開始嘗試找到回到電池充電站的路。它做出決定的依據是其電池當前的充電水平，以及它在過去能多快多容易地找到充電站。

這些例子的共同特點是非常基本的，以至於很容易被忽視。<mark style="color:red;">所有的例子都涉及到一個主動決策的代理人和它的環境之間的互動，在這種互動中，盡管環境不確定，但代理人仍試圖實現一個目標</mark>。

<mark style="color:red;">代理人的行動被允許影響環境的未來狀態（例如，下一個棋子的位置、煉油廠的儲油罐水平、機器人的下一個位置和其電池的未來充電水平），從而影響代理人在以後可利用的行動和機會</mark>。正確的選擇需要考慮到<mark style="color:blue;">行動的間接、延遲的後果</mark>，因此可能需要預知或計畫。同時，在所有這些例子中，行動的影響不能被完全預測；因此，代理人必須經常監測其環境並作出適當的反應。

<mark style="color:red;">所有這些例子涉及的目標都是明確的，即代理人可以根據它能直接感知的東西來判斷其目標的進展</mark>。棋手知道他是否贏了，煉油廠控制器知道正在生產多少石油，小羚羊知道它什麼時候跌倒，移動機器人知道它的電池什麼時候用完。

<mark style="color:red;">在所有這些例子中，代理人可以利用其經驗隨著時間的推移提高其效能</mark>。棋手完善了他用來評估位置的直覺，從而改善了他的棋藝；羚羊小腿提高了它的奔跑效率。

## 強化學習的重要元素

除了<mark style="color:red;">代理人(agent)</mark>和環境之外，我們可以確定強化學習系統的四個主要子元素：

* <mark style="color:red;">策略(policy)</mark>
* <mark style="color:red;">獎勵訊號(reward signal)</mark>
* <mark style="color:red;">價值函式(value function)</mark>
* <mark style="color:red;">環境模型(model of environment)(非必要)</mark>

<mark style="color:red;">策略定義了代理人在特定時間的行為方式</mark>。粗略地說，策略是一種從環境的感知狀態到處於這些狀態時要採取的行動的對映。它對應於心理學中所謂的一組刺激-反應規則或關聯。在某些情況下，策略可能是一個簡單的函式或查詢表，而在其他情況下，它可能涉及廣泛的計算，如搜尋過程。策略是強化學習代理的核心，因為它本身就足以決定行為。一般來說，策略可能是隨機的，每個行動有不同的機率。

<mark style="color:red;">獎勵訊號定義了強化學習問題的目標</mark>。在每個時間步驟中，環境向強化學習代理傳送一個稱為獎勵的單一數字。代理人的唯一目標是使其在長期內收到的<mark style="color:blue;">總獎勵最大化</mark>。因此，獎勵訊號定義了什麼是代理的好事件和壞事件。在一個生物系統中，我們可以認為獎勵類似於快樂或痛苦的體驗。它們是代理人所面臨的問題的直接和決定性特徵。獎勵訊號是改變策略的主要依據；如果策略所選擇的行動之後的獎勵很低，那麼就可以改變策略，在未來那種情況下選擇其他行動。一般來說，獎勵訊號可能是環境狀態和改採取的行動的隨機函式。

<mark style="color:red;">獎賞訊號表明什麼是眼前的好事(短期)，而價值函式則規定什麼是長期的好事</mark>。粗略地說，一個狀態的價值是一個代理人從該狀態開始，在未來可以期望積累的獎勵總量。獎勵決定了環境狀態的直接的、內在的可取性，而價值則表明了在考慮到可能會出現的狀態和這些狀態中可獲得的獎勵後，狀態的長期可取性。例如，一個狀態可能總是產生低的即時獎勵，但仍然有高的價值，因為它經常被其他產生高獎勵的狀態所跟隨。或者相反的情況也可能是真的。

獎勵在某種意義上是主要的，而作為對獎勵的預測，價值是次要的。沒有獎賞就沒有價值，而估計價值的唯一目的是為了獲得更多的獎賞。然而，<mark style="color:red;">在做決定和評估決定時，我們最關心的還是價值</mark>。行動選擇是基於價值判斷而做出的。<mark style="color:red;">我們尋求能夠帶來最高價值狀態的行動，而不是最高的回報，因為這些行動從長遠來看能夠為我們獲得最大的回報</mark>。

不幸的是，<mark style="color:blue;">確定價值比確定獎勵要難得多。獎勵基本上是由環境直接給予的，但價值必須從代理人在其整個生命週期中的觀察序列中進行估計和重新估計</mark>。事實上，我們所考慮的幾乎所有強化學習演算法的最重要組成部分是一種有效估計數值的方法。價值估計的核心作用可以說是過去六十年來人們在強化學習方面學到的最重要的東西。

一些強化學習系統的最後一個要素是一個環境模型。這是一種模仿環境行為的東西，或者更廣泛地說，它允許對環境的行為方式進行推斷。例如，給定一個狀態和行動，模型可以預測結果的下一個狀態和下一個獎勵。模型被用於規劃，我們指的是在實際經歷之前通過考慮未來可能的情況來決定行動方案的任何方式。<mark style="color:blue;">解決使用模型和規劃的強化學習問題的方法被稱為基於模型的方法，與之相對的是更簡單的無模型方法，後者是明確的試錯學習者，被視為幾乎與規劃相反</mark>。現代強化學習跨越了從低層次的試錯學習到高層次的慎重規劃的范圍。

### 限制與範圍

強化學習在很大程度上依賴於<mark style="color:red;">狀態(state)</mark>的概念，它是策略和價值函式的輸入，同時也是模型的輸入和輸出。<mark style="color:red;">非正式地講，我們可以把狀態看作是一個訊號，它向代理人傳達了某一特定時間的 "環境如何 "的感覺</mark>。然而，更廣泛地說，把狀態看作是代理人可以得到的關於其環境的任何資訊。實際上，我們假設狀態訊號是由一些預處理系統產生的，這些預處理系統在名義上是代理人環境的一部分。

我們不涉及構建、改變或學習狀態訊號的問題。採取這種方法並不是因為我們認為狀態表示是不重要的，而是為了充分關注決策問題。換句話說，我們<mark style="color:blue;">關注的不是設計狀態訊號，而是決定根據現有的任何狀態訊號採取何種行動</mark>。

我們在本書中考慮的大多數強化學習方法都是圍繞著估計價值函式(value function)而展開的，但嚴格來說，解決強化學習問題並不需要這樣做。例如，遺傳演算法、遺傳程式設計、模擬退火和其他最佳化方法等解決方法從不估計價值函式。<mark style="color:blue;">這些方法應用多個靜態策略，每個策略都在很長一段時間內與環境的單獨例項相互作用。獲得最多獎勵的策略，以及它們的隨機變化，被帶到下一代政策中，這個過程不斷重復</mark>。我們稱這些為進化方法，因為它們的操作類似於生物進化產生具有熟練行為的生物體的方式，即使它們在各自的生命中沒有學習。如果策略的空間足夠小，或者可以結構化，以便好的策略是常見的或容易找到的，或者如果有大量的時間可用於搜尋，那麼進化方法可以是有效的。

此外，進化方法在學習代理不能感知其環境的完整狀態的問題上也有優勢。我們的重點是強化學習方法，這些方法在與環境互動的同時進行學習，而進化方法不這樣做。在許多情況下，能夠利用個體行為互動的細節的方法比進化方法要有效得多。進化方法忽略了強化學習問題的許多有用的結構：他們沒有利用他們正在尋找的策略是一個從狀態到行動的函式這一事實；他們沒有注意到個體在其生命週期中經過哪些狀態，或者它選擇哪些行動。在某些情況下，這種資訊可能會產生誤導（例如，當狀態被誤解時），但更多的時候，它應該能使搜尋更有效率。雖然進化和學習有許多共同的特點，並且自然地一起工作，但我們認為進化方法本身並不特別適合強化學習問題。

## 範例：井字游戲

考慮一下我們熟悉的兒童游戲井字游戲。兩個玩家輪流在一個三乘三的棋盤上玩。一個人玩X，另一個人玩O，直到有一個人在水平、垂直或對角線上連放三個標記即贏得比賽。如果棋盤填滿了，雙方都沒有連續得到三個標記，那麼游戲就是平局。因為一個熟練的棋手可以做到永遠不輸，讓我們假設我們是在和一個不完美的棋手對弈，他的下法有時是不正確的，而讓我們贏。事實上，就目前而言，讓我們認為平局和輸棋對我們同樣不利。我們如何構建一個能夠發現對手棋局中的不完美之處，並學會將其獲勝的機會最大化的棋手？

雖然這是一個簡單的問題，但它不能輕易地通過經典技術得到滿意的解決。

* 例如，賽局理論中的經典 "最小化 "解決方案在這裡是不正確的，因為它假定了對手的一種特定的游戲方式。例如，一個最小化的玩家永遠不會達到一個可能會輸的游戲狀態，即使事實上它總是因為對手的不正確游戲而從該狀態中獲勝。
* 順序決策問題的經典最佳化方法，如動態規劃，可以計算出任何對手的最優解，但需要輸入該對手的完整規格，包括對手在每個棋盤狀態下的每一步棋的機率。

讓我們假設這個問題沒有先驗的資訊，因為絕大多數的實際問題都沒有這種資訊。另一方面，這種資訊可以從經驗中估計出來，在這種情況下，可以通過與對手下許多棋來估計。<mark style="color:blue;">在這個問題上，我們能做的最好的事情就是首先學習一個對手的行為模型，達到一定的置信度，然後應用動態規劃來計算一個給定的近似對手模型的最優解</mark>。這與我們研究的一些強化學習方法沒有什麼區別。

應用於這個問題的進化方法將直接在可能的策略空間中搜尋一個具有高機率贏得對手的政策。在這裡，策略是一個規則，它告訴棋手在游戲的每一種狀態下應該走什麼棋。對於所考慮的每一個策略，通過與對手進行一些游戲，可以獲得對其獲勝機率的估計。然後，這一評估將指導下一步考慮哪種或哪幾種策略。一個典型的進化方法將在策略空間中爬坡，連續生成和評估策略，試圖獲得增量改進。

下面是如何用價值函式的方法來處理井字游戲問題的。首先，我們將建立一個數字表，每個游戲的可能狀態都有一個表。每個數字都是對我們在該狀態下獲勝機率的最新估計。我們將這一估計值視為該狀態的價值，而整個表格就是所學的價值函式。

假設我們總是玩X，那麼對於所有連續有三個X的狀態，獲勝的機率是1，因為我們已經贏了。同樣地，對於所有連續有三個Os的狀態，或者被填滿的狀態，正確的機率是0，因為我們不能贏得遊戲。我們將所有其他狀態的初始值設為0.5，代表我們有50%的獲勝機會的猜測。然後我們與對手進行多次遊戲。為了選擇我們的棋步，我們檢查我們每一步可能的棋步所產生的狀態（棋盤上的每個空白處都有一個），並在表中查詢它們的當前值。大多數時候，我們都是貪婪地下棋，選擇有最大價值的狀態的棋步，也就是說，具有最高的估計獲勝機率。然而，偶爾我們也會從其他棋子中隨機選擇。這些被稱為探索性的行動，因為它們使我們體驗到我們可能永遠不會看到的狀態。

當我們在游戲時，我們會改變我們在游戲中發現自己所處的狀態的值。我們試圖使它們對獲勝的機率有更準確的估計。為了做到這一點，我們在每次貪婪行動後將狀態的值 "回升 "到行動前的狀態，如下圖中的紅色箭頭所示。更確切地說，早期狀態的當前值被更新為更接近後期狀態的值。

令$$S_t$$, $$S_{t+1}$$為貪婪行動前、後的狀態，則$$S_t$$​的價值函數更新如下：

* $$V(S_t) \leftrightarrow V(S_t) + \alpha [V(S_{t+1})- V(S_t)]$$
* $$\alpha$$為步長(step size)參數。
* 此為時序差分學習(temporal-difference learning)方法

此方法在這個任務上表現得相當好。例如，如果步長參數隨著時間的推移而適當減少，那麼對於任何固定的對手，這個方法都會收斂到我們的棋手在每個狀態下的真實獲勝機率。此外，隨後採取的棋步（除了探索性棋步）實際上是對這個（不完美）對手的最佳棋步。換句話說，該方法收斂到了與該對手對弈的最優策略上。如果步長參數沒有隨著時間的推移一直減少到零，那麼這個棋手在面對慢慢改變其下棋方式的對手時也會下得很好。

![井字游戲的決策過程，實線為實際採取的行動，虛線為可行的行動集合。](../.gitbook/assets/rl\_tic-toc-min.png)

這個例子說明了進化方法和學習價值函數的方法之間的區別。為了評估一個策略，進化方法將策略固定下來，與對手進行許多游戲，或使用對手的模型模擬許多游戲。勝利的頻率給出了對使用該政策獲勝的機率的無偏估計，並可用於指導下一次策略選擇。

但是，每一個策略的改變都是在很多場比賽之後才進行的，<mark style="color:blue;">而且只使用每場比賽的最終結果：比賽期間發生的事情被忽略了</mark>。例如，如果棋手贏了，那麼它在游戲中的所有行為都會被計入，而不考慮具體的棋步可能對贏棋有多大影響。甚至對從未發生過的動作也給予獎勵。與此相反，<mark style="color:red;">價值函數方法允許對個別狀態進行評估</mark>。

最後，進化方法和價值函數方法都在搜尋策略空間，但學習價值函數需要利用游戲過程中的可用資訊。這個簡單的例子說明了強化學習方法的一些關鍵特徵。

* 首先，強調的是在與環境互動時的學習，在這種情況下是與對手棋手互動。
* 其次，有一個明確的目標，<mark style="color:red;">考慮到一個人的選擇所帶來的延遲影響，正確的行為需要規劃或預見</mark>。例如，簡單的強化學習棋手會學會為目光短淺的對手設置多步陷阱。強化學習解決方案的一個顯著特點是，它可以在不使用對手模型和不對未來狀態和行動的可能序列進行明確搜尋的情況下實現計劃和展望的效果。

雖然井字游戲是一種雙人遊戲，<mark style="color:red;">但強化學習也適用於沒有外部對手的情況，即 "與自然的游戲 "的情況</mark>。強化學習也不侷限於行為被分解成不同的情節的問題，就像井字游戲一樣，只有在每個情節結束時才有獎勵。當行為無限期地持續下去，並且在任何時候都能得到不同程度的獎勵時，它也同樣適用。強化學習也適用於連續時間問題，盡管理論變得更加復雜，我們在這個介紹性章節中省略了它。

在這個井字游戲的例子中，學習開始時除了游戲規則外沒有任何先驗知識，但強化學習決不意味著對學習和智力的諱莫如深。相反，<mark style="color:red;">先驗資訊可以通過各種方式納入強化學習，這對高效學習至關重要</mark>。

在井字游戲的例子中，我們也可以獲得真實的狀態，而當部分狀態被隱藏時，或者當不同的狀態在學習者看來是相同時，強化學習也可以被應用。最後，井字棋手能夠預見未來，知道它的每一個可能的動作會產生什麼狀態。要做到這一點，它必須有一個游戲模型，使它能夠預見環境將如何對它可能永遠不會採取的行動作出反應。許多問題都是這樣的，但在其他問題中，甚至缺乏一個關於行動影響的短期模型。<mark style="color:blue;">強化學習可以適用於這兩種情況。不需要模型，但如果有模型或能學到模型，就可以很容易地使用</mark>。

另一方面，<mark style="color:red;">有一些強化學習方法根本不需要任何種類的環境模型。無模型的系統甚至不能考慮它們的環境將如何對一個單一的行動做出反應而變化</mark>。在這個意義上，井字棋手對其對手是無模型的：它沒有任何形式的對手模型。由於模型必須相當精確才有用，<mark style="color:blue;">當解決問題的真正瓶頸是難以構建足夠精確的環境模型時，無模型方法比更復雜的方法更有優勢</mark>。無模型方法也是基於模型的方法的重要構件。

## 小結

強化學習是一種理解目標導向的學習和決策並使之自動化的計算方法。<mark style="color:red;">它與其他計算方法的區別在於，它強調代理人從與環境的直接互動中學習，而不需要示範性的監督或環境的完整模型</mark>。

強化學習是第一個認真解決從與環境的互動中學習以實現長期目標的計算問題的領域。<mark style="color:red;">強化學習使用馬可夫決策過程的正式框架，以狀態、行動和獎勵的方式定義學習代理人與環境之間的互動</mark>。這個框架旨在成為代表人工智慧問題基本特徵的一種簡單方式。這些特徵包括<mark style="color:blue;">因果關系，不確定性和非確定性</mark>，以及明確目標的存在。

<mark style="color:red;">價值(value)和價值函數</mark>的概念是我們在本書中所考慮的大多數強化學習方法的關鍵。我們的立場是，價值函數對於在策略空間中的有效搜尋非常重要。價值函數的使用將強化學習方法與進化方法區分開來，後者直接在策略空間中以整個策略的評估為指導進行搜尋。

## 強化學習的早期歷史

強化學習的早期歷史有兩條主線，既漫長又豐富，它們在現代強化學習中交織在一起之前是獨立追求的。一條主線是關於通過<mark style="color:red;">試驗和錯誤的學習</mark>，它起源於動物學習的心理學。這條主線貫穿了人工智慧領域的一些最早的工作，並導致了強化學習在20世紀80年代初的復興。

第二條主線是關於<mark style="color:red;">最優控制(optimal control)的問題及其使用價值函數和動態規劃的解決方案</mark>。在大多數情況下，這條主線並不涉及學習。這兩條主線大多是獨立的，但在某種程度上，圍繞著第三條不太明顯的主線，即關於時序差分的方法，如本章中的井字游戲的例子，變得相互關聯。所有這三條線索在20世紀80年代末匯聚在一起，形成了現代強化學習領域。

專注於試錯學習的主線是我們最熟悉的，也是我們在這段簡短歷史中最有發言權的。然而，在這之前，我們先簡要地討論一下最優控制。術語 "最優控制 "是在20世紀50年代末出現的，用來描述設計一個控制器來最小化或最大化一個動態系統在一段時間內的行為的問題。這種方法使用動態系統的狀態和價值函數或 "最優報酬函數(optimal return ufnction) "的概念來定義一個函數方程，現在通常稱為Bellman方程。通過求解這個方程來解決最優控制問題的一類方法被稱為動態規劃（Bellman, 1957a）。Bellman（1957b）還介紹了最優控制問題的離散隨機版本，即馬可夫決策過程（MDPs）。Ronald Howard（1960）設計了MDPs的策略迭代方法。所有這些都是現代強化學習的理論和演算法的基本要素。

<mark style="color:red;">動態規劃被廣泛認為是解決一般隨機最優控制問題的唯一可行的方法</mark>。<mark style="color:red;">它受到Bellman所說的 "維度詛咒(curse of dimensionality) "的影響，也就是說，它的計算要求隨著狀態變量數量的增加而呈指數級增長，但它仍然比任何其他一般方法更有效、更廣泛地適用。</mark>自20世紀50年代末以來，動態規劃得到了廣泛的發展，包括對部分可觀察的MDP的擴展（由Lovejoy, 1991調查），許多應用（由White, 1985, 1988, 1993調查），近似方法（由Rust, 1996調查），以及異步方法（Bertsekas, 1982, 1983）。許多優秀的動態規劃現代處理方法都可以得到（例如，Bertsekas，2005，2012；Puterman，1994；Ross，1983；和Whittle，1982，1983）。Bryson（1996）提供了一個權威的最優控制的歷史。

最佳控制和動態規劃與學習之間的聯系被緩慢地認識到。我們不能確定是什麼原因造成了這種分離，但其主要原因可能是相關學科之間的分離和它們不同的目標。還有一個原因可能是人們普遍認為動態規劃是一種主要依靠精確的系統模型和Bellman方程的分析解來進行的精細計算。<mark style="color:blue;">此外，最簡單的動態規劃是一種在時間上向後進行的計算，因此很難看到它如何參與到一個必須向前進行的學習過程中</mark>。一些最早的動態規劃工作，如Bellman和Dreyfus（1959）的工作，現在可能被歸類為遵循學習方法。Witten（1977）的工作（在下文中討論）當然也可以說是學習和動態規劃思想的結合。Werbos（1987年）明確主張動態規劃和學習方法的更大的相互關系，以及動態規劃與理解神經和認知機制的相關性。對我們來說，動態規劃方法與在線學習的完全結合直到1989年Chris Watkins的工作才發生，他用MDP形式主義對強化學習的處理已經被廣泛採用。此後，這些關系被許多研究者廣泛發展，特別是Dimitri Bertsekas和John Tsitsiklis（1996），他們創造了 "神經動態規劃 "這一術語來指代動態規劃和人工神經網絡的結合。目前使用的另一個術語是 "近似動態規劃"。這些不同的方法強調了該主題的不同方面，但它們都與強化學習一樣，對規避動態規劃的經典缺點感興趣。

<mark style="color:blue;">我們把強化學習方法定義為解決強化學習問題的任何有效方法，現在很明顯，這些問題與最優控制問題密切相關，特別是隨機最優控制問題，如那些被表述為MDP的問題</mark>。因此，我們必須把最優控制的解決方法，如動態規劃，也看作是強化學習方法。因為幾乎所有的傳統方法都需要對要控制的系統有完整的瞭解，所以說它們是強化學習的一部分感覺有點不自然。另一方面，許多動態規劃演算法是增量的和迭代的。像學習方法一樣，它們通過連續的逼近逐漸達到正確的答案。完全知識和不完全知識情況下的理論和解決方法是如此密切相關，以至於我們覺得必須把它們作為同一主題的一部分來考慮。

現在讓我們回到導致現代強化學習領域的另一條主要線索，即以試錯學習思想為中心的線索。我們在這裡只觸及主要的聯系點。根據美國心理學家R.S.Woodworth（1938）的說法，試錯學習的思想最早可以追溯到19世紀50年代Alexander Bain對 "摸索和實驗 "學習的討論，更明確的是英國倫理學家和心理學家Conway Lloyd Morgan在1894年用這個詞來描述他對動物行為的觀察。也許第一個簡明扼要地表達試錯學習作為一種學習原則的本質的是Edward Thorndike。

> 在對同一情況作出的幾種反應中，那些伴隨著或緊隨其後的動物滿意的反應，在其他條件不變的情況下，將與該情況有更牢固的聯系，因此，當它再次出現時，它們將更有可能再次出現；那些伴隨著或緊隨其後的動物不舒服的反應，在其他條件不變的情況下，它們與該情況的聯系將被削弱，因此，當它再次出現時，它們將更不可能出現。滿意或不舒服的程度越大，紐帶的加強或削弱就越大。(Thorndike, 1911, p. 244)



## 參考資料

* Richard Suttion and Andrew G. Barto, "Reinforcement Learning: An Introduction," 2nd, 2018, chapter 1.
