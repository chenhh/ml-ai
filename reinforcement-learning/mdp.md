# Markov decision process (MDP)

## 簡介

MDP為強化學習(reinforcement learning, RL)的核心理論。

馬可夫決策過程，也稱為隨機動態程序或隨機控制問題，是在結果不確定時進行順序決策的模型。<mark style="color:red;">馬可夫決策過程模型由決策階段、狀態、行動、獎勵和轉移概率組成</mark>。在一個狀態中選擇一個動作將生成一個獎勵，並通過一個轉移概率函數確定下一個決策元的狀態。政策(policy)或策略(strategy)是在未來任何決策時期，在任何不預測情況下選擇行動的處方。決策者尋求某種意義上最優的政策。

模型的分析包括：

* 提供易於實現的最佳條件。
* 決定如何識別這些策略。
* 開發和改進計算它們的算法。
* 分析這些算法的收斂性。

> definition: Markov property
>
> 狀態$$S_t$$有馬可夫性質 $$\Leftrightarrow$$$$P(S_{t+1} \vert S_t) = P(S_{t+1} \vert S_t, S_{t-1}, S_{t-2}, \ldots, S_1)$$
>
> * <mark style="color:red;">當前的狀態可反應過去所有歷史狀態的訊息</mark>，因此只要知道當前的狀態$$S_t$$，不再需要其它的歷史資料$$S_{t-1}, S_{t-2}, \ldots,S_1$$。也就是說，我們所求的機率，可以捨去過去的所有狀態，只專注於眼前的狀態。這大大幫助我們減少計算量，且能夠用簡單的迭代法來求出結果。
> * 數理統計中也稱具有馬可夫性質$$S_t$$為未來狀態的充分統計量(sufficient statistics)。

**不是所有RL的問題都滿足Markov Property，但是我們可以假設問題滿足Markov Property來達到近似的結果**。

<mark style="color:red;">在強化學習中，有三個基本元素：狀態(State)、行動(Action)、報酬(Reward)。</mark>這三個元素就是我們的代理人(Agent)與環境(Environment)互相溝通的訊息，而代理人可以透過MDP選擇行動改變下一期的狀態，以提高報酬。

![狀態、行動、報酬的互動](../.gitbook/assets/rl-min.png)

* 代理人在狀態$$S_t$$時，選擇行動$$A_t$$後，會從環境得到報酬$$R_{t+1}$$且到達下一個狀態$$S_{t+1}$$。

![代理人(通常)無法控制環境，只能觀察狀態，決定行動，得到報酬](../.gitbook/assets/rl2-min.png)

$$\{r_t, s_t\}$$, $$a_t$$, $$\{ r_{t+1},  s_{t+1}\}$$, $$a_{t+1}$$$$\{ r_{t+2}, s_{t+2} \}$$, $$\ldots$$

* **大寫的字母表示隨機變數，小寫的字母表示為某一實現值**。
* <mark style="color:red;">如果決策圖中出現狀態->狀態的轉移時，表示兩個狀態間有一個決定性的行動(deterministic action)</mark>，即執行此行動後必定會轉移至下一個固定的狀態，而非是機率式的轉移。
* 行動之後必定是狀態，因此不會有行動至行動的情形出現。
* **在加入狀態進入模型後，代理人考慮的是全局最佳解的行動序列，即最大化**$$f(R_{t+1}, R_{t+2}, \ldots)$$**；而非局部最佳解(即只考慮**$$f(R_{t+1})$$**)**。

## RL問題假設

多強化學習都基於一種假設，即代理人(agent)與環境的交互作用可用一個MDP來表示。

* 可將代理人和環境表示為同步的有限狀態自動機；
* 代理人和環境在離散的時間內交互作用；
* 代理人態感知到環境的狀態，並做出反應性動作；
* 代理人執行完動作後，環境的狀態會發生變化；
* 在代理人執行完動作後，會得到某種回報；

<mark style="color:red;">也可以把代理人和環境的互動，以賽局理論(game theory)中的雙人賽局(玩家與環境)來分析的性質</mark>。

### 馬可夫決策過程(MDP)

> definition: Markov decision process
>
> 4-tuple $$(S, A, P, R)$$
>
> * $$S=\{s_0,s_1,\dots, s_n\}$$: 環境狀態集合 (有限或無限)。
> * $$A(s) =\{ a_0, a_1,\dots, a_n\}$$: 代理人行動集合(有限或無限)。這邊行動的集合範圍是依照狀態來決定，因為每個狀態可以做的行為不一定相同。
> * $$P: S \times A \times S \rightarrow [0,1]$$，狀態轉移函數。$$P(s^{'} \vert s, a)$$，即在目前的狀態$$s$$，決定行動$$a$$之後，轉移到狀態$$s^{'}$$的機率。因為在現實中，無法預測未來會發現什麼事，即使在時間點$$t$$，依據狀態$$s$$執行行動$$a$$，也無法保證會轉移到相同狀態。
> * $$R: S\times A \rightarrow \mathbb{R}$$，報酬函數 $$r(s,a)$$，在目前的狀態$$s$$，決定行動$$a$$後，會得到報酬。
>   * 報酬可能也是隨機的，即$$P(r \vert s,a)$$。
> * 也可將$$P, R$$寫在一起得 $$P(s^{'}, r \vert s, a) \equiv P(S_{t+1}=s^{'}, R_{t+1}=r \vert S_t =s, A_t =a)$$。
>   * $$P(s^{'} |s,a) = \sum_{r \in \mathbb{R}} P(s^{'}, r \vert s,a)$$
>   * $$P(r  \vert s,a) = \sum_{s^{'} \in S} P(s^{'}, r \vert s,a)$$
>   * 且 $$\sum_{s^{'} \in S} \sum_{r \in \mathbb{R}} P(s^{'}, r|s, a) = 1, \ \forall s \in S, a \in A(s)$$
> * \[Markov property] $$P(S_t|S_{t-1}, S_{t-2}, \dots, S_0) = P(S_t|S_{t-1})$$，現在的狀態包含著過去經歷過的所有狀態的資訊。也就是說，我們所求的機率，可以捨去過去的所有狀態，只專注於眼前的狀態。這大大幫助我們減少計算量，且能夠用簡單的迭代法來求出結果。

* <mark style="color:red;">MDP中，所有的過程(狀態，行動等)都有隨機性，下一個狀態轉移只與現在這個狀態有關</mark>。
* MDP假設環境在某個時刻$$t$$的狀態為$$s$$，則代理人在時間$$t$$採取行動$$a$$後，會使狀態轉變到下一狀態為$$s^{′}$$ 的機率為$$p(s^{′} \vert s,a)$$，且代理人得到的立即報酬為$$r(s,a)$$，只依賴於當前的狀態$$s$$與其選擇的行動$$a$$，而與歷史狀態和歷史動作無關。<mark style="color:red;">可</mark><mark style="color:red;">**解釋為將來的報酬與現在的選擇有關，而與過去無關**</mark>。
* **RL重視在轉移函數**$$P$$**與報酬函數**$$R$$**未知的條件下，學習算法如何獲得最佳行動策略**$$\pi : S \rightarrow A$$**。**$$\pi(s_t)=a_t$$。
* 強化學習關注的是智慧體如何在環境中採取一系列行為，從而獲得最大的(未來)累積回報。因此所有的RL方法共同的特點就是通過與環境的交互(試誤, trial-and-error)，來選擇適當的行動。
* RL是用不確定環境的報酬來發現最佳行動策略 ，不需事先是供訓練樣本，因此為在線學習(online learning)，與監督式學習為offline learning相異。

## MDP 的限制

* 無法處理非上帝視角的問題：我們生活的世界中，有很多東西是我們還無法觀測到的（比如人內心的想法、比如宇宙中的暗物質），所以我們無法描述這世界的真實狀態，這種問題就由更進階的 Partially Observable Markov Decision Processes 來嘗試建模。
* 只考慮到報酬，沒考慮到採取行動的成本(cost)：我們在採取行動時，除了會考慮獲得的報酬，也會考慮付出的代價。

## 參考資料

* Martin L. Puterman,  "Markov decision processes: discrete stochastic dynamic programming." John Wiley & Sons, 2014.
* 劉克，曹平，"馬爾可夫決策過程理論與應用"，科學出版社，2015。
