---
description: goal and reward
---

# 目標與報酬

## 目標與報酬(goal and rewards)

在強化學習中，代理人的目的或目標是以一種特殊的訊號（稱為獎賞）形式化的，從環境傳遞到代理人。在每個時間步驟中，獎勵是一個簡單的數字$$R_t \in \mathbb{ R}$$。<mark style="color:red;">這意味著最大化的不是眼前的獎勵，而是長期的累積獎勵。我們可以把這個非正式的想法清楚地表述為獎勵假說(reward hypothesis)</mark>。

> 那我們所說的目標和目的都可以很好地被認為是接收到的標量訊號（稱為獎勵）的累積總和的預期值的最大化。

使用獎勵訊號來正式確定目標的想法是強化學習的最顯著特徵之一。

雖然用獎勵訊號來制定目標最初可能會有侷限性，但在實踐中，它被證明是靈活和廣泛適用的。瞭解這一點的最好方法是考慮如何使用或可以使用它的例子。

例如，為了讓機器人學會走路，研究人員在每個時間步驟上提供與機器人向前運動成比例的獎勵。在讓機器人學習如何從迷宮中逃脫時，獎勵往往是在逃脫前的每一個時間步驟中都是-1；這鼓勵代理人盡可能快地逃脫。為了讓機器人學會尋找和收集空汽水罐進行回收，人們可能會在大部分時間內給它一個零的獎勵，然後每收集一個罐子就給它一個+1的獎勵。當機器人撞到東西或有人對它大喊大叫時，我們也可能想給它<mark style="color:red;">負獎勵</mark>。對於一個學習下跳棋或國際象棋的代理人來說，自然的獎勵是贏了+1，輸了-1，平局和所有非終端位置都是0。

你可以看到所有這些例子中發生了什麼。代理人總是學習最大化其獎勵。如<mark style="color:blue;">果我們想讓它為我們做一些事情，我們必須以這樣的方式向它提供獎勵，在最大化獎勵時，代理也將實現我們的目標</mark>。<mark style="color:red;">因此，至關重要的是，我們設定的獎勵要真正表明我們想要完成的事情</mark>。

特別是，獎勵訊號不是向代理人傳授關於如何實現我們希望它做的事情的事先知識的地方。例如，下棋的代理人應該只因實際獲勝而得到獎勵，而不是因實現諸如拿走對手的棋子或獲得棋盤中心的控制權等次級目標而得到獎勵。如果實現這些子目標得到獎勵，那麼代理人可能會找到一種方法來實現這些目標而不實現真正的目標。例如，它可能會找到一種方法來奪取對手的棋子，甚至以輸掉比賽為代價。獎勵訊號是你向機器人傳達你希望它實現什麼的方式，而不是你希望它如何實現。

## MDP 的限制

* 無法處理非上帝視角的問題：我們生活的世界中，有很多東西是我們還無法觀測到的（比如人內心的想法、比如宇宙中的暗物質），所以我們無法描述這世界的真實狀態，這種問題就由更進階的 Partially Observable Markov Decision Processes 來嘗試建模。
* 只考慮到報酬，沒考慮到採取行動的成本(cost)：我們在採取行動時，除了會考慮獲得的報酬，也會考慮付出的代價。

## 報酬與情節(episode)

到目前為止，我們已經非正式地討論了學習的目標。<mark style="color:red;">我們說代理人的目標是使其在長期內獲得的累積獎勵最大化</mark>。這可以如何正式定義呢？如果在時間步驟$$t$$之後收到的獎勵序列表示為$$R_{t+1},R_{t+2},R_{t+3}, \dots$$，<mark style="color:red;">一般來說，我們尋求期望報酬(expected return)的最大化，其中報酬表示為</mark>$$G_t$$，被定義為獎勵序列的一些特定函式。

在最簡單的情況下，報酬是獎勵的總和，$$T$$為期末時間：

$$Gt =R_{t+1}+ R_{t+2}+  \dots + R_{T}$$

這種方法在有最終時間步驟的自然概念的應用中是有意義的，也就是說，<mark style="color:red;">當代理人與環境的互動自然地分成子序列時，我們稱之為情節(episode)，有時也稱試驗(trial)</mark>，比如玩游戲、穿越迷宮或任何形式的重復互動。

每個情節都在一個特殊的狀態下結束，稱為終端狀態，然後被重置到一個標准的起始狀態或一個標准的起始狀態分佈中的樣本。即使你認為情節是以不同的方式結束的，比如游戲的輸贏，下一個情節的開始也與前一個情節的結束方式無關。因此，所有的情節都可以被認為是以相同的終端狀態結束的，不同的結果有不同的獎勵。有這種情節的任務被稱為<mark style="color:red;">情節任務(episode task)</mark>。在情節任務中，我們有時需要區分所有非終端狀態的集合（表示為$$\mathcal{S}$$）和所有狀態加終端狀態的集合（表示為$$\mathcal{S}^{+}$$）。終止的時間$$T$$，是一個隨機變數，通常在不同的劇情中會有所不同。

<mark style="color:red;">另一方面在許多情況下，代理人與環境的互動並不自然地分成可識別的情節，而是無限制地持續進行</mark>。我們把這些任務稱為<mark style="color:red;">持續任務(continuing task)</mark>。因為時間是無限的，所以將報酬改為折現率(discounted)的形式如下：

* $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}, ~ 0 \leq \gamma \leq 1$$
* 當$$\gamma <1$$且$$R_t=1, ~\forall t$$時，收斂為 $$G_t = \sum_{k=0}^\infty \gamma^k = \frac{1}{1-\gamma}$$

折現率決定了未來獎勵的現值：在未來$$k$$個時間步驟中收到的獎勵，其價值僅為立即收到的價值的$$\gamma^{k-1}$$倍。

* 如果$$\gamma<1$$，只要獎勵序列$$|R_k| < \infty$$是有界的，$$G_t$$中的無限和就有一個有限的值。
* 如果$$\gamma=0$$，代理人是 「短視近利(myopic」的，只關心即時獎勵的最大化：在這種情況下，它的目標是學習如何選擇$$A_t$$，以最大化$$R_{t+1}$$。<mark style="color:blue;">但是一般來說，為使眼前的回報最大化而行動，會減少對未來的累積報酬，從而使總報酬減少</mark>。
* 當$$\gamma \rightarrow 1$$時，回報目標更強烈地考慮到未來的回報；代理人變得更加遠視(farsighted)。

在連續的時間步驟中的回報是相互關聯的，這對強化學習的理論和演算法很重要：

$$\begin{aligned} G_t & = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} +\dots \\ 	& = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3}+ \dots) \\ 	& = R_{t+1}  + \gamma G_{t+1} \end{aligned}$$

上式對所有的時間步驟$$t<T$$都有效，即使終止發生在$$t+1$$，如果我們定義$$G_T=0$$。這通常會使我們很容易從獎勵序列中計算出報酬。

### 範例: 極點平衡(pole balancing)

這項任務的目標是對沿軌道移動的小車施加力，以使鉸接在小車上的桿子不致倒下。如果桿子從垂直方向跌落超過一個給定的角度，或者如果小車跑到軌道上，就說發生了故障。

每次失敗後，桿子會被重置為垂直狀態。這項任務可以被視為情節事件，其中自然情節事件是反復嘗試平衡桿子的過程。在這種情況下，失敗沒有發生的每一個時間步驟的獎勵可以是+1，因此，每次的回報將是直到失敗的步驟數。在這種情況下，永遠成功的平衡將意味著無窮大的回報。

另外，我們也可以把極點平衡當作一項持續的任務，使用折扣法。在這種情況下，每次失敗時的回報將是-1，其他時間的回報是0。那麼，每次的回報將與$$-\gamma^K$$有關，其中$$K$$是失敗前的時間步數。在任何一種情況下，通過盡可能長時間地保持極點的平衡，可使報酬是最大化。

## 情節性與持續性任務符號的統一

在上一節中，我們描述了兩種強化學習任務，一種是代理人與環境的互動自然分解為一連串獨立的情節（情節性任務），另一種則不是這樣的（持續性任務）。

因此，建立一個符號，使我們能夠同時准確地談論這兩種情況是很有用的。我們對每一情節的時間步驟從0開始進行編號。因此，我們不僅要參考時間$$t$$的狀態$$S_t$$，還要參考第$$i$$個情節時間t的狀態$$S_{t,i}$$（類似的還有$$A_{t,i}, R_{t,i}, \pi_{t,i}, T_i$$等）。然而，事實證明，當我們討論情節性任務時，我們幾乎不必區分不同的情節。我們幾乎總是在考慮一個特定的單一情節，或陳述對所有情節都是真實的東西。因此在實踐中，我們幾乎總是略微濫用符號，放棄對情節編號的明確提及。也就是說，我們用$$S_t$$來替代$$S_{t,i}$$，以此類推。

在一種情況下，我們將報酬定義為有限數量項的總和；在另一種情況下，定義報酬為無限數量項的總和。這兩種情況可以統一起來，即把情節終止看作是進入一個特殊的吸收狀態，該狀態只向自身轉換，並且只產生零的獎勵。

$$G_t = \sum_{k=t+1}^T \gamma^{k-t-1}R_k$$
