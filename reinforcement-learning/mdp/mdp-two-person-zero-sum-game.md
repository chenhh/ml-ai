# MDP與雙人零和賽局

## 比較表

|       |               |                  |
| ----- | ------------- | ---------------- |
| 決策者數量 | 單個代理人         | 兩名玩家             |
| 獎勵結構  | 單方最大化         | 零和（對抗性）          |
| 行動選擇  | 單方優化          | 雙方互動             |
| 計算方法  | 值迭代、策略迭代      | Minimax 值迭代、策略迭代 |
| 應用場景  | 單代理問題（如機器人導航） | 雙人對抗（如棋類賽局）      |

## MDP

Markov Decision Process 是一種決策制定的框架，用於解決具有馬可夫性質的問題。MDP 的基本特點包括：

* **馬可夫性**：未來狀態的機率只依賴於當前狀態，而不依賴於過去的狀態序列。
* **完全或部分資訊**：決策者可能對系統的狀態具有完全或部分的資訊。
* **長期報酬**：決策者關注的是長期報酬，而不僅僅是當前的報酬。
* **策略選擇**：決策者通過選擇最佳策略來最大化長期報酬。

## **雙人零和賽局**

* **定義**：雙人零和賽局中，兩名玩家的總收益恆為零，即一方的收益即為另一方的損失。
* **策略空間**：每個玩家在特定的狀態下需要選擇最佳策略以最大化自己的收益，並最小化對方的收益。
* **價值函式**：在零和賽局中，存在一個值 $$V(s)$$ 表示在狀態 $$s$$ 下，對於雙方都採用最優策略時，玩家 1 能獲得的收益。
* **零和性**：一方的獲利必然是另一方的損失，即總收益為零。
* **完全資訊**：兩個玩家對遊戲的狀態和可能的策略都有完全的資訊。

雙人零和賽局可以視為 擴充套件的 MDP，稱為 對策型馬可夫決策過程（Markov Game），，其特點包括：

* **多代理系統**：
  * 雙人零和賽局是多代理版本的 MDP，有兩個玩家交替進行動作。
  * 每個玩家都有自己的策略和目標函式。
* **收益結構**：
  * 對於零和賽局，玩家 1 的收益 R(s,a1,a2)R(s, a\_1, a\_2)R(s,a1​,a2​) 與玩家 2 的收益滿足關係： $$R_1(s,a1,a2)=−R_2(s,a1,a2)$$。
* **值迭代和策略迭代的應用**：
  * 在雙人零和賽局中，值迭代方法可擴充套件為 **極小化最大化（Minimax）方法**，用於計算在每個狀態下的賽局價值 $$V(s)$$。
  * 策略迭代則考慮雙方策略的更新以實現最優平衡。
* **轉移機率和動作空間**：
  * 轉移機率依賴於兩名玩家的行動。
  * 例如，狀態轉移函式 $$P(s′∣s,a_1,a_2)$$同時考慮玩家 1 和玩家 2 的行動 $$a_1$$​ 和 $$a_2$$​。
* 雙人零和賽局可以被視為一個特殊的MDP,其中:
  * 環境(對手)會針對主體的策略做出最優對抗
  * 一個玩家的收益正好是另一個玩家的損失
  * 兩個玩家輪流行動,形成交替決策過程

### 主要差異

* MDP假設環境是隨機的(stochastic),而不是對抗性的
* 雙人零和賽局中,對手明確試圖最小化玩家的收益
* MDP通常關注長期累積獎勵,而許多賽局關注單次對局結果

### 建模轉換

* 可以將雙人零和賽局轉換為特殊的MDP:
  * 狀態空間是遊戲的所有可能狀態
  * 行動空間是玩家的可選擇動作
  * 獎勵函式反映遊戲的得分機制
  * 轉移函式包含對手的最優對抗策略

### 求解方法的關聯

* MDP通常使用動態規劃或強化學習求解
* 雙人零和賽局可以使用minimax演算法
* 兩種方法都在尋找最優策略,但考慮的對手模型不同
