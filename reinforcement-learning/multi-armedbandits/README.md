---
description: Multi-armedBandits
---

# 多臂吃角子老虎機

## 簡介

<mark style="color:red;">強化學習區別於其他型別學習的最重要特徵是，它使用的訓練資訊是對改採取的行動進行評估(以行動-報酬評估價值函數)，而不是通過給出正確的行動進行指導</mark>。這就是創造主動探索的需要，明確地尋找好的行為。純粹的評價性反饋表明改採取的行動有多好，但不表明它是最好還是最差的行動。

## k臂吃角子老虎機問題(k-armed bandit problem)

決策者反復(多期決策)面臨著在$$k$$個不同的選項，或行動中的選擇。每次選擇之後，決策者都會收到一個數值獎勵（<mark style="color:blue;">**註：只知道所選的行動的獎勵，沒有選擇的行動不知道其獎勵**</mark>），這個獎勵是從一個<mark style="color:blue;">**固定的機率分佈(均為非時變且未知型式的分佈，但k個行動可有相異的分佈)**</mark>中選出，取決於你所選擇的行動。你的目標是在某個時間段內做出選擇，以最大化預期總報酬。

#### <mark style="color:red;">問題關鍵假設</mark>

經典 k-armed bandit 的關鍵假設是：每個拉桿的報酬分佈是靜態的(非時變)、獨立的，且符合某種固定的機率分佈（如正態、伯努利分佈或其它分佈）。

* **每個拉桿的報酬分佈是固定的（靜態假設）。**&#x6BCF;個拉桿$$k$$ 報酬是根據某個固定的機率分佈生成的，該分佈在整個問題期間不變。可以擴充為報酬分佈隨時間變化（動態環境），不在目前的討論範圍。
* 常見的假設是每個拉桿的報酬分佈相互獨立。
* 每次拉桿的報酬是獨立生成的，與之前或之後的操作無關。
* 每次拉桿立即給出報酬。

<figure><img src="../../.gitbook/assets/image (41).png" alt=""><figcaption><p>k臂吃角子老虎機，只有選中的機器才知道報酬。</p></figcaption></figure>

這是$$k$$臂吃角子老虎機問題的原始形式。<mark style="color:blue;">每一個行動選擇就像老虎機的一個槓桿，而獎勵就是擊中大獎的報酬</mark>。通過反復的行動選擇，你要把你的行動集中在最好的槓桿上，使你的贏利最大化。

在吃角子老虎機問題中，$$k$$個行動都有一個預期的或平均的獎勵，可稱為該行動的價值(value)。我們用$$A_t$$(隨機變數)表示在時間步驟t上選擇的行動，用$$R_t$$(隨機變數)表示相應行動的報酬。 那麼，任意行動$$a$$(實現值)的價值可表示為$$q_{*}(a)$$，是指在$$a$$被選中的情況下的預期報酬：

$$q_{*}(a) \equiv \mathrm{E}(R_t ~|~ A_t = a)$$

注意此處的預期報酬是在第$$t$$期決策選擇行動$$a$$的期望值(用某些方法估計得出)，而不是實際值。

<mark style="color:red;">如果你知道每個行動的價值</mark>$$q_*(a), ~\forall a$$<mark style="color:red;">，那麼解決吃角子老虎機問題將很簡單：每一次總是選擇期望價值最高的行動。但一般無法確切估計地知道行動的價值</mark>。我們把行動$$a$$在時間$$t$$的估計值表示為$$Q_t(a)$$，且希望$$Q_t(a)$$能夠逼近$$q_{*}(a)$$。

### 利用與探索

如果你保持對行動價值的估計，那麼在任何時間點，至少有一個行動的估計值是最大的。我們稱這些為<mark style="color:blue;">貪婪的行動 (greedy action)</mark>。

* 當你選擇這些行動之一時，我們說你是在利用(exploiting)你對行動價值的現有知識。
* 如果你選擇了一個非貪婪的行動，那麼我們說你是在探索(exploring)，因為這使你能夠提高你對非貪婪行動價值的估計。
* 利<mark style="color:red;">用是為了最大化一步達到的預期回報，但探索可能會在長遠看產生更高的總回報</mark>。

例如，假設一個貪婪的行動的價值是確定的，而其他幾個行動被估計為幾乎一樣好，但有很大的不確定性。這種不確定性使得這些其他行動中至少有一個可能實際上比貪婪行動更好，但你不知道是哪一個。如果你有很多時間可進行行動選擇，那麼探索非貪婪行動並發現其中哪些行動比貪婪行動更好，可能會更好。

在探索過程中，短期回報較低，但長期回報較高，因為在你發現更好的行動後，你可以多次利用它們。<mark style="color:blue;">由於不可能既探索又利用任何單一的行動選擇，人們經常提到探索和利用之間的「沖突」</mark>。​

在任何特定情況下，是否應該選擇探索還是利用，都取決於<mark style="color:red;">估計值、不確定性以及剩餘步驟</mark>數等因素，這是一個複雜的問題。對於k臂老虎機問題及相關問題的特定數學形式化，有許多高級方法可以平衡探索與利用。然而，大多數這些方法對於定態（stationarity）和先驗知識做出了強烈的假設，而這些假設在實際應用中要麼被違反，要麼根本無法驗證。這些方法所提供的優化或有界損失保證，在理論假設不適用的情況下，毫無幫助。

在這本書中，我們不會關注如何高級地平衡探索與利用；我們只關心如何平衡它們。在本章中，我們將介紹幾種簡單的平衡方法，用於解決k臂老虎機問題，並展示這些方法比始終選擇利用的方法效果更好。在強化學習中，平衡探索與利用是一個獨特的挑戰；而我們版本的k臂老虎機問題的簡單性，使得這一點表現得特別清晰。

## 行動-價值方法(action-value method)

<mark style="color:red;">行動價值法是利用已經採取的行動與得到的報酬估計行動的期望報酬的方法</mark>。

一個行動的真實價值是該行動被選中時的平均獎勵。估算的一個自然方法是對實際收到的獎勵進行平均化：

* $$\begin{aligned} Q_t(a)&=\frac{\text{時間t之前採取行動a的報酬總和} }{\text{時間t之前採取行動a的次數} } \\ &= \frac{\sum_{i=1}^{t-1} R_i \cdot \mathrm{I}(A_i = a) }{\sum_{i=1}^{t-1} \mathrm{I}(A_i = a) } \end{aligned}$$
* 若分母為0時，令$$Q_t(a)=0$$。​
* <mark style="color:blue;">註：此方法在行動集合</mark>$$A$$<mark style="color:blue;">有很多元素時，必須要有很多的資料才能得到較好的估計值</mark>。

最簡單的行動選擇規則是選擇具有最高估計值的行動之一。<mark style="color:red;">貪婪行動選擇方法</mark>寫成：

$$\displaystyle A_t \equiv \argmax_a Q_t(a)$$

貪婪的行動選擇總是利用當前的知識來最大化眼前的回報；它根本不花時間去抽查明顯較差的行動，看看它們是否真的會更好。

一個簡單的替代方案是在大多數時候表現得很貪婪，但每隔一段時間，比如說以小機率$$\epsilon$$，而從所有的行動中隨機選擇概率相同的行動，與行動價值估計無關。我們把使用這種近乎貪婪的行動選擇規則的方法稱為「$$\epsilon$$-<mark style="color:red;">貪婪方法</mark>」。

這些方法的一個優點是，在極限情況下，隨著步驟數的增加，每一個行動都會被實行無限次，從而確保所有的$$Q_t(a)$$都收斂到$$q_{*}(a)$$。這當然意味著選擇最優行動的機率會收斂到大於$$1-\epsilon$$，然而，這些只是漸進的保證，對於這些方法的實際有效性沒有說明什麼。

## 10臂吃角子老虎機的測試結果

以為下$$k=10$$的2000次隨機測試。

## 價值函數估計式的漸近實作

到目前為止，我們所討論的行動值方法都是將行動值作為觀察到的獎勵的樣本平均數來估計。我們現在要討論的問題是，這些平均值如何能夠以一種高效的計算方式來計算，特別是在恆定記憶體和恆定每步時間計算的情況下。

考慮單一行動，為了簡化符號，令$$R_i$$為第$$i$$​次選擇該行動的報酬，$$Q_n$$​為該行動已被選擇$$n-1$$​次後的估計價值，即：

$$Q_n\equiv \frac{R_1 + R_2 + \dots + R_{n-1}}{n}$$

上式為有批次資料時的計算方式，如果資料是逐漸在線進入時，計算方法可改為：

$$\begin{aligned} Q_{n+1} & = \frac{1}{n} \sum_{i=1}^n R_i \\ 	& = \frac{1}{n } (R_n + (n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} R_i) \\ 	& = \frac{1}{n } (R_n + (n-1) Q_n) \\ 	& = Q_n + \frac{1}{n} (R_n - Q_n) \end{aligned}$$

因此可得$$\epsilon$$-貪婪的吃角子老虎機演算法的虛擬碼如下：

```python
for a = 1 to k:
    Q(a) = 0  # 行動a的價值函數估計值
    N(a) = 0  # 行動a被執行的次數
	

while(true){
    A = argmax_{a} Q(a) with probability 1 - e 
        or a random action with probability e
    R = bandit(A)  # 以行動A玩老虎機，得到報酬R
    N(A) = N(A) + 1
    Q(A) = Q(A) + 1/N(A)[R-Q(A)]
}
```

## 追踨非定態問題(tracking a non-stationary problem)

上述討論的平均方法適合獎勵機率不隨時間變化的老虎機問題。<mark style="color:red;">而我們經常遇到的強化學習問題實際上是非穩態的。在這種情況下，給最近的獎勵比給過去的獎勵更多的權重是有意義的</mark>。

### 固定權重加權平均

最流行的方法之一是使用一個恆定的步長引數。例如，用於更新$$n-1$$個過去獎勵的平均值$$Q_n$$的增量更新規則被修改為：

* $$Q_{n+1} = Q_n  + \alpha [R_n - Q_n], ~ \alpha \in (0, 1]$$
* $$\alpha$$為固定的常數。

因此$$Q_{n+1}$$為過去報酬的加權平均值如下。

$$\begin{aligned} Q_{n+1} & = Q_n  + \alpha [R_n - Q_n]  \\ 	& = \alpha R_n + (1-\alpha) Q_n \\ 	& = \alpha R_n + (1-\alpha )[\alpha R_{n-1} + (1-\alpha )Q_{n-1}]  \\ 	& = \alpha R_n + (1-\alpha) \alpha R_{n-1} + (1-\alpha)^2 Q_{n-1} \\ 	& \dots \\ 	& = (1-\alpha)^n Q_1 + \sum_{i=1}^n \alpha (1-\alpha)^{n-i}R_i  \end{aligned}$$

因為$$(1-\alpha)^n  + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}=1$$，所以稱為加權平均。事實上，權重是按照指數$$1-\alpha$$衰減的。因此，這有時被稱為<mark style="color:red;">指數化的時間加權平均值</mark>。

### 變動權重加權平均

令$$\alpha_n(a)$$表示用於處理第$$n$$次選擇行動$$a$$後收到的獎勵的步長參數。當$$\alpha_n(a)=\frac{1}{n}$$時退化為簡單平均法。但當然不能保證對序列$$\{\alpha_n(a)\}$$的所有選擇都能收斂。

隨機近似理論中一個著名的結果給我們<mark style="color:red;">保證機率收斂所需的條件為</mark>：

$$\begin{aligned}  & \displaystyle \sum_{n=1}^\infty \alpha_n(a) = \infty \\  & \sum_{n=1}^\infty \alpha_n^2 (a) < \infty \end{aligned}$$

第一個條件是為了保證步長足夠大，以最終克服任何初始條件或隨機波動。第二個條件保證最終步長變得足夠小，以保證收斂。

請注意，這兩個收斂條件在樣本平均的情況下成立$$\alpha_n(a)=\frac{1}{n}$$，但在恆定步長引數的情況下不成立$$\alpha_n(a)=\alpha$$。

<mark style="color:red;">在後一種情況下，第二個條件沒有得到滿足，這表明估計值從未完全收斂，而是繼續隨著最近收到的獎勵而變化</mark>。正如我們上面提到的，這在非平穩環境中實際上是可取的，而實際上非平穩的問題是強化學習中最常見的。

<mark style="color:blue;">此外，滿足上述條件的步長引數序列往往收斂得很慢，或者需要相當的調整才能獲得滿意的收斂率</mark>。盡管滿足這些收斂條件的步長引數序列經常被用於理論工作中，但它們很少被用於應用和實證研究中。

## 樂觀的行動價值函數初始值

到目前為止，我們所討論的所有方法都在一定程度上依賴於初始行動值的估計，$$Q_1(a)$$。

用統計學的語言來說，這些方法都因其初始估計值而有偏差。對於樣本平均法來說，一旦所有的行動都被選擇了至少一次，這種偏差就會消失，但是對於常數$$\alpha$$的方法來說，這種偏差是永久性的，盡管隨著時間的推移而減少影響。<mark style="color:blue;">在實踐中，這種偏差通常不是一個問題，有時可能非常有幫助。缺點是初始估計值在中成為一組必須由用戶挑選的參數，如果只是為了將它們全部設置為零的話</mark>。好處是，它們提供了一種簡單的方法來提供一些關於可以預期的獎勵水平的預先知識。

初始動作值也可以作為鼓勵探索的一種簡單方式來使用。假設我們沒有像在10臂老虎機中那樣將初始行動值設置為零，而是將它們全部設置為+5。回顧一下，這個問題中的$$q_{*}(a)$$是從均值為0、變異數為1的常態分佈中選出的。因此，+5的初始估計是非常樂觀的。但這種樂觀主義鼓勵了行動價值方法的探索。無論最初選擇哪種行動，獎勵都小於初始估計值；學習者切換到其他行動，對它所得到的獎勵感到 "失望"。其結果是，在價值估計收斂之前，所有的行動都要嘗試幾次。即使一直選擇貪婪的行動，該系統也會進行相當數量的探索。

## 信心上限的行動選擇(Upper-Confidence-Bound Action Selection)

之所以需要探索，是因為行動-價值估計的准確性總是存在不確定性。$$\epsilon$$-貪婪的行動選擇迫使非貪婪的行動被嘗試，但不加區分，對那些接近貪婪的或特別不確定的行動沒有偏好。

<mark style="color:red;">更好的做法是根據非貪婪行動的實際最優潛力來選擇它們，同時考慮到它們的估計值離最大值有多近以及這些估計值的不確定性</mark>。這樣做的一個有效方法是根據以下因素選擇行動：

* $$A_t = \argmax_a\left\{   Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}  \right\}$$
* $$N_t(a)$$在時間$$t$$(不包含$$t$$​)之前，行動$$a$$被選中的次數。
* 當$$N_t(a)=0$$ , $$a$$被定義為有最大值的行動。
* $$c >0$$為控制探索強度的參數。

這種置信度上限（UCB）行動選擇的想法是，平方根項是對行動$$a$$的估計值的不確定性或變異數的衡量。因此，被最大化的數量是一種對行動$$a$$的可能真實值的上限，$$c$$決定了信心水平。

* 每次選擇行動$$a$$時，不確定性大概都會減少。因為$$N_t(a)$$遞增，並且由於它出現在分母中，不確定性項減少了。
* 另一方面，每次選擇$$a$$以外的行動時，$$t$$增加，但$$N_t(a)$$不增加；
* 因為$$t$$出現在分子中，不確定性估計增加。使用自然對數意味著隨著時間的推移，增幅會越來越小，但卻是無限制的；
* 所有的行動最終都會被選擇，<mark style="color:red;">但價值估計較低的行動，或者已經被頻繁選擇的行動，會隨著時間的推移，被選擇的頻率越來越低</mark>。

UCB通常表現良好，但比「$$\epsilon$$-貪婪方法」更難超越老虎機問題擴展到普遍的強化學習環境。另一個困難是處理非定態問題；需要更復雜的方法。另一個困難是處理大的狀態空間，特別是函數近似法時。使得UCB在實務上並不好用。

## Gradient Bandit Algorithms(梯度老虎機算法)

。在本節中，我們考慮為每個行動$$a$$學習一個數字的偏好$$H_t(a)$$。偏好越大，該行動就越經常被採取，但該偏好在獎勵方面沒有影響。只有一個行動對另一個行動的相對偏好是重要的；如果我們把所有的行動偏好都加到1000，就不會對行動機率產生影響，這些機率是根據軟最大分佈（即Gibbs或Boltzmann分佈）確定的，如下所示：

* $$\displaystyle \mathrm{P}(A_t=a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)$$
* $$\pi_t(a)$$為時間$$t$$​時選擇行動$$a$$​的機率。
* 初始時，所有行動的偏好均相同，即$$\forall a, ~H_1(a)=0$$​，此時所有行動被選擇的機率均相同。

基於隨機梯度上升的思想，有一種自然的軟最大行動偏好學習演算法。在每一步，在選擇行動$$A_t$$並獲得獎勵$$R_t$$後，行動偏好通過以下方式更新：

* \[被選中的行動更新偏好]$$H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \overline{R}_t)(1-\pi_t(A_t))$$
* \[沒被選中的行動也更新偏好] $$H_{t+1}(a) = H_t(a) - \alpha(R_t - \overline{R}_t)\pi_t(a), ~ \forall a \neq A_t$$
* $$\alpha$$​為步長(step-size)參數。
* $$\overline{R}_t \in \mathbb{R}$$​為到時間$$t$$​(不包含$$t$$​)的平均報酬，做為行動$$A_t$$​報酬$$R_t$$的基準值。
* 如果獎勵高於基準值。那麼在未來採取$$A_t$$的機率就會增加，如果獎勵低於基準值，那麼該機率就會下降。沒被選擇的行動則向相反的方向發展。由於獎勵基線項的存在，所有獎勵的上移對梯度老虎機演算法完全沒有影響，它能立即適應新的水平。但是，如果省略基線（也就是說如果$$\overline{R}_t=0$$），那麼效能將大大降低。

## 關聯搜尋（上下文老虎機）

到目前為止，在本章中我們只考慮<mark style="color:blue;">了非關聯性任務，也就是不需要將不同的行動與不同的情況聯絡起來的任務</mark>。在這些任務中，當任務是靜止的時候，學習者試圖找到一個單一的最佳行動，或者當任務是非靜止的時候，試圖跟蹤最佳行動隨時間的變化。

然而，在一般的強化學習任務中，有不止一種情況，目標是學習一個策略：<mark style="color:red;">從情況到在這些情況下最好的行動的對映</mark>。為了給整個問題做個鋪墊，我們簡要地討論一下非關聯任務延伸到關聯環境的最簡單方式。

舉個例子，假設有幾個不同的k臂老虎機任務，在每一步你都要面對其中一個隨機選擇的任務。因此，老虎機任務在每一步都會隨機變化。這在你看來就是一個單一的、非穩態的k臂老虎機任務，其真實行動值在每一步都隨機變化。你可以嘗試使用本章中描述的可以處理非平穩性的方法之一，但是除非真實行動值變化緩慢，否則這些方法的效果不會很好。

然而，現在假設當為你選擇一個老虎機任務時，你得到了一些關於任務的獨特線索（但不是其行動值）。也許你面對的是一台實際的老虎機，它在改變動作值的時候會改變其螢幕的顏色。現在你可以學習一種策略，將你看到的顏色所表示的每項任務與面對該任務時應採取的最佳行動聯絡起來。例如，如果是紅色，選擇手臂1；如果是綠色，選擇手臂2。有了正確的策略，你通常可以比在沒有任何資訊的情況下做得更好。

這是一個關聯搜尋任務的例子，<mark style="color:red;">之所以被稱為關聯搜尋，是因為它既涉及到試錯學習以尋找最佳行動，又涉及到這些行動與它們在什麼情況下是最佳的關聯</mark>。

關聯搜尋任務是介於k臂老虎機問題和完全強化學習問題之間。它們與完全強化學習問題一樣，涉及到學習策略，但與我們的k臂老虎機問題一樣，每個行動隻影響到即時獎勵。<mark style="color:blue;">如果允許行動同時影響下一個情況和獎勵，那麼我們就有了完整的強化學習問題</mark>。

## 小結

本章中介紹了幾種平衡探索和利用的簡單方法。

* $$\epsilon$$-貪婪方法在一小部分時間內是隨機選擇的。
* UCB方法是確定選擇的，但通過在每一步巧妙地偏向於迄今為止收到較少樣本的行動來實現探索。
* 梯度老虎機演算法估計的不是行動的價值，而是行動的偏好，並以一種分級的、機率性的方式使用軟最大分佈偏向於更多的行動。
* 以樂觀的態度初始化估計的簡單權宜之計，甚至使貪婪的方法也有了明顯的探索。

很自然地，我們會問這些方法中哪一個是最好的。雖然這個問題一般來說很難回答，但我們可以在簡單的問題測試這虛方法，並比較它們的效能。

一個復雜的問題是，它們都有一個參數；為了得到一個有意義的比較，我們必須考慮它們的效能是會隨其參數而改變。

到目前為止，我們的圖表顯示了每種演算法和引數設定隨時間變化的學習過程，以產生該演算法和引數設定的學習曲線。如果我們繪制所有演算法和所有引數設定的學習曲線，那麼圖表將過於復雜和擁擠，無法進行清晰的比較。

相反，我們通過1000步的平均值來總結一個完整的學習曲線；這個值與學習曲線下的面積成正比。下圖顯示了本章中各種老虎機演算法的這一衡量標准，每種演算法都是其自身引數的函式，在$$X$$軸上顯示為單一刻度。這種圖被稱為參數研究(parameter study)。請注意，參數值是以2的系數變化的，並以對數尺度顯示。還要注意每種演算法效能的倒U型特徵；所有的演算法在其參數的中間值時表現最好，既不會太大也不會太小。

<mark style="color:red;">在評估一種方法時，我們不僅要關注它在最佳參數設定下的表現，還要關注它對引數值的敏感程度</mark>。所有這些演算法都相當不敏感，在大約一個數量級的引數值變化范圍內表現良好。總的來說，在這個問題上，UCB似乎表現最好。盡管它們很簡單，但在我們看來，本章介紹的方法可以公平地被認為是最先進的。還有更復雜的方法，但它們的復雜性和假設使得它們對於我們真正關注的全面強化學習問題來說是不切實際的。盡管本章所探討的簡單方法可能是我們目前能做到的最好的方法，但它們遠不是平衡探索和利用問題的完全滿意的解決方案。

在k臂老虎機問題中平衡探索和利用的一種被充分研究的方法是計算一種特殊的行動值，稱為<mark style="color:red;">Gittins指數</mark>。<mark style="color:blue;">在某些重要的特殊情況下，這種計算是可操作的，並直接導致最佳解，盡管它確實需要對可能的問題的先驗分佈有完整的瞭解，而我們通常認為這是不可能的</mark>。此外，這種方法的理論和計算的可操作性似乎都不能推廣到我們在其餘部分考慮的全部強化學習問題。

Gittins指數方法是貝式方法的一個例項，它假定行動值有一個已知的初始分佈，然後在每一步之後精確地更新分佈（假定真實的行動值是定態的的分佈）。一般來說，更新計算可能非常復雜，但對於某些特殊的分佈（稱為共軛先驗），它們很容易。一種可能性是在每一步根據其作為最佳行動的後驗機率來選擇行動。這種方法有時被稱為後驗抽樣或Thompson抽樣，通常與我們在本章中介紹的最好的無分佈方法有類似的表現。

在貝式設定中，甚至可以想像到計算探索和利用之間的最佳平衡。我們可以為任何可能的行動計算每個可能的即時獎勵的機率以及由此產生的行動值的後驗分佈。這種不斷變化的分佈成為問題的資訊狀態。

給定一個水平線，例如1000步，我們可以考慮所有可能的行動，所有可能的結果獎勵，所有可能的下一個行動，所有下一個獎勵，等等，所有1000步。考慮到這些假設，每個可能的事件鏈的獎勵和機率都可以確定，人們只需要選擇最好的。但是可能性之樹增長得非常快；即使只有兩個行動和兩個獎勵，這棵樹也會有$$2^{2000}$$個葉子。准確地進行這種巨大的計算通常是不可行的，但也許可以有效地進行近似計算。這種方法將有效地把老虎機問題變成一個完整的強化學習問題的例項。最後，我們也許能夠使用近似的強化學習方法，如本書第二部分所介紹的方法來接近這個最優解。但這是一個研究的課題，超出了這本介紹性書籍的范圍。

## 參考資料

* Richard Suttion and Andrew G. Barto, "Reinforcement Learning: An Introduction," 2nd, 2018, chapter 2.
