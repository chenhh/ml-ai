# UCB算法

## 信心上限的行動選擇(Upper-Confidence-Bound Action Selection)

<mark style="color:red;">行動之所以需要探索，是因為行動-價值估計的准確性總是存在不確定性</mark>。

$$\epsilon$$-貪婪的行動選擇迫使非貪婪的行動被嘗試，但不加區分，對那些接近貪婪的或特別不確定的行動沒有偏好。<mark style="color:red;">更好的做法是根據非貪婪行動的實際最優潛力來選擇它們，同時考慮到它們的估計值離最大值有多近以及這些估計值的不確定性</mark>。

**Upper-Confidence-Bound (UCB) 算法**是一種有效平衡探索與利用的策略，用於解決多臂吃角子老虎機問題。它基於對動作價值的不確定性進行量化，選擇在當前看似最優的動作中加入不確定性的「上置信界」，以實現探索。

**UCB 演算法的核心思想**

UCB 演算法的核心思想是為每個老虎機維護一個估計的平均回報，並加上一個「信心上限 (confidence bound)」。這個信心上限代表了我們對這個估計平均回報的不確定性。演算法會選擇具有最高「信心上限」的老虎機，也就是說，選擇那些我們認為可能具有高回報，但我們還沒有充分探索的老虎機。

自適應性：隨著更多的資訊被收集，演算法能夠自動調整其行為，減少不必要的探索。

#### **UCB 演算法的具體步驟**

目標是最大化累積獎勵：$$\displaystyle \sum_{t=1}^T R_t$$。其中動作的真實價值$$q_{*}(a)=\mathrm{E}(R_t~|~A_t=a)$$未知，需要在探索動作（以估計其價值）和利用已知最優動作之間做出權衡。

UCB 的核心想法是每個動作的真實價值 $$𝑞_{*} ( 𝑎 )$$ 位於其當前估計值$$𝑄_𝑡 ( 𝑎 )$$的置信區間內。我們可以基於這個置信區間的上界進行動作選擇，以同時考慮：

* 當前價值估計$$Q_t(a)$$ （利用）。
* 不確定性（探索），由置信區間的寬度表示。

對於動作$$a$$，我們的目標是估計其真實期望值 $$q_{*}(a)$$。，對樣本平均值的估計值$$Q_t(a)$$，其置信區間([Hoeffding不等式](../../probability-and-statistics/probability-inequality.md))可以用以下公式表示：$$Q_t(a) \pm c  \sqrt {\frac{\ln(t)}{N_t(a)}}$$。

假設我們從分佈範圍有限的隨機變量中抽取樣本，其樣本均值$$𝑄_𝑡 ( 𝑎 )$$  與真實期望值 $$𝑞_{∗} ( 𝑎 )$$ 的差異可以表示為 $$\displaystyle \mathrm{P}(|Q_t(a) - q_{*}(a)| \geq \epsilon) \leq 2 \exp (-2 \epsilon^2 N_t(a) )$$。

* $$\epsilon$$是偏離真實值的允許範圍。
* 右側的指數項描述了置信水平：隨$$N_t(a)$$增加，偏差的機率會指數級減小。

根據 Hoeffding 不等式，我們可以構建$$𝑞_{∗} ( 𝑎 )$$ 的置信區間：$$𝑞_{∗} ( 𝑎 ) \in [Q_t(a) - \epsilon,  Q_t(a) + \epsilon]$$。

為了確定置信區間上限，我們選擇 𝜖 ϵ 使得置信區間涵蓋真實值的概率非常高。將 𝜖表達為樣本數$$N_t (a)$$ 的函數：$$\epsilon = c \sqrt{\frac{\ln(t)}{N_t(a)}}$$，代入Hoeffding不等式可得：

$$\displaystyle  \begin{aligned} \mathrm{P}\{(|Q_t(a) - q_{*}(a)| \geq \epsilon)\} & \leq 2 \exp (-2 \epsilon^2 N_t(a) ) \\ \mathrm{P}\{(|Q_t(a) - q_{*}(a)| \geq  c \sqrt{\frac{\ln(t)}{N_t(a)}})\} & \leq 2 \exp  (-2  c^2 \frac{\ln(t) }{N_t(a)} N_t(a) ) \\ \mathrm{P}\{(|Q_t(a) - q_{*}(a)| \geq  c \sqrt{\frac{\ln(t)}{N_t(a)}})\} & \leq 2 t^{-2 c^2}   \end{aligned}$$

每次選擇置信區間上界最大的動作，即$$A_t = \argmax_a\left\{   Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}  \right\}$$

> 1. **初始化：** 為每個動作設置初始值$$𝑄_1 ( 𝑎 )=0$$並令$$𝑁_ 1 ( 𝑎 )=0$$, $$a=1,2,\dots,k$$。
> 2. **迭代：** 在每個時間點 _t_，對於每個行動a，計算其 UCB 值：$$UCB_t(a)=Q_t(a)+c \sqrt{ \frac{\ln(t)}{N_t(a)}}$$。
> 3. **選擇：** 選擇具有最高 UCB 值的行動 $$\displaystyle A_t = \argmax_{a} 	UCB_t(a)$$。
> 4. **更新：** 根據行動$$A_t=a$$與報酬$$R_t$$更新相$$\displaystyle Q_{t+1}(a)= Q_t(a) + \frac{1}{N_t(a)}(R_t - Q_t(a))$$ 和$$\displaystyle N_{t+1}(a)= N_t(a) +1$$。
> 5. 重複步驟 2-4，直到達到時間限制或收斂。

這樣做的一個有效方法是根據以下因素選擇行動：

* $$A_t = \argmax_a\left\{   Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}  \right\}$$
* $$N_t(a)$$在時間$$t$$(不包含$$t$$​)之前，行動$$a$$被選中的次數。
* 當$$N_t(a)=0$$ , $$a$$被定義為有最大值的行動。
* $$c >0$$為控制探索強度的參數。如果 _c_ 很大，則演算法會更傾向於探索那些還沒有充分探索的老虎機。如果 _c_ 很小，則演算法會更傾向於利用那些已經知道回報較高的老虎機。

這種置信度上限（UCB）行動選擇的想法：

* $$Q_t(a)$$代表了我們對行動$$a$$報酬的當前最佳估計。
* 平方根項是對行動$$a$$的估計值的不確定性或變異數的衡量。
* 被最大化的數量是一種對行動$$a$$的可能真實值的上限，$$c$$決定了信心水平。
* 每次選擇行動$$a$$時，不確定性大概都會減少。因為$$N_t(a)$$遞增，並且由於它出現在分母中，不確定性項減少了。
* 另一方面，每次選擇$$a$$以外的行動時，$$t$$增加，但$$N_t(a)$$不增加；
* 因為$$t$$出現在分子中，不確定性估計增加。使用自然對數意味著隨著時間的推移，增幅會越來越小，但卻是無限制的；
* 所有的行動最終都會被選擇，<mark style="color:red;">但價值估計較低的行動，或者已經被頻繁選擇的行動，會隨著時間的推移，被選擇的頻率越來越低</mark>。

#### **UCB 演算法的優點**

* **理論保證：** UCB 演算法具有理論上的後悔界限 (regret bound)，這意味著隨著時間的推移，它與始終選擇最佳老虎機的策略之間的差距會以次線性速度增長。
* **易於實現：** UCB 演算法的實現相對簡單。
* **適用範圍廣泛：** UCB 演算法可以應用於各種需要權衡探索和利用的問題，例如線上廣告、推薦系統和A/B測試。

UCB通常表現良好，但比「$$\epsilon$$-貪婪方法」更難超越老虎機問題擴展到普遍的強化學習環境。另一個困難是處理大的狀態空間，特別是函數近似法時。使得UCB在實務上並不好用。

### 遺憾分析(regret analysis)

K臂吃角子老虎機(K-arm MAB)問題中，$$K \in \mathbb{N}$$，每個行動$$a \in \{1,2,\dots, K\}$$有各自的報酬分佈$$R(a)$$。

令$$q(a)$$為$$R(a)$$的期望值(存在但未知)，定義$$q_{*}(a)=\max_{a \in [K]}q(a)$$，$$a_{*}=\argmax_{a \in [k]} q(a)$$為有最大期望報酬的行動。如果行動$$a$$被選中足夠多次後，則每次的報酬平均值可近似於真實的報酬期望值。



### 廣告點擊率範例

```python
import math


def ucb_algorithm(N: int, d: int, rewards: list[float]) -> (float, list[int]):
    """
    :param N: 決策次數 
    :param d: 行動個數
    :param rewards: 行動的報酬
    :return: (總報酬，每個行動被選中的次數)
    """
    numbers_of_selections = [0] * d
    sums_of_rewards = [0.0] * d
    total_reward = 0.0

    for n in range(N):
        max_ucb = 0.0
        ad = 0
        for i in range(d):
            if numbers_of_selections[i] == 0:
                ucb = float('inf')
            else:
                avg_reward = sums_of_rewards[i] / numbers_of_selections[i]
                ucb = avg_reward + math.sqrt(2 * math.log(n + 1) / numbers_of_selections[i])
            if ucb > max_ucb:
                max_ucb = ucb
                ad = i
        reward = rewards[ad]
        numbers_of_selections[ad] += 1
        sums_of_rewards[ad] += reward
        total_reward += reward

    return total_reward, numbers_of_selections


# 廣告資料
N = 10000  # 總決策次數
d = 10  # 廣告數量
rewards = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  # 每個的點擊率

total_reward, selections = ucb_algorithm(N, d, rewards)
print("總報酬:", total_reward)
print("每個廣告每選中的次數:", selections)

# 總報酬: 9648.999999999929
# 每個廣告每選中的次數: [21, 26, 34, 45, 62, 93, 154, 303, 855, 8407]
```

## Gradient Bandit Algorithms(梯度老虎機算法)

。在本節中，我們考慮為每個行動$$a$$學習一個數字的偏好$$H_t(a)$$。偏好越大，該行動就越經常被採取，但該偏好在獎勵方面沒有影響。只有一個行動對另一個行動的相對偏好是重要的；如果我們把所有的行動偏好都加到1000，就不會對行動機率產生影響，這些機率是根據軟最大分佈（即Gibbs或Boltzmann分佈）確定的，如下所示：

* $$\displaystyle \mathrm{P}(A_t=a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)$$
* $$\pi_t(a)$$為時間$$t$$​時選擇行動$$a$$​的機率。
* 初始時，所有行動的偏好均相同，即$$\forall a, ~H_1(a)=0$$​，此時所有行動被選擇的機率均相同。

基於隨機梯度上升的思想，有一種自然的軟最大行動偏好學習演算法。在每一步，在選擇行動$$A_t$$並獲得獎勵$$R_t$$後，行動偏好通過以下方式更新：

* \[被選中的行動更新偏好]$$H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \overline{R}_t)(1-\pi_t(A_t))$$
* \[沒被選中的行動也更新偏好] $$H_{t+1}(a) = H_t(a) - \alpha(R_t - \overline{R}_t)\pi_t(a), ~ \forall a \neq A_t$$
* $$\alpha$$​為步長(step-size)參數。
* $$\overline{R}_t \in \mathbb{R}$$​為到時間$$t$$​(不包含$$t$$​)的平均報酬，做為行動$$A_t$$​報酬$$R_t$$的基準值。
* 如果獎勵高於基準值。那麼在未來採取$$A_t$$的機率就會增加，如果獎勵低於基準值，那麼該機率就會下降。沒被選擇的行動則向相反的方向發展。由於獎勵基線項的存在，所有獎勵的上移對梯度老虎機演算法完全沒有影響，它能立即適應新的水平。但是，如果省略基線（也就是說如果$$\overline{R}_t=0$$），那麼效能將大大降低。

## 關聯搜尋（上下文老虎機）

到目前為止，在本章中我們只考慮<mark style="color:blue;">了非關聯性任務，也就是不需要將不同的行動與不同的情況聯絡起來的任務</mark>。在這些任務中，當任務是靜止的時候，學習者試圖找到一個單一的最佳行動，或者當任務是非靜止的時候，試圖跟蹤最佳行動隨時間的變化。

然而，在一般的強化學習任務中，有不止一種情況，目標是學習一個策略：<mark style="color:red;">從情況到在這些情況下最好的行動的對映</mark>。為了給整個問題做個鋪墊，我們簡要地討論一下非關聯任務延伸到關聯環境的最簡單方式。

舉個例子，假設有幾個不同的k臂老虎機任務，在每一步你都要面對其中一個隨機選擇的任務。因此，老虎機任務在每一步都會隨機變化。這在你看來就是一個單一的、非穩態的k臂老虎機任務，其真實行動值在每一步都隨機變化。你可以嘗試使用本章中描述的可以處理非平穩性的方法之一，但是除非真實行動值變化緩慢，否則這些方法的效果不會很好。

然而，現在假設當為你選擇一個老虎機任務時，你得到了一些關於任務的獨特線索（但不是其行動值）。也許你面對的是一台實際的老虎機，它在改變動作值的時候會改變其螢幕的顏色。現在你可以學習一種策略，將你看到的顏色所表示的每項任務與面對該任務時應採取的最佳行動聯絡起來。例如，如果是紅色，選擇手臂1；如果是綠色，選擇手臂2。有了正確的策略，你通常可以比在沒有任何資訊的情況下做得更好。

這是一個關聯搜尋任務的例子，<mark style="color:red;">之所以被稱為關聯搜尋，是因為它既涉及到試錯學習以尋找最佳行動，又涉及到這些行動與它們在什麼情況下是最佳的關聯</mark>。

關聯搜尋任務是介於k臂老虎機問題和完全強化學習問題之間。它們與完全強化學習問題一樣，涉及到學習策略，但與我們的k臂老虎機問題一樣，每個行動隻影響到即時獎勵。<mark style="color:blue;">如果允許行動同時影響下一個情況和獎勵，那麼我們就有了完整的強化學習問題</mark>。

## 參考資料

* [https://wensun.github.io/CS4789\_data/UCB\_note\_new.pdf](https://wensun.github.io/CS4789_data/UCB_note_new.pdf)

