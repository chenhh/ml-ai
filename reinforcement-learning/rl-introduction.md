# 強化學習介紹

## 簡介

強化學習的問題<mark style="color:red;">具體來說，就是對不完全已知的馬可夫決策過程(MDP)的最佳控制</mark>。基本的想法是簡單地捕捉一個學習代理所面臨的實際問題的最重要方面，隨著時間的推移與環境互動以實現目標。<mark style="color:blue;">一個學習型代理必須能夠在某種程度上感知其環境的狀態，並且必須能夠採取影響狀態的行動。代理人還必須有一個或多個與環境狀態有關的目標</mark>。MDP旨在包括這三個方面：<mark style="color:red;">**感覺、行動和目標**</mark>，以其最簡單的形式，而不使其中任何一個瑣碎化。任何適合解決此類問題的方法，我們都認為是一種強化學習方法。

#### **定義**

* 通過與環境互動以達成目標的代理學習方式。
* 比其他類型的機器學習更具現實性與挑戰性。
* 通過試錯與延遲的評估回饋（獎勵）學習。

#### **特色**

* 強化學習模仿自然學習過程。
* 可以自我判斷對錯的學習系統。
* 為理解智能科學的一部分。

#### **多領域應用**

涵蓋計算機科學、經濟學、數學、工程、神經科學、心理學等。

#### 成功案例

* Tesauro 的 TD-Gammon（西洋雙陸棋）。
* 自動駕駛、廣告優化、語音識別和遊戲人工智慧等。
* Google DeepMind 的 Atari 遊戲與 AlphaGo。

#### RL 的挑戰

* 回饋延遲與評估。
* 需要平衡探索（探索未知）與利用（最大化已知獎勵）。
* 環境可能為未知、非線性或隨機的。

### 與監督式學習(supervised learning)的區別

<mark style="color:blue;">監督學習是指從一個有知識的外部監督者提供的有標簽的例子的訓練集中學</mark>習。每個例子都是對一種情況的描述，以及系統對這種情況應該採取的正確行動的說明--標簽，這通常是為了確定該情況所屬的類別。

這種學習的目的是讓系統推斷或概括其反應，以便在訓練集中不存在的情況下正確行事。這是一種重要的學習方式，但僅憑這一點還不足以從互動中學習。在互動問題中，要獲得既正確又能代表代理人必須採取行動的所有情況的期望行為的例子往往是不實際的。在未知的領域--人們期望學習是最有益的--<mark style="color:red;">代理人必須能夠從它自己的經驗中學習</mark>。

### 與非監督式學習(unsupervised learning)的區別

強化學習也不同於無監督學習，後者通常是尋找隱藏在未標記資料集合中的結構。雖然人們可能會認為強化學習是一種無監督學習，因為它不依賴於正確行為的例子，<mark style="color:red;">但強化學習是試圖使獎勵訊號最大化，而不是試圖找到隱藏的結構</mark>。

### 探索和利用(exploration and exploitation)的平衡

強化學習中出現的一個挑戰，是探索和利用之間的權衡。<mark style="color:blue;">為了獲得大量的獎勵，強化學習代理必須傾向於它在過去嘗試過的行動，並且發現這些行動在產生獎勵方面是有效的。但為了發現這些行動，它必須嘗試以前沒有選擇過的行動</mark>。代理人必須利用它已經經歷過的東西來獲得獎勵，但它也必須探索，以便在未來做出更好的行動選擇。

無論是探索還是利用，都不能完全追求，否則就會失敗的任務。代理人必須嘗試各種行動，並逐步傾向於那些似乎是最好的行動。在一個隨機的任務中，每個行動都必須被多次嘗試，以獲得對其預期回報的可靠估計。數學家們對探索-開發困境進行了幾十年的深入研究，但仍未得到解決。平衡探索和利用的整個問題沒有出現在監督和無監督學習的正規型式中。

### 代理人(agent)

強化學習的另一個關鍵特徵是，<mark style="color:blue;">它明確考慮了目標導向的代理人與不確定環境互動的整個問題</mark>。這與許多考慮子問題而不解決它們如何融入大局的方法形成對比。例如，我們已經提到，許多機器學習研究關注的是監督學習，而沒有明確說明這種能力最終將如何發揮作用。其他研究人員開發了具有一般目標的規劃理論，但沒有考慮規劃在實時決策中的作用，也沒有考慮規劃所需的預測模型從何而來的問題。盡管這些方法產生了許多有用的結果，但它們對孤立的子問題的關注是一個重要的限制。

強化學習採取了相反的做法，從一個完整的、互動的、尋求目標的代理人開始。<mark style="color:blue;">所有的強化學習代理人都有明確的目標，可以感知其環境的各個方面，並可以選擇行動來影響其環境。此外，通常從一開始就假設代理人必須在它所面臨的環境有很大不確定性的情況下進行操作</mark>。

### 強化學習的應用

* 一個國際象棋高手如何走下一步。這種選擇既是通過規劃預測可能的回應和反饋，也是通過對特定位置和步驟的可取性的直接、直觀的判斷。
* 一個自適應控制器實時調整一個工廠的執行引數。該控制器在指定的邊際成本基礎上，最佳化產量/成本/質量的權衡，而不嚴格遵守工程師最初建議的設定點。
* 一隻小羚羊在出生幾分鐘後就掙扎著站起來。半小時後，它就 以每小時20英裡的速度奔跑。
* 掃地機器人決定它是應該進入一個新的房間尋找更多的垃圾來收集，還是開始嘗試找到回到電池充電站的路。它做出決定的依據是其電池當前的充電水平，以及它在過去能多快多容易地找到充電站。

這些例子的共同特點是非常基本的，以至於很容易被忽視。<mark style="color:red;">所有的例子都涉及到一個主動決策的代理人和它的環境之間的互動，在這種互動中，盡管環境不確定，但代理人仍試圖實現一個目標</mark>。

<mark style="color:red;">代理人的行動被允許影響環境的未來狀態（例如，下一個棋子的位置、煉油廠的儲油罐水平、機器人的下一個位置和其電池的未來充電水平），從而影響代理人在以後可利用的行動和機會</mark>。正確的選擇需要考慮到<mark style="color:blue;">行動的間接、延遲的後果</mark>，因此可能需要預知或計畫。同時，在所有這些例子中，行動的影響不能被完全預測；因此，代理人必須經常監測其環境並作出適當的反應。

<mark style="color:red;">所有這些例子涉及的目標都是明確的，即代理人可以根據它能直接感知的東西來判斷其目標的進展</mark>。棋手知道他是否贏了，煉油廠控制器知道正在生產多少石油，小羚羊知道它什麼時候跌倒，移動機器人知道它的電池什麼時候用完。

<mark style="color:red;">在所有這些例子中，代理人可以利用其經驗隨著時間的推移提高其效能</mark>。棋手完善了他用來評估位置的直覺，從而改善了他的棋藝；羚羊小腿提高了它的奔跑效率。

## 強化學習的重要元素

除了<mark style="color:red;">代理人(agent)</mark>和環境之外，我們可以確定強化學習系統的四個主要子元素：

* <mark style="color:red;">策略(policy)</mark>
  * 定義代理人在特定時間的行為方式。
  * 是從環境的感知狀態到行動的對映。如心理學中的刺激-反應規則。
  * 可能是簡單的函式、查詢表或涉及計算的過程。
  * 是強化學習代理的核心，決定行為。
  * 一般來說，策略可能是隨機的，每個行動有不同的機率。
* <mark style="color:red;">獎勵訊號(reward signal)</mark>
  * 定義強化學習問題的目標。
  * 每個時間步驟環境傳送一個數字獎勵。
  * 代理目標是<mark style="background-color:red;">**最大化長期總獎勵**</mark>。
  * 定義什麼是好的事件與壞的事件。
  * 是代理面臨問題的直接且決定性的特徵。
  * 是改變策略的主要依據：若行動後的獎勵低，可調整策略以選擇其他行動。
  * 可能是環境狀態與行動的隨機函式。
* <mark style="color:red;">價值函數(value function)</mark>
  * 規定狀態的長期好處：狀態的價值 = 從該狀態開始期望積累的獎勵總量。
  * 獎勵反映即時可取性，價值反映長期可取性。狀態可能產生低即時獎勵，但有高長期價值。
  * 獎勵是主要的，價值是次要的（預測獎勵的工具）。
  * 行動選擇基於價值判斷：尋求帶來最高價值的行動，以實現長期最大回報。
  * 價值估計比獎勵難：
    * 獎勵由環境直接給予，價值需從觀察序列中估計。
    * 強化學習演算法的核心是有效估計價值。
* <mark style="color:red;">環境模型(model of environment)(非必要)</mark>
  * 模仿或推斷環境行為。可預測給定狀態和行動的下一個狀態與獎勵。
  * 用於規劃：在實際經歷前考慮未來可能情況來制定行動。
  * 解決方式分為：
    * 基於模型的方法：通過規劃進行學習。
    * 無模型的方法：試錯學習者，與規劃相反。
  * 現代強化學習範圍：從低層次的試錯學習到高層次的慎重規劃。

### 限制與範圍

**狀態 (State)**

1. <mark style="background-color:orange;">狀態是描述物理系統在某一時刻全部資訊的集合</mark>。
2. 作為策略、價值函式以及模型的輸入和輸出。
3. 非正式定義：是一個訊號，向代理人(agent)傳達當前環境的狀況。
4. 廣義定義：代理人可獲得的任何關於環境的資訊。
5. 假設狀態訊號由預處理系統產生，並視為環境的一部分。
6. 本書關注的不是設計狀態訊號，而是根據現有狀態訊號決定行動。

**價值函式與其他解決方法**

1. 多數強化學習方法以<mark style="color:red;">**估計價值函數為核心**</mark>，但並非唯一解法。
2. 其他方法（如遺傳演算法、模擬退火等）不估計價值函數，屬於進化方法。

**進化方法**

1. 特點：
   * 使用多個靜態策略，每個策略與環境互動並獲得回報。
   * 獲得最多獎勵的策略及其隨機變化進入下一代。
   * 模仿生物進化過程，無需代理人在其生命週期中學習。
2. 適用條件：
   * 策略空間足夠小或結構化以易於找到好的策略。
   * 時間充裕，可進行大量搜尋。
   * 代理無法感知完整環境狀態時。
3. 侷限性：
   * 忽略許多強化學習問題的有用結構：
     * 未利用策略是從狀態到行動的函式這一特徵。
     * 未考慮代理的生命週期中經過的狀態及選擇的行動。
   * 某些情況下，這些資訊可能誤導，但通常可提高搜尋效率。
4. 評價：
   * 雖然進化方法與強化學習有共同特點並能協同工作，但本身不特別適合強化學習問題。

**強化學習與進化方法的比較**

1. 強化學習方法：
   * 在與環境互動的同時進行學習。
   * 更能利用行為互動的細節，有效性高。
2. 進化方法：
   * 靠靜態策略長期與環境互動進行優化。
   * 忽略行為細節，效率較低，但在特殊條件下可能有效。

## 範例：井字游戲

1. **遊戲規則**：
   * 雙方輪流下棋，一方用X，另一方用O。
   * 在水平、垂直或對角線連成三個相同標記即勝。
   * 棋盤填滿但無人連線則為平局。
2. 假設對手是一個不完美的棋手，有時下錯棋，增加獲勝機會。
3. 假設平局和輸棋同樣不利，目標是最大化獲勝機率。

#### **傳統解決方法的侷限性**

1. **賽局理論的最小化解法**：
   * 假設對手有特定的遊戲方式。
   * 不會進入可能輸的狀態，即使該狀態實際上可能因對手失誤而獲勝。
2. **動態規劃**：
   * 可計算任何對手的最優解，但需完整的對手行為模型。
3. 實際問題中，對手的行為通常沒有先驗資訊。可通過多次對局從經驗中估計對手行為模型。

#### **進化方法應用**

1. **方法概述**：
   * 在策略空間中搜尋能高機率擊敗對手的策略。
   * 策略：決定在每個遊戲狀態下應採取的行動規則。
2. **操作流程**：
   * 對每個策略，通過對局估計其獲勝機率。
   * 根據評估結果調整策略，進行策略爬坡以逐步改進。
3. **特點**：
   * 不需要價值函數。
   * 依賴策略空間搜尋和策略評估。

#### **使用價值函數的方法**

1. **建立價值函式**：
   * 建立一個表格(table)，記錄所有可能遊戲狀態的數字值。
   * 每個值代表該狀態下獲勝機率的估計。
2. **初始化**：
   * 有三個X連線的狀態：值為1（已勝）。
   * 有三個O連線或填滿的狀態：值為0（不可勝）。
   * 其他狀態：初始值為0.5（猜測有50%獲勝機率）。
3. **遊戲過程**：
   * 每步檢查可能棋步產生的狀態，查詢其當前價值。
   * **行動策略**：
     * 大多數時候選擇價值最大的棋步（貪婪行動）。
     * 偶爾隨機選擇棋步（探索行動）。
4. **更新價值函式**：
   * 隨著遊戲進行，不斷調整狀態的值以更準確估計獲勝機率。
   * 更新方法：
     * 早期狀態的值向後期狀態的值靠攏（值“回升”）。
     * 貪婪行動後，用後期狀態的值更新前期狀態的值。

#### **探索與學習**

1. **探索行動**：使棋手能體驗到可能無法通過貪婪行動看到的狀態。
2. **學習目標**：不斷改進狀態值的估計，使得每次行動更接近最優策略。

令$$S_t$$, $$S_{t+1}$$為貪婪行動前、後的狀態，則$$S_t$$​的價值函數更新如下：

* $$V(S_t) \leftrightarrow V(S_t) + \alpha [V(S_{t+1})- V(S_t)]$$
* $$\alpha$$為步長(step size)參數。
* 此為時序差分學習(temporal-difference learning)方法

此方法在這個任務上表現得相當好。例如，如果步長參數隨著時間的推移而適當減少，那麼對於任何固定的對手，這個方法都會收斂到我們的棋手在每個狀態下的真實獲勝機率。此外，隨後採取的棋步（除了探索性棋步）實際上是對這個（不完美）對手的最佳棋步。換句話說，該方法收斂到了與該對手對弈的最優策略上。如果步長參數沒有隨著時間的推移一直減少到零，那麼這個棋手在面對慢慢改變其下棋方式的對手時也會下得很好。

![井字游戲的決策過程，實線為實際採取的行動，虛線為可行的行動集合。](../.gitbook/assets/rl_tic-toc-min.png)

#### **進化方法與價值函式方法的區別**

1. **進化方法**：
   * 固定策略，與對手進行多次對局或模擬，計算勝率作為策略評估。
   * 每次策略改變僅在多場比賽後進行，且僅利用最終結果。
   * 忽略比賽中具體行動對結果的影響。
   * 無法針對單一狀態進行評估。
2. **價值函式方法**：
   * 可評估單一狀態的價值。
   * 利用比賽過程中的所有可用資訊來改進策略。
   * 搜尋策略空間的過程中，結合了狀態和行動的延遲影響。

**強化學習方法的關鍵特徵**

1. **基於與環境互動的學習**：例如，與對手互動的過程中學習最佳行為。
2. **規劃和預見的能力**：
   * 考慮選擇行動的延遲影響。例如，學會設定**多步陷阱**對付目光短淺的對手。
3. **無需對手模型即可實現規劃**：不必明確搜尋未來狀態和行動的可能序列，但仍能取得規劃效果。

<mark style="color:red;">先驗資訊可以通過各種方式納入強化學習，這對高效學習至關重要</mark>。

#### **強化學習的適用性**

1. **雙人對抗與自然互動**：
   * 適用於與對手對抗的遊戲（如井字遊戲）。
   * 也適用於與自然環境互動的問題。
2. **獎勵的型別**：
   * 適用於分情節獲得獎勵的問題。
   * 適用於無限期持續行為且可隨時獲得不同獎勵的問題。
3. **時間維度**：適用於離散時間和連續時間的問題（但連續時間理論更複雜）。

<mark style="color:red;">有一些強化學習方法根本不需要任何種類的環境模型。無模型的系統甚至不能考慮它們的環境將如何對一個單一的行動做出反應而變化</mark>。在這個意義上，井字遊戲對於對手是無模型的：它沒有任何形式的對手模型。由於模型必須相當精確才有用，<mark style="color:blue;">當解決問題的真正瓶頸是難以構建足夠精確的環境模型時，無模型方法比更復雜的方法更有優勢</mark>。無模型方法也是基於模型的方法的重要構件。

## 小結

強化學習是一種理解目標導向的學習和決策並使之自動化的計算方法。<mark style="color:red;">它與其他計算方法的區別在於，</mark><mark style="color:red;">**它強調代理人從與環境的直接互動中學習，而不需要示範性的監督或環境的完整模型**</mark>。

強化學習是第一個認真解決從與環境的互動中學習以實現長期目標的計算問題的領域。<mark style="color:red;">強化學習使用馬可夫決策過程的正式框架，以</mark><mark style="color:red;">**狀態、行動和獎勵**</mark><mark style="color:red;">的方式定義學習代理人與環境之間的互動</mark>。這個框架旨在成為代表人工智慧問題基本特徵的一種簡單方式。這些特徵包括<mark style="color:blue;">因果關系，不確定性和非確定性</mark>，以及明確目標的存在。

<mark style="color:red;">價值(value)和價值函數</mark>的概念是我們在本書中所考慮的大多數強化學習方法的關鍵。我們的立場是，價值函數對於在策略空間中的有效搜尋非常重要。價值函數的使用將強化學習方法與進化方法區分開來，後者直接在策略空間中以整個策略的評估為指導進行搜尋。

## 強化學習的早期歷史

強化學習的早期歷史有兩條主線，既漫長又豐富，它們在現代強化學習中交織在一起之前是獨立追求的。一條主線是關於通過<mark style="color:red;">試驗和錯誤的學習</mark>，它起源於動物學習的心理學。這條主線貫穿了人工智慧領域的一些最早的工作，並導致了強化學習在20世紀80年代初的復興。

第二條主線是關於<mark style="color:red;">最優控制(optimal control)的問題及其使用價值函數和動態規劃的解決方案</mark>。在大多數情況下，這條主線並不涉及學習。這兩條主線大多是獨立的，但在某種程度上，圍繞著第三條不太明顯的主線，即關於時序差分的方法，如本章中的井字游戲的例子，變得相互關聯。所有這三條線索在20世紀80年代末匯聚在一起，形成了現代強化學習領域。

專注於試錯學習的主線是我們最熟悉的，也是我們在這段簡短歷史中最有發言權的。然而，在這之前，我們先簡要地討論一下最優控制。術語 "最優控制 "是在20世紀50年代末出現的，用來描述設計一個控制器來最小化或最大化一個動態系統在一段時間內的行為的問題。這種方法使用動態系統的狀態和價值函數或 "最優報酬函數(optimal return ufnction) "的概念來定義一個函數方程，現在通常稱為Bellman方程。通過求解這個方程來解決最優控制問題的一類方法被稱為動態規劃（Bellman, 1957a）。Bellman（1957b）還介紹了最優控制問題的離散隨機版本，即馬可夫決策過程（MDPs）。Ronald Howard（1960）設計了MDPs的策略迭代方法。所有這些都是現代強化學習的理論和演算法的基本要素。

<mark style="color:red;">動態規劃被廣泛認為是解決一般隨機最優控制問題的唯一可行的方法</mark>。<mark style="color:red;">它受到Bellman所說的 "維度詛咒(curse of dimensionality) "的影響，也就是說，它的計算要求隨著狀態變量數量的增加而呈指數級增長，但它仍然比任何其他一般方法更有效、更廣泛地適用。</mark>自20世紀50年代末以來，動態規劃得到了廣泛的發展，包括對部分可觀察的MDP的擴展（由Lovejoy, 1991調查），許多應用（由White, 1985, 1988, 1993調查），近似方法（由Rust, 1996調查），以及異步方法（Bertsekas, 1982, 1983）。許多優秀的動態規劃現代處理方法都可以得到（例如，Bertsekas，2005，2012；Puterman，1994；Ross，1983；和Whittle，1982，1983）。Bryson（1996）提供了一個權威的最優控制的歷史。

最佳控制和動態規劃與學習之間的聯系被緩慢地認識到。我們不能確定是什麼原因造成了這種分離，但其主要原因可能是相關學科之間的分離和它們不同的目標。還有一個原因可能是人們普遍認為動態規劃是一種主要依靠精確的系統模型和Bellman方程的分析解來進行的精細計算。<mark style="color:blue;">此外，最簡單的動態規劃是一種在時間上向後進行的計算，因此很難看到它如何參與到一個必須向前進行的學習過程中</mark>。一些最早的動態規劃工作，如Bellman和Dreyfus（1959）的工作，現在可能被歸類為遵循學習方法。Witten（1977）的工作（在下文中討論）當然也可以說是學習和動態規劃思想的結合。Werbos（1987年）明確主張動態規劃和學習方法的更大的相互關系，以及動態規劃與理解神經和認知機制的相關性。對我們來說，動態規劃方法與在線學習的完全結合直到1989年Chris Watkins的工作才發生，他用MDP形式主義對強化學習的處理已經被廣泛採用。此後，這些關系被許多研究者廣泛發展，特別是Dimitri Bertsekas和John Tsitsiklis（1996），他們創造了 "神經動態規劃 "這一術語來指代動態規劃和人工神經網絡的結合。目前使用的另一個術語是 "近似動態規劃"。這些不同的方法強調了該主題的不同方面，但它們都與強化學習一樣，對規避動態規劃的經典缺點感興趣。

<mark style="color:blue;">我們把強化學習方法定義為解決強化學習問題的任何有效方法，現在很明顯，這些問題與最優控制問題密切相關，特別是隨機最優控制問題，如那些被表述為MDP的問題</mark>。因此，我們必須把最優控制的解決方法，如動態規劃，也看作是強化學習方法。因為幾乎所有的傳統方法都需要對要控制的系統有完整的瞭解，所以說它們是強化學習的一部分感覺有點不自然。另一方面，許多動態規劃演算法是增量的和迭代的。像學習方法一樣，它們通過連續的逼近逐漸達到正確的答案。完全知識和不完全知識情況下的理論和解決方法是如此密切相關，以至於我們覺得必須把它們作為同一主題的一部分來考慮。

現在讓我們回到導致現代強化學習領域的另一條主要線索，即以試錯學習思想為中心的線索。我們在這裡只觸及主要的聯系點。根據美國心理學家R.S.Woodworth（1938）的說法，試錯學習的思想最早可以追溯到19世紀50年代Alexander Bain對 "摸索和實驗 "學習的討論，更明確的是英國倫理學家和心理學家Conway Lloyd Morgan在1894年用這個詞來描述他對動物行為的觀察。也許第一個簡明扼要地表達試錯學習作為一種學習原則的本質的是Edward Thorndike。

> 在對同一情況作出的幾種反應中，那些伴隨著或緊隨其後的動物滿意的反應，在其他條件不變的情況下，將與該情況有更牢固的聯系，因此，當它再次出現時，它們將更有可能再次出現；那些伴隨著或緊隨其後的動物不舒服的反應，在其他條件不變的情況下，它們與該情況的聯系將被削弱，因此，當它再次出現時，它們將更不可能出現。滿意或不舒服的程度越大，紐帶的加強或削弱就越大。(Thorndike, 1911, p. 244)

<mark style="color:red;">Thorndike把這稱為 "效果法則( law of effect)"，因為它描述了強化事件對選擇行動的傾向的影響</mark>。Thorndike後來對該定律進行了修改，以更好地解釋後來的動物學習數據（如獎勵和懲罰的影響之間的差異），該定律的各種形式在學習理論家中引起了相當大的爭議（例如，見Gallistel, 2005; Herrnstein, 1970; Kimble, 1961, 1967; Mazur, 1994）。盡管如此，效果法則在某種形式上被廣泛認為是許多行為的基本原則（例如，Hilgard和Bower, 1975；Dennett, 1978；Campbell, 1960；Cziko, 1995）。它是Clark Hull（1943年，1952年）的有影響力的學習理論和B. F. Skinner（1938年）有影響力的實驗方法的基礎。

在動物學習的背景下，<mark style="color:red;">"強化 "一詞是在Thorndike表達效果法則之後開始使用的</mark>，第一次出現在這個背景下（據我們所知）是在1927年Pavlov關於條件反射的專著的英譯本。<mark style="color:blue;">Pavlov把強化描述為由於動物接受刺激--強化物--與另一個刺激或反應有適當的時間關系而導致的行為模式的強化</mark>。一些心理學家把強化的概念擴展到包括行為的減弱和加強，並把強化物的概念擴展到可能包括刺激的省略或終止。要被認為是強化物，強化或削弱必須在強化物撤走後持續存在；僅僅吸引動物的注意力或激發其行為而不產生持久變化的刺激物將不被認為是強化物。

在計算機中實現試錯學習的想法出現在關於人工智慧可能性的最早的想法中。在1948年的一份報告中，Alan Turing描述了一個 "<mark style="color:blue;">快樂-痛苦系統</mark> "的設計，該系統按照效果法則運作。

> 當達到一個動作未定的配置時，對缺失的數據進行隨機選擇，並在描述中暫定適當的條目，並加以應用。當疼痛刺激發生時，所有暫定的條目都被取消，而當快樂刺激發生時，它們都被永久化。(Turing, 1948)

許多巧妙的電動機械被建造出來，展示了試錯學習。最早的可能是Thomas Ross製造的機器（1933年），它能夠在一個簡單的迷宮中找到自己的路，並通過開關的設置記住路徑。1951年，W.Grey Walter建造了一個版本的 "機械烏龜"（Walter, 1950），能夠進行簡單形式的學習。1952年，Claude Shannon展示了一隻名為Theseus的跑迷宮的老鼠，它利用試驗和錯誤來尋找通過迷宮的方法，迷宮本身通過其地板下的磁鐵和繼電器來記住成功的方向（另見Shannon，1951）。J. A. Deutsch (1954)根據他的行為理論（Deutsch, 1953）描述了一種解迷宮的機器，它與基於模型的強化學習有一些共同的屬性。Marvin Minsky在他的博士論文中討論了強化學習的計算模型，並描述了他建造的模擬機器，該機器由他稱為SNARC（隨機神經模擬強化計算器）的部件組成，旨在類似於大腦中可修改的突觸連接。cyberneticzoo.com網站上有大量關於這些和其他許多電子機械學習機的資訊。

建立機電式學習機讓位於對數字計算機的程式，以執行各種類型的學習，其中一些實現了試錯式學習。Farley和Clark（1954年）描述了一個通過試錯學習的神經網路學習機的數位模擬。但他們的興趣很快就從試錯學習轉移到了泛化和模式識別，即從強化學習到監督學習（Clark和Farley，1955）。這就開始了對這些學習類型之間關係的混亂模式。許多研究人員似乎認為他們在研究強化學習，而實際上他們是在研究監督學習。例如，人工神經網路的先驅，如Rosenblatt（1962）和Widrow和Ho（1960）顯然是受強化學習的激勵--他們使用了獎懲的語言，但他們研究的系統是適用於模式識別和感知學習的監督學習系統。即使在今天，一些研究人員和教科書也盡量減少或模糊了這些類型的學習之間的區別。例如，一些人工神經網路教科書用 "試錯 "一詞來描述從訓練樣本中學習的網絡。這是一個可以理解的混淆，因為這些網絡使用錯誤資訊來更新連接權重，但這忽略了試錯學習的基本特徵，即根據評價性反饋來選擇行動，而不依賴於對正確行動的瞭解。

部分由於這些混淆，在20世紀60年代和70年代，對真正的試錯學習的研究變得很少，盡管有一些明顯的例外。在20世紀60年代，"強化 "和 "強化學習 "這兩個詞第一次在工程文獻中被用來描述試錯學習的工程應用（例如，Waltz和Fu，1965；Mendel，1966；Fu，1970；Mendel和McClaren，1970）。特別有影響的是 Minsky的論文《邁向人工智慧》（Minsky, 1961），其中討論了與試錯學習相關的幾個問題，包括預測、期望和他所說的復雜強化學習系統的基本信用分配問題。你如何在許多可能參與產生成功的決定中分配成功的功勞？

在接下來的幾段中，我們將討論在20世紀60年代和70年代相對忽視對真正的試錯學習的計算和理論研究的其他一些例外和部分例外。一個例外是紐西蘭研究員John Andrea的工作，他開發了一個叫做STeLLA的系統，在與環境的互動中通過試錯學習。這個系統包括一個內部的世界模型，後來還包括一個 "內部獨白 "來處理隱藏狀態的問題（Andreae, 1963, 1969a, b）。Andreae後來的工作（1977年）更強調從教師那裡學習，但仍然包括通過試驗和錯誤學習，產生新的事件是該系統的目標之一。這項工作的一個特點是 "回漏過程"，在Andreae（1998）中得到了更充分的闡述，它實現了一種類似於我們所描述的備份更新操作的信用分配機制。不幸的是，他的開創性研究並不為人所知，也沒有對後來的強化學習研究產生很大影響。最近的總結可參考（Andreae，2017a，b）。

更有影響力的是Donald Michie的工作。在1961年和1963年，他描述了一個簡單的試錯學習系統，用於學習如何玩井字游戲（或九宮格游戲），稱為MENACE（火柴盒可教育九宮格引擎）。它由每個可能的游戲位置的火柴盒組成，每個火柴盒包含一些彩色的珠子，不同的顏色代表該位置的每個可能的動作。通過從對應於當前游戲位置的火柴盒中隨機抽出一顆珠子，就可以確定MENACE的行動。當游戲結束時，珠子被新增到游戲過程中使用的盒子中或從盒子中取出，以獎勵或懲罰MENACE的決定。Michie和Chambers(1968)描述了另一個叫GLEE(游戲學習期望值引擎)的井字形強化學習器和一個叫BOXES的強化學習控制器。他們將BOXES應用於學習如何平衡一根鉸接在可移動小車上的桿子的任務，其依據是只有在桿子倒下或小車到達軌道末端時才會出現失敗信號。這項任務改編自Widrow和Smith（1964）的早期工作，他們使用監督學習方法，假設教師已經能夠平衡桿子。Michie和Chambers的平衡桿版本是不完全知識條件下強化學習任務的早期最佳例子之一。它影響了後來的強化學習工作，首先是我們自己的一些研究（Barto, Sutton, and Anderson, 1983; Sutton, 1984）。Michie一直強調試錯和學習的作用是人工智慧的重要方面（Michie, 1974）。

Widrow、Gupta和Maitra（1973）修改了Widrow和Ho（1960）的最小均方（LMS）演算法，產生了一個強化學習規則，可以從成功和失敗信號中學習，而不是從訓練實例中學習。他們稱這種學習形式為 "選擇性引導適應"，並將其描述為 "與批評家(critic)一起學習 "而不是 "與老師一起學習"。他們分析了這個規則，並展示了它是如何學會玩21點的。這是Widrow對強化學習的一次孤立的嘗試，他對監督學習的貢獻要大得多。我們對 "批評家 "一詞的使用來自於Widrow, Gupta, and Maitra的論文。Buchanan、Mitchell、Smith和Johnson（1978年）在機器學習的背景下獨立使用了批判者這個術語（也見Dietterich和Buchanan，1984年），但對他們來說，批判者是一個能夠做得比評估效能更多的專家系統。

關於學習自動機的研究對導致現代強化學習研究的試錯線索有更直接的影響。這些方法用於解決一個非關聯性的、純粹的選擇學習問題，通過與老虎機或 "單臂老虎機 "的類比，被稱為k臂老虎機，但有k個槓桿。學習型自動機是簡單的、低記憶體的機器，用於提高這些問題中獎勵的機率。學習型自動機起源於俄羅斯數學家和物理學家M.L.Tsetlin及其同事在20世紀60年代的工作（在Tsetlin, 1973中追述），此後在工程中得到了廣泛的發展（見Narendra和Thathachar, 1974, 1989）。這些發展包括對隨機學習自動機的研究，它是在獎勵信號基礎上更新行動機率的方法。雖然不是在隨機學習自動機的傳統中發展起來的，但Harth和Tzanakou（1974）的Alopex演算法（模式提取演算法）是一種用於檢測行動和強化之間相關性的隨機方法，影響了我們早期的一些研究（Barto, Sutton, and Brouwer, 1981）。隨機學習自動機被心理學的早期工作所預示，從William Estes (1950)對學習的統計理論的嘗試開始，並由其他人進一步發展（例如，Bush和Mosteller, 1955；Sternberg, 1963）。

在心理學中發展起來的統計學習理論被經濟學研究者採用，導致該領域中出現了一條專門研究強化學習的線索。這項工作始於1973年，Bush和Mosteller的學習理論被應用於一系列經典的經濟模型（Cross，1973）。這項研究的一個目標是研究比傳統的理想化經濟代理更像真人的人工代理（Arthur，1991）。這種方法擴展到賽局理論背景下的強化學習研究。經濟學中的強化學習在很大程度上獨立於人工智慧中強化學習的早期工作，盡管賽局理論仍然是兩個領域都感興趣的話題（超出了本書的范圍）。Camerer（2011）討論了經濟學中的強化學習傳統，Nowé、Vrancx和De Hauwere（2012）從我們在本書中介紹的方法的多代理人擴展的角度對該主題進行了概述。賽局理論背景下的強化是一個與用於玩井字棋、跳棋和其他娛樂游戲的程式中的強化學習大不相同的課題。例如，見Szita（2012）對強化學習和游戲這方面的概述。

John Holland（1975）概述了一個基於選擇原則的適應性系統的一般理論。他的早期工作主要涉及非關聯形式的試錯，如進化方法和k臂老虎機。1976年和1986年，他更全面地介紹了分類器系統，真正的強化學習系統包括關聯和價值函數。Holland的分類器系統的一個關鍵組成部分是用於信用分配的 "桶隊演算法"，它與我們的井字游戲例子中使用的、在第六章中討論的時序差分演算法密切相關。另一個關鍵部分是遺傳演算法，這是一種進化方法，其作用是進化出有用的表徵。分類器系統已經被許多研究者廣泛開發，形成了強化學習研究的一個主要分支（Urbanowicz和Moore，2009年回顧），但是遺傳演算法--我們不認為它本身是強化學習系統--已經得到了更多的關注，就像其他進化計算方法一樣（例如Fogel, Owens and Walsh, 1966, and Koza, 1992）。

在人工智慧領域，對恢復強化學習的試錯線索負有最大責任的人是Harry Klopf（1972, 1975, 1982）。Klopf認識到，由於學習研究人員幾乎只關注有監督的學習，適應性行為的基本方面正在丟失。<mark style="color:red;">Klopf認為，所遺失的是行為的享樂方面，即從環境中獲得某種結果的驅動力，即控制環境朝向期望的目的並遠離不期望的目的。這就是試錯學習的基本思想</mark>。Klopf的思想對作者的影響特別大，因為我們對他們的評估（Barto and Sutton, 1981a）導致了我們對監督學習和強化學習之間的區別的欣賞，並導致我們最終對強化學習的關注。我們和同事們早期完成的大部分工作都是為了證明強化學習和監督學習確實是不同的（Barto, Sutton, and Brouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985）。其他研究表明強化學習如何解決人工神經網絡學習中的重要問題，特別是如何產生多層網絡的學習演算法（Barto, Anderson, and Sutton, 1982; Barto and Anderson, 1985; Barto, 1985, 1986; Barto and Jordan, 1987; 見第15.10節）。

現在我們來看看強化學習歷史的第三條線索，即關於時序差分學習。<mark style="color:red;">時序差分學習方法的獨特之處在於，它是由對同一數量的時間性連續估計之間的差異所驅動的</mark>--例如，在井字棋的例子中，對獲勝機率的估計。這條主線比其他兩個主線要小，也不那麼明顯，但它在該領域發揮了特別重要的作用，部分原因是時序差分方法對強化學習來說似乎是新的和獨特的。

時序差分學習的起源部分來自於動物學習心理學，特別是次級強化物的概念。次要強化物是指與主要強化物（如食物或疼痛）配對的刺激物，因此，它具有類似的強化特性。Minsky（1954）可能是第一個意識到這個心理學原理對人工學習系統很重要的人。Arthur Samuel（1959年）是第一個提出並實施了包括時序差分學習方法的人，作為他著名的跳棋游戲程式的一部分（第16.2節）。

Samuel沒有提到Minsky的工作或與動物學習的可能聯系。他的靈感顯然來自於Claude Shannon（1950年）的建議，即計算機可以被編寫程式為使用一個評估函數來下棋，並且它可能能夠通過在線修改這個函數來改進它的游戲。(Shannon的這些想法有可能也影響了Bellman，但我們不知道這方面的證據)。Minsky（1961）在他的 "Steps "論文中廣泛地討論了Samuel的工作，提出了與自然和人工的二次強化理論的聯系。正如我們所討論的，在Minsky和Samuel的工作之後的十年裡，很少有關於試錯學習的計算工作，而且顯然也沒有關於時序差分學習的計算工作。1972年，Klopf將試錯學習與時序差分學習的一個重要組成部分結合起來。克洛普夫對能夠擴展到大型系統中的學習的原則很感興趣，因此對區域性強化的概念很感興趣，即整個學習系統的子部件可以相互強化。他提出了 "廣義強化 "的概念，即每個組件（名義上是每個神經元）都以強化的方式看待其所有輸入：興奮性輸入為獎勵，抑制性輸入為懲罰。這與我們現在所知道的時序差分不是同一個概念，而且回過頭來看，它比Samuel的工作更無關離。另一方面，Klopf將這一想法與試錯學習聯繫起來，並將其與動物學習心理學的大量經驗數據庫聯繫起來。

Sutton（1978a,b,c）進一步發展了Klopf的思想，特別是與動物學習理論的聯系，描述了由時間上連續的預測變化驅動的學習規則。他和Barto完善了這些觀點，並發展了一個基於時間連續學習的古典條件的心理學模型（Sutton and Barto, 1981a; Barto and Sutton, 1982）。隨後，還有其他幾個有影響力的基於時序差分方法的古典條件反射的心理學模型（例如，Klopf，1988；Moore等人，1986；Sutton和Barto，1987，1990）。此時開發的一些神經科學模型很好地解釋了時序差分學習（Hawkins和Kandel，1984；Byrne，Gingrich和Baxter，1990；Gelperin，Hopfield和Tank，1985；Tesauro，1986；Fristo等人，1994），盡管在大多數情況下沒有歷史聯系。我們早期關於時序差分學習的工作受到了動物學習理論和Klopf工作的強烈影響。與Minsky的 "Steps "論文和Samuel的跳棋選手的關系是後來才認識到的。然而，到了1981年，我們已經完全意識到上述所有先前的工作，作為時序差分關系和試錯線的一部分。在這個時候，我們開發了一種使用時序差分學習與試錯學習相結合的方法，稱為演員-批評(actor-critic)結構，並將這種方法應用於Michie和Chambers的平衡桿問題（Barto, Sutton, and Anderson, 1983）。這個方法在Sutton（1984）的博士論文中得到了廣泛的研究，並在Anderson（1986）的博士論文中擴展到使用反向傳播神經網絡。大約在這個時候，Holland（1986）以他的Bucket-brigade演算法的形式將時序差分時間-思想明確地納入他的分類器系統。Sutton（1988）邁出了關鍵的一步，他將時序差分學習從控制中分離出來，將其視為一種一般的預測方法。該論文還介紹了TD( $$\lambda$$)演算法並證明了它的一些收斂特性。

當我們在1981年完成我們的行為准則架構的工作時，我們發現了Ian Witten（1977, 1976a）的一篇論文，這似乎是最早發表的時序差分學習規則。他提出的方法，我們現在稱之為表格TD(0)，作為解決MDP的自適應控制器的一部分使用。這項工作在1974年首次提交給期刊發表，也出現在Witten 1976年的博士論文中。Witten的工作是Andreae對STeLLA和其他試錯學習系統的早期實驗的後裔。因此，Witten1977年的論文跨越了強化學習研究的兩條主線--試錯學習和最優控制--同時對時差學習做出了獨特的早期貢獻。

<mark style="color:blue;">1989年，隨著Chris Watkins對Q-learning的開發，時序差分和最優控制這兩條線被完全結合起來。這項工作擴展並整合了強化學習研究中所有三條主線的前期工作</mark>。Paul Werbos（1987年）通過論證1977年以來試錯學習和動態規劃的收斂性，為這種整合做出了貢獻。到沃特金斯工作的時候，強化學習研究已經有了巨大的發展，主要是在人工智慧的機器學習子領域，但也在人工神經網絡和更廣泛的人工智慧領域。1992年，Gerry Tesauro的雙陸棋游戲程式TD-Gammon的顯著成功給該領域帶來了更多關注。

## 表格法(tabular method)

強化學習演算法的所有核心思想的最簡單形式：<mark style="color:red;">即狀態和行動空間足夠小，近似的價值函數可以表示為表格</mark>。在這種情況下，這些方法通常可以找到精確的解決方案，也就是說，它們通常可以准確地找到最優價值函數和最優策略。這與<mark style="color:blue;">近似方法形成對比，後者只能找到近似解，但反過來可以有效地應用於更大的問題</mark>。

有限馬可夫決策問題的三類基本方法：<mark style="color:red;">動態規劃、蒙特卡洛方法和時序差分學習</mark>。每一類方法都有其優勢和劣勢。

* 動態規劃方法在數學上有很好的發展，但需要一個完整和准確的環境模型。
* 蒙特卡洛方法不需要模型，在概念上也很簡單，但不太適合分步遞增的計算。
* 最後，時序差分方法不需要模型，是完全增量的，但分析起來更復雜。

## 參考資料

* Richard Suttion and Andrew G. Barto, "Reinforcement Learning: An Introduction," 2nd, 2018, chapter 1.
